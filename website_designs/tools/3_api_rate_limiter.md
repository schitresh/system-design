# API Rate Limiter
- Throttles users based upon the number of the received requests
  - To allow only a certain number of requests per second or min
    - So that the servers are not overloaded
  - It limits the number of events an entity (user, device, IP, etc.) can perform
    - In a particular time window
- Examples
  - A user can send only one message per second
  - A user is allowed only three failed credit card transactions per day
  - A single IP can only create twenty accounts per day

## Requirements
- Functional Requirements
  - Limit the number of requests an entity can send to an API within a time window
    - E.g. 15 requests per second
  - The APIs are accessible through a cluster
    - So the rate limit should be considered across different servers
  - The user should get an error message when the threshold is crossed
- Non-Functional Requirements
  - System should be highly available
    - It should always work to protect the service from external attacks
  - Should not introduce substantial latencies affecting the user experience

# Component Design
## Rate Limiting
- Rate limiting defines the rate and speed at which consumers can access APIs
  - Throttling controls the usage of the APIs by customers during a given period
- Helps to protect services against attacks on application layer
  - Like denial of service (DOS), brute force password or credit card transactions
  - These attacks are usually a barrage of https requests
    - Which may look like they are coming from real users
    - But are typically generated by machines or bots
- Also used to prevent revenue loss and to reduce infrastructure costs
  - To stop spam and to stop online harassment
  - To make sure that the service stays up for everyone
  - To make sure low priority requests don't affect the high priority traffic
    - Like high volume of requests for analytics data
    - Should not be hamper critical transactions for other users
  - There could be default limits for the API service
    - To go beyond that, the user has to buy higher limits

### Types of Rate Limiting
- Hard Throttling: Number of requests cannot exceed the throttle limit
- Soft Throttling: Number of requests can exceed a certain percentage
  - E.g. rate-limit of 100 messages/minute with 10% exceed limit
  - Allows up to 110 messages/minute
- Elastic or Dynamic Throttling
  - Number of requests can go beyond the threshold if free resources are available

## Fixed Window Algorithm
- Time window is considered from the start of the time-unit to the end of the time-unit
  - A period is considered as 0-60 seconds for a minute
  - Irrespective of the time frame at which the request is made
- For example, consider the rate-limit of 2 messages/second
  - There are two messages between 0-1 second and three messages between 1-2 second
  - The messages will be considered individually for each of these seconds
  - Hence, only the last message in 1-2 second will be throttled
- Disadvantage: It can potentially allow twice the number of requests
  - If we send 2 messages at the last of the second
  - We can immediately send 2 messages at the start of the next second
  - Resulting in 4 messages within a span of 1 second

### Atomicity
- In a distributed environment
  - Read-and-then-write behavior can create a race condition
- If the current count is 2 and we issue two more requests
  - If two separate processes serve these requests
  - And concurrently read the count before either of them update it
  - Then, each process will allow one more request
- If we use Redis and Memcached to store our key-value
  - We can use their locking feature for the duration of the operation
  - But this will also slow down concurrent requests from the same user
- If we use simple hash-table, we can custom implement locking feature

### Memory Estimation
- Let's assume the simple solution where we're storing all the data in a hash table
  - { user_id: { count: integer, count_started_at: epoch_time } }
- Row size: 12 bytes
  - user_id: 8 bytes
  - count: 2 bytes (can count upto 65k)
  - count_started_at: 2 bytes
    - Although epoch time will need 4 bytes
    - But we can store only the minute and second parts
- Total row size: 12 + 20 + 4 = 32 bytes
  - Assume an overhead of 20 bytes for each record
  - Assume that we need a 4 byte number to lock each user's record
    - To resolve the atomicity problems
- Users: 1 M
- Total Storage = 36 bytes * 1 M = 36 MB
  - This can easily fit on a single server
  - But we won't like to route all of the traffic through a single machine
  - Also, if we assume a rate-limit of 10 requests/second for each user
    - This would translate into 10 M queries/second for the rate limiter
    - This would be too much for a single server

## Rolling/Sliding Window Algorithm
- Time window is considered
  - From the fraction of the time at which the request is made
  - Plus the time window length
- For example, consider the rate-limit of 2 messages/second
  - There are two messages sent at 300th & 400th millisecond between 0-1 second
  - There are two messages sent at 100th & 200th millisecond between 1-2 second
  - The messages will be counted from the 300th millisecond of the first second
    - Up to the 300th millisecond of next second
  - Hence, the last two messages will be throttled

### Memory Estimation
- Store timestamps and counts in redis
  - { user_id: { count: integer, timestamps: sorted set of epoch-times } }
- Assume rate limit of 600 requests/hour = 10 requests/min
- Timestamps: 24 bytes * 10 requests/min * 60 mins = 14.4 KB
  - Each epoch time will require 4 bytes
  - We will need 16 bytes for pointers
    - In a sorted set, we can assume that we need at least 2 pointers
    - To maintain order among elements
    - One pointer to the previous element and one to the next element
    - On a 64 bit machine, each pointer will cost 8 bytes
  - We can add an extra word (4 bytes) for storing other overhead
- Row size: 14428 bytes = 15 KB
  - user_id: 8 bytes
  - timestamps: 15 KB
  - Assume an overhead of 20 bytes for each record
- Users: 1 M
- Total Storage: 15 KB * 1 M = 15 GB
- It takes a lot of memory compared to the fixed window algorithm
  - This can lead to scalability issue
  - What if we combine the two algorithms to optimize the memory usage?

## Sliding Window with Counters
- Keep track of request counts for each user using multiple fixed time windows
  - E.g. the 1/60th size of rate-limit's time window
  - So for an hourly rate limit, we can keep count for each minute
  - And calculate the sum of all these minor windows to calculate the throttling limit

### Memory Estimation
- Store counters in redis which offers efficient storage for fewer than 100 keys
  - { user_id: { count: integer, counter_hash: epoch-time with counts } }
- Assume rate limit of 600 requests/hour = 10 requests/min
  - There will be 60 minor windows since this is hourly limit
- Counter hash: 26 bytes * 60 minor windows = 1.560 KB
  - Each epoch time will need 4 bytes
  - Counter would need 2 bytes
  - Assume an overhead of 20 bytes
- Row size: 1588 bytes = 1.6 KB
  - user_id: 8 bytes
  - counter_hash: 1.560 KB
  - Assume an overhead of 20 bytes
- Users: 1 M
- Storage: 1.6 KB * 1 M = 1.6 GB
- This uses 86% less memory than the simple sliding window algorithm

## Rate Limit Criterias
### IP
- It's not optimal in differentiating good & bad actors
  - But still better than not having rate limiting at all
- When multiple users share a single public IP
  - One bad user can cause throttling for all others
- There are a huge number of IPv6 addresses available to a hacker from even one computer

### User Id
- On user authentication, the user will be provided with a token
  - Which the user will have to paass with each request
- This will ensure that we rate limit against a particular API
  - That has a valid authentication token
- But this cannot rate limit on the login API itself
  - A hacker can perform denial of service (DOS) attack by entering wrong credentials
  - After that, the actual user won't be able to login

### Hybrid
- Use both per-IP and per-user rate limiting
- But this will result in more cache entries with more details per entry
  - Hence this will require more memory and storage

# Scalability
## Data Sharding
- We can shard based on User Id
- If we want different limits for different APIs, we can shard per user per API
  - For example, in url shortener we have create_url & delete_url APIs
  - We can consider separate rate limiter for each API shard as well

## Caching
- We can cache recently active users
- Write-back cache
  - Update all counters and timestamps in cache only
  - The write to the permanent storage can be done at fixed intervals
  - This will ensure minimum latency added to the user requests by the rate limiter
