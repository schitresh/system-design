{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"System Design","text":"<ul> <li>Design Concepts</li> <li>Design Patterns</li> <li>Website Designs</li> </ul>"},{"location":"design_concepts","title":"Index","text":""},{"location":"design_concepts#low-level-design","title":"Low Level Design","text":"<ul> <li>Object Oriented Principles</li> <li>Design Patterns</li> <li>UML Diagrams</li> <li>Schema Design</li> <li>Design Problems</li> <li>Machine Coding</li> </ul>"},{"location":"design_concepts#high-level-design","title":"High Level Design","text":"<ul> <li>Architecture Design</li> <li>Distributed Systems</li> <li>Load Balancing</li> <li>Horizontal vs Vertial Scaling</li> <li>Consistency - Weak, Eventual, Strong</li> <li>Consistent Hashing</li> <li>Clustering</li> <li>Caching</li> <li>CDN</li> <li>Proxy</li> <li>CAP Theorem</li> <li>Queues</li> <li>Architectural Styles</li> <li>REST, GraphQL, gRPC</li> <li>Cloud Design Patterns</li> <li>Long vs Short Polling</li> <li>Web Sockets</li> <li>SSE</li> <li>Transactions</li> <li>SQL vs NoSQL</li> <li>Designing Real Life Systems</li> </ul>"},{"location":"design_concepts#scaling","title":"Scaling","text":"<ul> <li>Load Balancing<ul> <li>Instances &amp; clones</li> <li>Heartbeat</li> <li>Rate Limiting</li> </ul> </li> <li>Requests &amp; Data<ul> <li>Sync vs Async</li> <li>Paging &amp; filtering</li> </ul> </li> <li>Scaling<ul> <li>Vertical Scaling</li> <li>Horizontal Scaling</li> </ul> </li> <li>Database<ul> <li>SQL vs NoSQL</li> <li>Database Replication</li> <li>Database Partitioning</li> <li>Database Sharding</li> </ul> </li> <li>Caching<ul> <li>Persistent Cache</li> </ul> </li> <li>Events &amp; Logging</li> </ul>"},{"location":"design_concepts#object-oriented-analysis-and-design","title":"Object Oriented Analysis and Design","text":"<ul> <li>Tiny URL</li> <li>Elevator</li> <li>Parking Lot</li> <li>Online book reader system</li> <li>Bookmyshow</li> </ul>"},{"location":"design_concepts/introduction","title":"Introduction","text":""},{"location":"design_concepts/introduction#requirements","title":"Requirements","text":""},{"location":"design_concepts/introduction#functional-requirements","title":"Functional Requirements","text":"<ul> <li>Requirements specified by the end user as part of the contract</li> <li>Features that need to be included and what will be their inputs &amp; outputs</li> </ul>"},{"location":"design_concepts/introduction#non-functional-requirements","title":"Non-functional Requirements","text":"<ul> <li>Quality constraints that the system must satisfy</li> <li>Defined conditions under which the system must operate</li> <li>Constraints<ul> <li>Portability</li> <li>Security</li> <li>Maintainability</li> <li>Reliability</li> <li>Scalability</li> <li>Performance</li> <li>Reusability</li> <li>Flexibility</li> </ul> </li> </ul>"},{"location":"design_concepts/introduction#extended-requirements","title":"Extended Requirements","text":"<ul> <li>Nice to have requirements that are not the core functionalities</li> </ul>"},{"location":"design_concepts/architecture/types_of_architecture","title":"Types of Architecture","text":""},{"location":"design_concepts/architecture/types_of_architecture#monolithic-systems","title":"Monolithic Systems","text":"<ul> <li>The system where the whole application is deployed as a single unit</li> <li>Benefits<ul> <li>Easy to develop, deploy and test, ideal for small organizations</li> <li>Easy to rollback changes and debug</li> </ul> </li> <li>Challenges<ul> <li>Any kind of changes require complete redeployment</li> <li>Less scalable since each element can have different scalability requirement</li> </ul> </li> <li>Sections<ul> <li>Client Tier / User Layer / Presentation Layer / Front-end Layer<ul> <li>Takes input from users, interacts with server and displays the result to the user</li> </ul> </li> <li>Middle Tier / Service Layer<ul> <li>Includes the business logic</li> <li>Receives requests from the client, acts on them and stores the data</li> </ul> </li> <li>Data Tier / Persistence Layer<ul> <li>Databases, message queues</li> <li>Handles communication with other applications</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/types_of_architecture#microservices","title":"Microservices","text":"<ul> <li>Small services that handle a specific part of the functionality and data</li> <li>They communicate with each other directly using APIs and lightweight protocols like HTTP</li> <li>Each mircoservice has its own database</li> <li>Easy to scale independently</li> <li>Other services continue to operate in case of failure or maintainence</li> </ul>"},{"location":"design_concepts/architecture/types_of_architecture#distributed-systems","title":"Distributed Systems","text":"<ul> <li>Collection of multiple individual systems<ul> <li>Connected through network sharing resources to achieve a common goal</li> </ul> </li> <li>Requests and data are shared across multiple servers<ul> <li>Increasing availability and decreasing latency</li> </ul> </li> <li>Highly scalable and solves single point of failure</li> <li>Complex to manage data and hardware<ul> <li>In a network failure, conflicting information can be passed</li> </ul> </li> <li>Race Conditions<ul> <li>Bugs that arise in systems due to timing mismatch<ul> <li>Of the execution order of multiple system services</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/types_of_architecture#soa-service-oriented-architecture","title":"SOA (Service Oriented Architecture)","text":"<ul> <li>SOA is an enterprise-wide concept</li> <li>It enables existing applications to be exposed over loosely-coupled interfaces</li> <li>Each interface corresponding to a business function<ul> <li>That enables applications in one part of an extended enterprise</li> <li>To reuse functionality in other applications</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/types_of_architecture#event-driven-architecture","title":"Event Driven Architecture","text":"<ul> <li>System components communicate by generating, detecting, responding to events</li> <li>Events represent significant occurences like user actions or changes in system state</li> <li>When an event occurs, a message is sent to other components triggering an appropriate response</li> <li>Benefits<ul> <li>Rapid response to changing conditions and new information</li> <li>Flexibility due to loose coupling of components</li> <li>Decentralized communication reducing the need for point to point connections</li> <li>Improved fault tolerance and scalability</li> </ul> </li> <li>Challenges<ul> <li>Can become complex as the number of events and components grow</li> <li>Maintaining the order of events and ensuring consistency</li> <li>Overhead of event bus</li> <li>Debugging and tracing events can be challenging</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/types_of_architecture#events","title":"Events","text":"<ul> <li>Triggered by occurrences like user actions, changes in data, external stimuli, system processes</li> <li>Represented as messages or signals that convey information about an occurrence</li> <li>Asynchronous communication allowing for parallel processing</li> <li>Typically handled using pub-sub model</li> <li>Includes event type and payload to provide context and details about an event</li> <li>Event broker or event bus handles distribution, filtering and routing of events<ul> <li>Aggregator may be used to combine related events into a single meaningful event</li> <li>Dispatcher may be used to route events to appropriate event handlers and manage flow</li> <li>Filters and rules engine to determine which subscribers should receive them</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/microservices","title":"Microservices","text":"<ul> <li>Small services that handle a specific part of the functionality and data</li> <li>They communicate with each other directly using APIs and lightweight protocols like HTTP</li> <li>Each mircoservice has its own database<ul> <li>It ensures loose coupling but may result in some duplicate data</li> <li>Different databases can be used depending on the requirements</li> </ul> </li> <li>Benefits<ul> <li>Each service offers a secure module boundary and can be written in different languages</li> <li>Easy to scale independently</li> <li>Other services continue to operate in case of failure or maintainence</li> </ul> </li> <li>Challenges<ul> <li>Creation of large number of small services can increase complexity</li> <li>Increased overhead of network communication, data consistency, serialization/de-serialization of data</li> <li>Challenges to monitor, manage, test the services with each other</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/microservices#components","title":"Components","text":"<ul> <li>API Gateway<ul> <li>Central entry point for external clients to interact with the mircoservice</li> <li>Manges requests, handles authentication, routes requests</li> </ul> </li> <li>Load Balancer</li> <li>Service Registry and Discovery<ul> <li>Keeps track of the locations and network addresses of all microservices</li> </ul> </li> <li>Containerization<ul> <li>Containers like Docker encapsulate the mircoservice and its dependencies</li> <li>Orchestration tools like Kubernetes manage the deployment, scaling, operation of containers</li> </ul> </li> <li>Event Bus or Message Broker<ul> <li>Allows services to publish and subscribe to events</li> <li>Enables asynchronous communication and decoupling</li> </ul> </li> <li>Database &amp; Caching</li> <li>Centralized Logging and Monitoring</li> <li>Fault Tolerance &amp; Resilience Components<ul> <li>Circuit breakers, retry mechanisms, etc</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/microservices#anti-patterns","title":"Anti-Patterns","text":"<ul> <li>Data Monolith<ul> <li>Sharing a centralized database among microservices undermining independence &amp; scalability</li> </ul> </li> <li>Chatty Services<ul> <li>Mircoservices overly communicating for small tasks, leading to increased network overhead and latency</li> </ul> </li> <li>Overusing Microservices<ul> <li>Creating too many microservices for trivial functionalities introducing unnecessary complexity</li> </ul> </li> <li>Inadequate Service Boundaries<ul> <li>Poorly defined boundaries of microservice resulting in ambiguity and unclear responsibilities</li> </ul> </li> <li>Ignoring Security<ul> <li>Neglecting security concerns in microservice risking vulnerabilities and data breaches</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/microservices#example-e-commerce","title":"Example: E-commerce","text":"<ul> <li>User Service<ul> <li>Manages user accounts, authentication, preferences</li> <li>Handles user registration, login, profile management ensuring a personalized experience for users</li> </ul> </li> <li>Search Service<ul> <li>Indexes product information, provides relevant search results based on user queries</li> <li>Enables users to find products quickly</li> </ul> </li> <li>Cart Service<ul> <li>Manages the shopping cart for users (add, remove, modify items before checkout)</li> <li>Ensures a seamless shopping experience by keeping track of selected items</li> </ul> </li> <li>Wishlist Service<ul> <li>Manages user wishlists allowing them to save products for future purchase</li> <li>Let's them track their desired items to purchase later or if not available currently</li> </ul> </li> <li>Order Taking Service<ul> <li>Accepts and processes orders placed by customers</li> <li>Validates orders, checks for product availability, initiates order fulfillment process</li> </ul> </li> <li>Order Processing Service<ul> <li>Manages processing and fulfillment of orders</li> <li>Coordinates with inventory, shipping, payment services to ensure timely and accurate order delivery</li> </ul> </li> <li>Payment Service<ul> <li>Securely processes payment transactions and manages payment-related data</li> <li>Integrates with payment gateways</li> </ul> </li> <li>Logistics Service<ul> <li>Calculates shipping costs, assigns carriers, tracks shipments, manages delivery routes</li> </ul> </li> <li>Warehouse Service<ul> <li>Manages inventory across warehouuses</li> <li>Tracks inventory levels, updates stock availability, coordinates stock replenishment</li> </ul> </li> <li>Notification Service<ul> <li>Sends notifications to users regarding their orders, promotions</li> <li>Keeps users updated about status of their interactions with the platform</li> </ul> </li> <li>Recommendation Service<ul> <li>Provides personalized product recommendations to users</li> <li>Analyzes user behavior and preferences to suggest relevant products</li> <li>Improves user experience and drives sales</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider","title":"Factors To Consider","text":""},{"location":"design_concepts/architecture/factors_to_consider#scalability","title":"Scalability","text":"<ul> <li>Capability of a system to grow, manage and balance the load without performance loss</li> <li>Required to manage increased demand for requests, data, transactions</li> <li>It's said that when designing a system for load x, plan for 10x and test for 100x</li> <li>Factors<ul> <li>Number of requests per day</li> <li>Number of database calls (read &amp; write) per day</li> <li>Amount of cache hit or miss</li> <li>Daily active users</li> <li>Traffic distribution</li> </ul> </li> <li>Examples<ul> <li>Surge in e-commerce platforms on big sale or holidays</li> <li>Buffering or low quality video streams due to strained network</li> <li>E-banking app can experience authentication bottlenecks due to high volume of requests</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Adding more machines to the pool of resources</li> <li>Distributes workload across the individual units</li> <li>Increases fault tolerance since the number of nodes increase</li> <li>Can introduce complexity and requires load balancing management</li> <li>Ideal for handling massive scalability needs<ul> <li>Unlimited potential to grow</li> <li>Effective for microservices allowing to scale independently</li> <li>Useful where responsiveness and speed are critical</li> <li>Cost effective than vertical scaling in the long run due to flexibility</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>Adding more power (CPU, RAM, storage) to existing machines</li> <li>Limited to the capacity of machines</li> <li>Limited fault tolerance since the number of nodes remain the same</li> <li>Suitable for moderate scalability requirements<ul> <li>If the application isn't expected to experience rapid growth in traffic and resource demands</li> <li>Simpler and requires fewer changes</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#serverless","title":"Serverless","text":"<ul> <li>If the traffic fluctuates a lot, serverless is a good option<ul> <li>Unpredictable traffic patterns or infrequent bursts of activity</li> </ul> </li> <li>Cost-effective because only the actual used resources are charged<ul> <li>Beneficial when the service is idle</li> </ul> </li> <li>Platforms like AWS Lambda, Azure Functions handle the underlying infrastructure reducing operational overhead</li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#availability","title":"Availability","text":"<ul> <li>Measure of time that a system, service or machine remains operational</li> <li>It takes into account maintainability, repair, spares, and other logistics time</li> <li>Strategies to achieve high availability<ul> <li>Redundancy</li> <li>Load balancing</li> <li>Failover mechanisms</li> <li>Monitoring and alerting</li> <li>Performance optimization</li> <li>Disaster recovery plan</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#reliability","title":"Reliability","text":"<ul> <li>Capability of a system to deliver its services when some components fail<ul> <li>If some item is added to cart, it shouldn't be lost</li> <li>If something fails, another cart with same items should replace it</li> </ul> </li> <li>Reliability is 'availability over time' considering all conditions that can occur<ul> <li>If a system is reliable, it is available but vice-versa may not be true</li> </ul> </li> <li>Achieved by Redundancy and Availability factors</li> <li>Avoid single point of failures</li> <li>Measured by mean time between failures and mean time to repair</li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#serviceability-or-manageability","title":"Serviceability (or Manageability)","text":"<ul> <li>How easy it is to operate and maintain</li> <li>Simplicity and speed with which system can be repaired</li> <li>Ease of diagnosing and understanding problems, making updates or modifications</li> <li>It affects availability if fixing a failure takes long time</li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>Can a system continue to function even in the presence of faults</li> <li>How quickly the system can detect &amp; handle failures and become operational again</li> <li>Faults are the errors that arise in a component, it may not be failure of the system</li> <li>Failure is the state when the system is not able to perform and provide services as expected</li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#redundancy","title":"Redundancy","text":"<ul> <li>Having duplicate resources or backups<ul> <li>To make sure that the system keeps working even if something breaks</li> <li>Duplication of critical data or services to increase reliability of the system</li> <li>It can remove single point of failure and provide backups</li> <li>Load balancing ensures that everything works efficiently and reliably</li> </ul> </li> <li>Example: Extra servers, database replicas, RAID, geographic servers or data centers</li> <li>Types<ul> <li>Active: Multiple resources doing the same job at the same time<ul> <li>If one of them fails, the others take over to keep running smoothly</li> </ul> </li> <li>Passive: Backup resources that don't peform any job until required<ul> <li>They stay in background, ready to jump in whenever a problem occurs</li> </ul> </li> </ul> </li> <li>Testing<ul> <li>Redundancy: Simulating failures to verify that redundant components function correctly</li> <li>Validation: Ensuring that data synchronization and consistency are maintained</li> <li>Load: Assessing how the system performs under heavy loads to identify bottlenecks</li> </ul> </li> <li>Shared-nothing architecture<ul> <li>Each node can operate independently, there shouldn't be any central service managing activities</li> <li>New servers can be added without conditions and there is no single point of failure</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/factors_to_consider#modularity","title":"Modularity","text":"<ul> <li>Dividing a complex system into smaller independent modules<ul> <li>That can be developed and tested individually and then integrated into the overall system</li> <li>Improves flexibility and reliability of the system</li> </ul> </li> <li>Each module is designed to perform a specific function and is self-contained<ul> <li>With well defined interfaces to other modules</li> </ul> </li> <li>Allows different teams to work on different modules concurrently</li> <li>Interfaces<ul> <li>Set of rules or guidelines that define how different components of a system interact with each other</li> <li>Specifies inputs, outputs, and behaviors of a component</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/consistency","title":"Consistency","text":"<ul> <li>Ensuring that all the nodes have the same view of the data at any given time</li> <li>Despite possible concurrent operations and network delays</li> </ul>"},{"location":"design_concepts/architecture/consistency#strong-consistency","title":"Strong Consistency","text":"<ul> <li>Also known as strict consistency</li> <li>Every read operation receives the most recent write operation's value or an error</li> <li>Ensures that all clients see the same sequence of updates<ul> <li>And that updates appear to be instantaneous</li> </ul> </li> <li>Requires coordination and synchronization between distributed notes<ul> <li>Which can impact system performance &amp; availability</li> </ul> </li> <li>Example: Traditional SQL database system with a single master node and multiple replicas</li> </ul>"},{"location":"design_concepts/architecture/consistency#eventual-consistency","title":"Eventual Consistency","text":"<ul> <li>Allows replicas of data to diverge temporarily<ul> <li>But ensures that they will eventually converge to the same value</li> </ul> </li> <li>Better availability and performance than strong consistency</li> <li>Example: Amazon DynamoDB</li> </ul>"},{"location":"design_concepts/architecture/consistency#causal-consistency","title":"Causal Consistency","text":"<ul> <li>Preserves the causality between related events</li> <li>If event A causally precedes event B, all nodes will agree on this ordering</li> <li>Ensures that clients observing concurrent events<ul> <li>Maintain a consistent view of their causality relationship</li> <li>Essential for maintaining application semantics and correctness</li> </ul> </li> <li>Example: Collaborative document editing application<ul> <li>Where users can concurrently edit different sections of a document</li> <li>If user A makes edits that depend on the content written by user B</li> <li>All users should observe these edits in the correct causal order</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/consistency#read-your-writes-consistency","title":"Read-Your-Writes Consistency","text":"<ul> <li>Ensures that the individual clients observe their own updates immediateely</li> <li>After a client writes a value to a data item<ul> <li>It should always be able to read that value or any subsequent value it has written</li> </ul> </li> <li>It is important for maintaining session consistency in applications<ul> <li>Where users expect to see their own updates reflected immediately</li> </ul> </li> <li>Example: Social media platform<ul> <li>Where users expect to immediately see the new post or comments published by them</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/consistency#monotonic-consistency","title":"Monotonic Consistency","text":"<ul> <li>Ensures that if a client observes a particular order of updates (reads or writes) to a data<ul> <li>It will never observe a conflicting order of updates</li> </ul> </li> <li>Prevents the system from reverting to previous states</li> <li>Example: Distributed key-value store<ul> <li>If a client reads A, B, C, then it will never later observe C, A, B</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/consistency#weak-consistency","title":"Weak Consistency","text":"<ul> <li>Does not provide any guarantees about when or if replicas will converge</li> <li>Allows for concurrent updates and may result in temporary inconsistencies</li> <li>Used in systems where low latency and high availability are prioritized over strict consistency</li> <li>Example: Distributed caching system like Redis or Memcached</li> </ul>"},{"location":"design_concepts/architecture/consistency#conflict-resolution-techniques","title":"Conflict Resolution Techniques","text":"<ul> <li>Last Writer Wins (LWW)<ul> <li>Favor the update with the latest timestamp or version</li> <li>Might lead to data loss or inconsistency in some scenarios</li> </ul> </li> <li>Merge Strategies<ul> <li>Use custom merge strategies depending on the specific requirements</li> <li>Reconcile conflicting updates based on application specific semantics and user preferences</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/efficiency","title":"Efficiency","text":"<ul> <li>Two standard measures of efficiency are<ul> <li>Latency (or response time)</li> <li>Throughput (or bandwidth)</li> </ul> </li> <li>The two measures correspond to these unit costs<ul> <li>Number of messages sent by the nodes of the system regardless of the message size</li> <li>Size of messages representing the volume of data exchanges</li> </ul> </li> <li>The complexity of operations supported by distributed data structures<ul> <li>Can be characterized as a function of one of these cost units</li> </ul> </li> <li>The analysis of a distributed system in terms of number of messages is over simplistic<ul> <li>It ignores the impact of many aspects like network topology, network load</li> <li>Heterogeneity of the software &amp; hardware components<ul> <li>Involved in data processing &amp; routing</li> </ul> </li> <li>However, it is quite difficult to devlop such a precise cost model</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/efficiency#latency-or-response-time","title":"Latency (or Response Time)","text":"<ul> <li>Time taken for a data or signal to travel from one point to another in a system</li> <li>It encompasses delays like processing time, transmission time, response time</li> <li>Low latency is important in payments, transactions, gaming</li> </ul>"},{"location":"design_concepts/architecture/efficiency#factors-affecting-latency","title":"Factors Affecting Latency","text":"<ul> <li>Physical distance</li> <li>Network congestion</li> <li>Inefficient network infrastructure</li> <li>Wireless interference</li> <li>Slow hardware</li> <li>Software inefficiency</li> <li>Database access: complex query, overloaded databases</li> <li>Resource competition: CPU, memory</li> <li>Data compression and encryption</li> </ul>"},{"location":"design_concepts/architecture/efficiency#measurement","title":"Measurement","text":"<ul> <li>Ping: Send data packeets to target service and measure round trip time</li> <li>Traceroute: Analyse path taken by data packets, which network hops contribute the most to latency</li> <li>MTR (traceroute with ping)</li> <li>Insert timestamps at various points</li> <li>Network monitoring tools</li> <li>Performance profiling tools</li> </ul>"},{"location":"design_concepts/architecture/efficiency#throughput-or-bandwidth","title":"Throughput (or Bandwidth)","text":"<ul> <li>The rate at which a system, process, or network<ul> <li>Can transfer data or perform operations in a given time period</li> </ul> </li> <li>Difference from Latency<ul> <li>Latency: Time consumed for the transfer of one data packet</li> <li>Throughput: Number of data packets arrived within a second</li> </ul> </li> <li>Measurement in<ul> <li>Network: Amount of data transmitted in a given period</li> <li>Disk: How quickly data can be read or written</li> <li>Processing: Number of operations/instructions completed in a unit time</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/cap_theorem","title":"CAP Theorem","text":"<ul> <li>Factors: Consistency, Availability, Partition Tolerance</li> <li>Only two out of CAP can be guaranteed simultaneously</li> <li>We cannot build a general data store<ul> <li>That is continually available</li> <li>Sequentially consistent</li> <li>And tolerant to any partition failures</li> </ul> </li> <li>When designing a distributed system, trading off among CAP is the first thing to consider</li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#consistency","title":"Consistency","text":"<ul> <li>All nodes have the same data at the same time</li> <li>Achieved by updating the nodes before allowing further reads</li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#availability","title":"Availability","text":"<ul> <li>Every request gets a response in a bounded amount of time</li> <li>System is up despite node failures</li> <li>Achieved by replicating data across servers</li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#partition-tolerance","title":"Partition Tolerance","text":"<ul> <li>System continues to work despite message loss or partial failure</li> <li>Communication or network failure between nodes is a common cause of partitions</li> <li>System can gracefully recover from partitions once the partition heals</li> <li>Data is sufficiently replicated across combinations of nodes and networks<ul> <li>To keep the system up through intermittent outages</li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#trade-offs","title":"Trade Offs","text":"<ul> <li>CAP system is not possible</li> <li>To be consistent, all nodes should see the same set of updates in the same order<ul> <li>But if the network suffers a partition<ul> <li>Updates in one partition might not make it to other partitions</li> <li>Before a client reads from the out-of-date partition</li> </ul> </li> <li>The only thing to cope with this possibility<ul> <li>Is to stop serving requests from the out-of-date partition</li> <li>But then it is no longer 100% available</li> </ul> </li> </ul> </li> <li>If two servers (S1, S2) cannot communicate (partition)<ul> <li>And data is updated in S1, we can do one of these for S2:<ul> <li>Send the last consistent data (no-tolerance: CA)</li> <li>Deny the service for S2 requests (unavailable: CP)</li> <li>Return whatever data is present in S2 (unconsistent: AP)</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#ca-system","title":"CA System","text":"<ul> <li>Delivers consistency and availability unless there is a partition between any nodes<ul> <li>In that case, it sends the last consistent data</li> </ul> </li> <li>Examples: RDBMS, PostgreSQL</li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#cp-system","title":"CP System","text":"<ul> <li>When a partition occurs, the system shuts down the non-available nodes</li> <li>Application: Banking Transactions<ul> <li>Each transaction must be accurately reflected across all servers</li> <li>Even if individual branches face network disruption</li> </ul> </li> <li>Examples: MongoDB, Redis, BigTable, HBase</li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#ap-system","title":"AP System","text":"<ul> <li>When a partition occurs, all nodes remain available<ul> <li>But those at the wrong end of partition might return an older version of data</li> </ul> </li> <li>Application: Social Media<ul> <li>Users expect immediate access to their newsfeeds</li> <li>Even if parts of the netwrok are temporarily down</li> <li>Slight inconsistencies are tolerable like latest post not visible, old likes count</li> </ul> </li> <li>Examples: Cassandra, DynamoDB, CounchDB</li> </ul>"},{"location":"design_concepts/architecture/cap_theorem#hybrid-system","title":"Hybrid System","text":"<ul> <li>Switch seamlessly between modes at different stages of the worflow</li> <li>Application: Online Shopping Cart (AP + CP System)<ul> <li>Allow uninterrupted browsing if any network glitches occur (AP)</li> <li>But when confirming the order and processing payment, it should be consistent (CP)</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/network_protocols","title":"Network Protocols","text":"<ul> <li>Effective functioning of networks is essential<ul> <li>For seamless communication and efficient data transmission</li> </ul> </li> <li>Network protocols are a set of rules and conventions that initiate the communication<ul> <li>And data exchange between different devices in a network</li> </ul> </li> <li>They define the standards for data encoding, transmission, reception<ul> <li>Ensuring that devices can understand &amp; interpret each other's messages</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/network_protocols#importance","title":"Importance","text":"<ul> <li>Interoperability<ul> <li>They permit devices from different manufacturers<ul> <li>And with various functionalities to communicate seamlessly</li> </ul> </li> </ul> </li> <li>Optimizes data transmission minimizing latency and packet loss</li> <li>Allow scalability of data traffic without compromising overall performance</li> <li>Protocols like TCP ensure reliable &amp; ordered data transmission<ul> <li>Crucial for applications that require data integrity (file transfers, email communication)</li> </ul> </li> <li>Security protocols like SSL/TLS encrypt data during transmission<ul> <li>Protect sensitive data from unauthorized access and ensure confidentiality of communications</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/network_protocols#internet-protocol-ip","title":"Internet Protocol (IP)","text":"<ul> <li>Responsible for addressing and routing packets throughout networks</li> <li>Assigns a unique IP address to each device</li> <li>Determines the best suitable path for data packets to reach their destination</li> <li>Fundamental to all networked systems and is used with other protocols for data transmission</li> </ul>"},{"location":"design_concepts/communication/network_protocols#transmission-control-protocol-tcp","title":"Transmission Control Protocol (TCP)","text":"<ul> <li>Core protocol that operates at the transport layer of the OSI model</li> <li>Ensures that data packets are delivered reliably and in order from the sender</li> <li>Establishes a connection earlier than data transfer<ul> <li>Ensures the receipt of packets and retransmits if there is any loss or corrupted data</li> </ul> </li> <li>Used where data integrity and sequencing are critical<ul> <li>Web browsing, data transfers, e-mail delivery</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/network_protocols#user-datagram-protocol-udp","title":"User Datagram Protocol (UDP)","text":"<ul> <li>Simple connectionless protocol that offers minimal services than TCP</li> <li>Also referred as fire and forget protocol</li> <li>Does not guarantee delivery or order of packets</li> <li>Doesn't provide error checking or flow control mechanisms</li> <li>Doesn't setup a connection before data transfer</li> <li>Used in real-time applications where low latecy is important<ul> <li>Like VoIP, online gaming, video streaming</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/network_protocols#domain-name-system-dns","title":"Domain Name System (DNS)","text":"<ul> <li>Translates user-friendly domain names (www.example.com) to numerical IP addresses (192.0.2.1)</li> <li>Computers and network devices use only numerical addresses to locate one another on the internet</li> <li>This removes the tedious need of remembering the addresses by users or systems</li> </ul>"},{"location":"design_concepts/communication/network_protocols#hypertext-transfer-protocol-http","title":"HyperText Transfer Protocol (HTTP)","text":"<ul> <li>Used for transferring hypertext files on World Wide Web</li> <li>Defines how messages are formatted and transmitted between different web servers &amp; clients</li> <li>Used in web browsers to retrieve and display web pages</li> </ul>"},{"location":"design_concepts/communication/network_protocols#http-ssltls-https","title":"HTTP + SSL/TLS (HTTPS)","text":"<ul> <li>Secure Socket Layer / Transport Layer Security</li> <li>SSL/TLS protocols provide stable and secure connection<ul> <li>For information exchange between different devices on a computer network</li> </ul> </li> <li>They encrypt information during transmission</li> </ul>"},{"location":"design_concepts/communication/network_protocols#file-transfer-protocol-ftp","title":"File Transfer Protocol (FTP)","text":"<ul> <li>Used to transfer documents between a client and a server on a computer network</li> <li>Allows users to upload, download, manipulate files on remote servers</li> <li>Also used for website maintenance and software distribution</li> </ul>"},{"location":"design_concepts/communication/network_protocols#smtp-and-pop3imap","title":"SMTP and POP3/IMAP","text":"<ul> <li>SMTP: Simple Mail Transfer Protocol<ul> <li>Used for sending email messages</li> <li>Transfers outgoing mail from a user to a server</li> </ul> </li> <li>POP3: Post Office Protocol, IMAP: Internet Message Access Protocol<ul> <li>Used for receiving email messages</li> <li>Retrieve incoming mail from a server to a user</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/communication_protocols","title":"Communication Protocols","text":"<ul> <li>Communication protocols are used for communication<ul> <li>Between a server and a client (like web browser)</li> </ul> </li> <li>Defines rules, syntax, semantics and synchronization of communication<ul> <li>And possible error recovery methods</li> </ul> </li> <li>Types of communication: synchronous, aynchronous</li> </ul>"},{"location":"design_concepts/communication/communication_protocols#polling-or-short-polling","title":"Polling (or Short Polling)","text":"<ul> <li>Client repeatedly polls (requests) server for data, at regular intervals using HTTP</li> <li>It waits for the server to respond with data</li> <li>If no data is available, an empty request is returned<ul> <li>Which may accumulate to create HTTP overhead</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/communication_protocols#long-polling","title":"Long Polling","text":"<ul> <li>Also known as hanging GET</li> <li>Variation of polling which allows server to push information to client<ul> <li>Only when the data is available</li> </ul> </li> <li>Client sends request over HTTP to server and waits</li> <li>Server holds request instead of sending empty response<ul> <li>Sends response when data is available</li> </ul> </li> <li>Client re-requests server (polls) after getting response<ul> <li>So that server will almost always have an available waiting request</li> <li>That it can use to deliver data in response to an event</li> </ul> </li> <li>Each long-poll request has a timeout<ul> <li>Client has to reconnect periodically after the connection is closed due to timeouts</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/communication_protocols#web-socket","title":"Web Socket","text":"<ul> <li>Full duplex communication (send &amp; receive) over a single TCP connection<ul> <li>Client establishes connection through 3-way handshake</li> <li>Client sends request, server sends handshake, websocket connection is established</li> </ul> </li> <li>Stateful protocol<ul> <li>Persistent connection between client and server to send data any time</li> <li>The connection is not terminated until either the client or the server disconnects</li> </ul> </li> <li>Real-time communication without waiting for a response<ul> <li>Allows message passing back and forth while keeping connection open</li> <li>Server sends content to client without being asked</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/communication_protocols#sse-server-sent-events","title":"SSE (Server Sent Events)","text":"<ul> <li>Only server sends data on this connection</li> <li>Client establishes a persistent and long-term connection</li> <li>If client wants to send data, it would require another protocol</li> <li>Usage: Get real-time traffic, server is generating data in loop</li> </ul>"},{"location":"design_concepts/communication/web_services","title":"Web Services","text":""},{"location":"design_concepts/communication/web_services#rest","title":"REST","text":"<ul> <li>Representational State Transfer</li> <li>Uses HTTP, supports HTTP methods GET, POST, PUT, DELETE</li> <li>Pros: Lightweight, human-readable, easy to build</li> <li>Cons: P2P communication, lack of standards</li> </ul>"},{"location":"design_concepts/communication/web_services#soap","title":"SOAP","text":"<ul> <li>Simple Object Access Protocol</li> <li>Data exchanges using XML, transmission using HTTP &amp; SMTP</li> <li>Pros: Easy to consume, more standards (WSDL), distributed computing</li> <li>Cons: Difficult setup, convoluted coding, hard to build</li> </ul>"},{"location":"design_concepts/communication/web_services#api","title":"API","text":"<ul> <li>Application Programming Interface</li> <li>Set of routines, protocols, and tools for building software applications</li> <li>Computing interface that defines interactions between multiple software intermediaries</li> <li>Defines calls/requests that can be made<ul> <li>How to make them, data formats to be used, conventions to follow, etc.</li> </ul> </li> <li>Can be entirely custom<ul> <li>Or specific to a component</li> <li>Or based on an industry-standard to ensure interoperability</li> </ul> </li> <li>Through information hiding<ul> <li>APIs enable modular programming</li> <li>Allowing users to use the interface independently of the implementation</li> </ul> </li> <li>Supports request/response headers, caching, versioning, content-format</li> </ul>"},{"location":"design_concepts/communication/web_services#rpc","title":"RPC","text":""},{"location":"design_concepts/communication/web_services#graphql","title":"GraphQL","text":""},{"location":"design_concepts/communication/messaging_queues","title":"Messaging Queues","text":"<ul> <li>Enables exchange of messages between different nodes in a distributed system</li> <li>Allows asynchronuous communication, helpful when:<ul> <li>Writes can take long time</li> <li>Data may have to be written in different places</li> <li>System could be under high load</li> <li>It may have to wait on other processes</li> </ul> </li> <li>Used in large scale systems<ul> <li>Can be used to decouple different parts of the system (e.g. microservices)</li> <li>Allowing them to operate independently and improving resilience &amp; scalability</li> </ul> </li> <li>Fault tolerance: it can retry service requests that have failed</li> <li>Frameworks: Apache Kafka, RabbitMQ, AWS SQS (Simple Queue Service)</li> </ul>"},{"location":"design_concepts/communication/messaging_queues#workflow","title":"Workflow","text":"<ul> <li>Producers send messages to a queue</li> <li>The queue stores and manages the messages until they are consumed</li> <li>As soon as any consumer/worker has capacity to process, they can pick up from queue</li> <li>Multiple consumers can read messages concurrently from the queue</li> <li>In some systems, there is a broker for message routing, filtering, transformation</li> <li>In some cases, consumers send back an acknowledgement<ul> <li>To ensure message delivery and preventing message loss</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/messaging_queues#messages","title":"Messages","text":"<ul> <li>Structure<ul> <li>Headers: Metadata about the message, unique identifier, timestamp, message type, routing info</li> <li>Body: Contains message payload</li> </ul> </li> <li>Serialization<ul> <li>Converting complex data structures or objects into a format<ul> <li>That can be easily transmitted, stored, reconstructed</li> </ul> </li> <li>JSON (JavaScript Object Notation)</li> <li>XML (eXtensible Markup Language)</li> <li>Protobuf (Protocol Buffers)</li> <li>Binary Serialization<ul> <li>Used for performance critical applications due to their compactness and speed</li> </ul> </li> </ul> </li> <li>Routing<ul> <li>Topic-based: Sent to topics or channels which can be subscribed</li> <li>Direct: Sent to specific queues or consumers based on addresses or routing keys</li> <li>Content-based: Filters or rules are defined to route messages</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/messaging_queues#types","title":"Types","text":"<ul> <li>Point to Point (P2P): Messages are delivered to a specific recipient</li> <li>Publish Subsribe (Pub-Sub):<ul> <li>Messages are published to a topic and are delivered to all the subscribers of that topic</li> </ul> </li> <li>Hybrid: Combination of P2P &amp; Pub-Sub</li> <li>Dead Letter Queues<ul> <li>Temporarily stores &amp; handle messages that cannot be processed successfully<ul> <li>Messages with errors in their content or format</li> <li>Messages that exceed time-to-live (TTL) or delivery attempts</li> <li>Messages that cannot be delivered to any consumer</li> </ul> </li> <li>Investigates and reprocesses failed messages preventing them from blocking the system</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/messaging_queues#security","title":"Security","text":"<ul> <li>Keep separate queues for sensitive data or urgent processing</li> <li>Enforce access controls to restrict who can send, receive, or administer the message queue</li> <li>Implement data encryption in transmit and at rest to protect messages from eavesdropping</li> </ul>"},{"location":"design_concepts/communication/authentication_and_authorization","title":"Authentication and Authorization","text":""},{"location":"design_concepts/communication/authentication_and_authorization#authentication","title":"Authentication","text":"<ul> <li>Verifies the identity of users of entities trying to access a system, application or network</li> <li>Grants access to only authorized users protecting sensitive information and resources</li> </ul>"},{"location":"design_concepts/communication/authentication_and_authorization#requirements","title":"Requirements","text":"<ul> <li>Storage: Figure out the storage of username and password with encyption</li> <li>Login Management</li> <li>Session Management: How long the user can stay logged in</li> <li>Security: Plan against hacks, attempts to predict password, rate limiting</li> </ul>"},{"location":"design_concepts/communication/authentication_and_authorization#types","title":"Types","text":"<ul> <li>Password: Passwords should be complex and stored securely using encypted hash</li> <li>Multi-Factor Authentication (MFA): Password and a code (sent as email, sms)</li> <li>Biometric: Fingerprints, facial recognition, voice recognition</li> <li>Token: Physical or digital token like a security key or smart card</li> <li>OAuth: Delegated authentication using third party applications</li> </ul>"},{"location":"design_concepts/communication/authentication_and_authorization#authentication-information","title":"Authentication Information","text":"<ul> <li>Tokens are included in HTTP headers (generally Bearer token), URL Params, or Request Payload</li> <li>Session ID is stored in a cookie which is sent with each HTTP request</li> <li>Certificates and keys are verified like SSL/TLS certificates</li> <li>Encryption<ul> <li>SSL/TLS (Secure Sockets Layer / Transport Layer Security)<ul> <li>Encrypts data transmission between a user browser and a web server</li> <li>Secures communication channel and prevents eavesdropping</li> </ul> </li> <li>End-to-End Encryption</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/authentication_and_authorization#authorization","title":"Authorization","text":"<ul> <li>Determines what actions or operations a user, system or entity<ul> <li>Is allowed to perform within a system or network</li> </ul> </li> </ul>"},{"location":"design_concepts/communication/authentication_and_authorization#requirements_1","title":"Requirements","text":"<ul> <li>How the permissions will be structured<ul> <li>Roles &amp; heirarchy, permission types, how users or roles get these roles</li> </ul> </li> <li>Access Control Method: Access Control Lists (ACL) or Role-Based Access Control (RBAC)</li> <li>Protect resources according to the permissions</li> <li>Context &amp; conditions on which access might change, like time of the day or user location</li> <li>Keep track of the access for security monitoring</li> </ul>"},{"location":"design_concepts/communication/authentication_and_authorization#models","title":"Models","text":"<ul> <li>Role Based Access Control (RBAC)<ul> <li>Assigning roles to users or groups letting them access only what their role requires</li> <li>For example, HR personnel can access HR data but not finance information</li> </ul> </li> <li>Access Control List<ul> <li>Lists permissions associated with a system resource</li> <li>Specifies which user or system processes are granted access to resources<ul> <li>And what operations are allowed</li> </ul> </li> </ul> </li> <li>Security Assertion Markup Language (SAML)<ul> <li>Using an XML based protocol for single sign on</li> <li>Access permissions are communicated through digitally signed documents</li> </ul> </li> <li>Mandatory Access Control<ul> <li>Controlling permissions at a deep level in the computer system usually managed by admin</li> </ul> </li> </ul>"},{"location":"design_concepts/components/components","title":"Components","text":""},{"location":"design_concepts/components/components#rate-limiters","title":"Rate Limiters","text":"<ul> <li>Used to limit the rate at which a system of application processes requests or performs actions</li> <li>Types<ul> <li>Request<ul> <li>Protects system from being overloaded by too many requests within a given time period</li> </ul> </li> <li>Action<ul> <li>Limits the number of times a specific action can be performed within a given time period</li> </ul> </li> <li>User<ul> <li>Prevents specific users from making excessive requests to undermine the performance of the system</li> <li>E.g. Denial of service attacks</li> </ul> </li> <li>Token bucket<ul> <li>Limits the rate at which requests are processed by a system</li> <li>Any excess requests are held in a bucket until the next time period</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/components/components#monitoring-system","title":"Monitoring System","text":"<ul> <li>Collects, analyzes and reports metrics and performance related data</li> <li>Helps to monitor if the desired service levels are being met</li> <li>Types<ul> <li>Network: routers, switches, servers</li> <li>System: CPU, memory, disk usage</li> <li>Application: web servers, databases</li> <li>Infrastructure: virtual machines, containers</li> </ul> </li> </ul>"},{"location":"design_concepts/components/components#unique-id-generator","title":"Unique ID Generator","text":"<ul> <li>Generates unique identifiers that are used to identify objects or entities in a distributed system</li> <li>Helpful to provide a stable identifer for a resource accessed over a network</li> <li>Can be done using centralized service, distributed consensus algorithm, timestamps</li> </ul>"},{"location":"design_concepts/components/components#search-services","title":"Search Services","text":"<ul> <li>Using multiple nodes or servers to index and search large datasets in a distributed system</li> <li>Allows parallel processing of search queries and distribution of data</li> <li>Approaches<ul> <li>Distributed Search Engine that scales horizontally across nodes like Elastic search, Apache Solr</li> <li>Database with built-in search capabilities like Mongodb, Cassandra</li> <li>Cloud based search services like AWS Elastic Search, Google Cloud Search</li> </ul> </li> </ul>"},{"location":"design_concepts/components/components#logging-services","title":"Logging Services","text":"<ul> <li>Collecting, storing and analyzing log data from multiple sources</li> <li>Used to track health &amp; performance, and for debugging issues</li> <li>Approaches: Centralized, Distributed, Cloud based</li> </ul>"},{"location":"design_concepts/components/components#task-scheduler","title":"Task Scheduler","text":"<ul> <li>Used to automate execution of tasks at regular intervals<ul> <li>On a specific schedule or in response to certain events</li> </ul> </li> <li>Approaches<ul> <li>Standalone<ul> <li>Simple to implement, allows flexibility, complex to manage, requires infrastructure</li> </ul> </li> <li>Built-in task scheduler in container orchestration platforms or cloud-based serverless platforms<ul> <li>Simple to implement and mangage, less flexible</li> </ul> </li> <li>Cloud based like AWS SNS (Simple Notification Service), Google Cloud Scheduler<ul> <li>Highly scalable and fault-tolerant</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/components/components#api-gateway","title":"API Gateway","text":"<ul> <li>Simplifies the client interaction with the underlying services and enhances security<ul> <li>Routes requests to appropriate mircoservice or backend service</li> <li>Provides versioning and ensures backward compatibility</li> <li>Logging and monitoring, captures metadata like timestamps, client IPs, request headers</li> </ul> </li> <li>Security<ul> <li>SSL (Secure Socket Layer) is used to establish an encrypted link between the server and the client</li> <li>Handles SSL/TLS termination</li> <li>Rate limiting and throttling to prevent one service from overloading</li> <li>Rate limiting sets maximum number of requests per unit time from a client</li> <li>Throttling delays requests beyond the defined rate limit</li> </ul> </li> <li>Authentication<ul> <li>Authenticates users using username &amp; password, API keys, OAuth tokens, JWTs</li> <li>Verifies that the user or application has the necessary permissions</li> </ul> </li> <li>Serialization &amp; De-serialization of data<ul> <li>Transforms requests and responses as they pass through to ensure compatibility between services</li> <li>For example, converting JSON to XML and vice versa</li> <li>Can aggregate data from multiple services into a single response</li> </ul> </li> <li>Filtering<ul> <li>Filter and sort the required resources</li> <li>Default filters can be based on geographic factors</li> </ul> </li> <li>Pagination<ul> <li>Break the large set of resources into pages of a given size</li> <li>Makes it easy to query results and reduces load time</li> </ul> </li> </ul>"},{"location":"design_concepts/components/servers","title":"Servers","text":""},{"location":"design_concepts/components/servers#web-server","title":"Web Server","text":"<ul> <li>Hosts websites and serves requests from clients</li> <li>Types: shared host, dedicated host, custom host</li> <li>Other uses<ul> <li>Processing data sent from another server</li> <li>Hosting hypervisors (programs that manage virtual machines)</li> <li>Distributing and monitoring software updates</li> </ul> </li> </ul>"},{"location":"design_concepts/components/servers#working","title":"Working","text":"<ul> <li>Web client sends a request to the web server via HTTP</li> <li>If the data is present for the request in the static database within the web server<ul> <li>Then immediately the corresponding response is sent back</li> </ul> </li> <li>Else the web server sends a servlet request for processing to the application server<ul> <li>The application server looks up the application datastore<ul> <li>To run the servlet and fetch the required info</li> </ul> </li> <li>And sends back the servlet response to the web server</li> </ul> </li> </ul>"},{"location":"design_concepts/components/servers#application-server","title":"Application Server","text":"<ul> <li>Provides environment and computational power to run specific applications</li> <li>Required when intense computation is required which web server cannot handle</li> </ul>"},{"location":"design_concepts/components/servers#proxy-server","title":"Proxy Server","text":"<ul> <li>Intermediary hardware or software sitting between a client and a server<ul> <li>That filters, logs &amp; transforms requests</li> <li>E.g. adding/removing headers, encrypting/decrypting, compressing a resource</li> </ul> </li> <li>Proxies can sit on the client's local server side<ul> <li>And also between the client and remote servers</li> </ul> </li> <li>Acts as an intermediary<ul> <li>For requests from clients seeking resources from other servers</li> </ul> </li> <li>Two types: forward proxy, reverse proxy</li> </ul>"},{"location":"design_concepts/components/servers#features","title":"Features","text":"<ul> <li>Filtering<ul> <li>Looks addresses in its database of allowed or disallowed sites</li> <li>Filters out content or websites based on different policies</li> <li>Blocks malicious content and filters out dangerous websites</li> <li>Prevents unauthorized access to sensitive resources</li> </ul> </li> <li>Logging: Transforms requests using encryption, compression, etc.</li> <li>Caching: Caches frequently used pages, can serve a lot of requests</li> <li>Batching<ul> <li>Batches multiple client requests for the same URI<ul> <li>To be processed as one request to the backend server</li> </ul> </li> </ul> </li> <li>Coordinates requests from multiple servers<ul> <li>And optimizes request traffic from a system-wide perspective</li> </ul> </li> <li>Collapsed Forwarding<ul> <li>Collapses requests for similar data or data that is spatially close together in storage</li> <li>It minimizes the data reads and returns single result</li> </ul> </li> <li>Security<ul> <li>Provides network address translation (NAT)<ul> <li>Which makes individual users on the network anonymous</li> </ul> </li> <li>Makes it harder for hackers to access individual computers on the network</li> </ul> </li> <li>Though it can add latency due to all these extra features</li> </ul>"},{"location":"design_concepts/components/servers#forward-proxy","title":"Forward Proxy","text":"<ul> <li>Sits in front of one or more client computers<ul> <li>And serves as a conduit between clients and internet</li> </ul> </li> <li>Stores and forwards internet services like DNS or web pages<ul> <li>For users within a networking group</li> </ul> </li> <li>Working<ul> <li>Receives request from the client machine</li> <li>And sends it to the internet on the client's behalf</li> <li>On receiving a response from the internet, it forwards it to the client</li> </ul> </li> <li>Usage<ul> <li>To reduce and control the bandwidth used by the group</li> <li>To hide the IP of clients from the internet or web servers</li> <li>To block a specific website</li> </ul> </li> <li>Clients -&gt; Forward Proxy -&gt; Internet -&gt; Web Servers</li> </ul>"},{"location":"design_concepts/components/servers#reverse-proxy","title":"Reverse Proxy","text":"<ul> <li>Sits in front of one or more web servers<ul> <li>And serves as a conduit between web servers and internet</li> </ul> </li> <li>Working<ul> <li>Typically sits behind a firewall in a private network</li> <li>Directs client requests to an appropriate backend server</li> <li>Provides an additional level of abstraction and control</li> <li>Ensures a smooth flow of network traffic between the clients &amp; the servers</li> </ul> </li> <li>Used for<ul> <li>Traffic control and caching server response</li> <li>Reduces DDoS attacks because only proxy server is accessible to the outer world</li> <li>Provides SSL encyption that reduces the strain on web server</li> </ul> </li> <li>Clients -&gt; Internet -&gt; Reverse Proxy -&gt; Web Servers</li> </ul>"},{"location":"design_concepts/components/cdn_content_distribution_network","title":"CDN (Content Distribution Network)","text":"<ul> <li>Globally distributed network of servers<ul> <li>Designed to enhance the performance and availability of web content</li> </ul> </li> <li>CDNs reduce latency and accelerate the delivery of static assets<ul> <li>Like images, videos, scripts, main page of website</li> </ul> </li> <li>Stores copies of these assets on servers strategically positioned around world</li> <li>Automatically routes requests to the nearest server, reducing load time</li> <li>Offloads some traffic from the origin server and distributes to multiple servers</li> </ul>"},{"location":"design_concepts/components/cdn_content_distribution_network#factors","title":"Factors","text":"<ul> <li>Content Distribution<ul> <li>How the system should distribute web content material globally</li> <li>Includes techniques for replication &amp; synchronization of content material</li> <li>Ensuring that the latest content is updated across all CDN nodes</li> </ul> </li> <li>Caching<ul> <li>Specifying caching approach for each static and dynamic content material</li> <li>Figuring what content needs to be cached, for how long, cache eviction policy</li> <li>Keeping it coherent with database is a maintenance, solution is cache invalidation:<ul> <li>Write-through (to database and cache)</li> <li>Write-around (to database)</li> <li>Write-back (to cache and later to database)</li> </ul> </li> </ul> </li> <li>Load Balancing across CDN nodes</li> <li>Content Purge Mechanism<ul> <li>Mechanism to invalidate or update cached content material</li> <li>Situations under which content material should be purged</li> <li>How rapidly the purge must propogate</li> </ul> </li> <li>Content Optimization<ul> <li>Techniques for image compression and minification<ul> <li>To enhance the rate of content delivery</li> </ul> </li> <li>Techniques to optimize content and delivery for mobile devices<ul> <li>Considering responsive design and adaptive content</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/components/cdn_content_distribution_network#components","title":"Components","text":"<ul> <li>Edge Servers<ul> <li>Distributed servers located in proximity to end users</li> <li>Responsible for caching and delivering content quickly</li> </ul> </li> <li>Origin Server<ul> <li>Central repository where original content is stored and managed</li> <li>Serves as the source for content distribution</li> </ul> </li> <li>Content Distribution Nodes<ul> <li>Network nodes responsible for routing and optimizing content delivery within CDN</li> <li>Ensures efficient traffic management</li> </ul> </li> <li>Control Plane<ul> <li>Software or services that manage and orchestrate the CDN's operations</li> <li>Including content caching, routing, and load balancing</li> </ul> </li> </ul>"},{"location":"design_concepts/components/cdn_content_distribution_network#applications","title":"Applications","text":"<ul> <li>Streaming media: Delivering video and audio content with minimal buffering and lag</li> <li>Software distribution<ul> <li>Distributing software updates and patches efficiently to global users</li> <li>Providing faster and reliable downloads for software applications</li> </ul> </li> <li>E-commerce<ul> <li>Optimizing online shopping experience by ensuring fast and reliable content delivery</li> <li>For product images and descriptions</li> </ul> </li> <li>Gaming: Minimizing latency and providing seamless gameplay experience</li> <li>API delivery: Improving performance and scalability of APIs used by mobile apps &amp; other servies</li> </ul>"},{"location":"design_concepts/components/load_balancing","title":"Load Balancing","text":"<ul> <li>Distributes traffic across a cluster of servers<ul> <li>Prevents overloading any one server</li> <li>Improves availability and reliability</li> <li>Minimizes response time and maximizes throughput</li> <li>Provides horizontal scaling</li> </ul> </li> <li>Keeps track and performs health checks regularly<ul> <li>Stops sending signal to erred server and reduces downtime</li> </ul> </li> <li>Sits between client and server<ul> <li>And distributes the incoming traffic across multiple servers</li> <li>Prevents any one server from becoming a single point of failure</li> <li>Cluster of LB to avoid the LB being the single point of failure</li> </ul> </li> <li>Can be added between<ul> <li>Users and Servers</li> <li>Servers and Internal platform (application server, cache server, job server)</li> <li>Internal platform and Database</li> </ul> </li> </ul>"},{"location":"design_concepts/components/load_balancing#types-of-load-balancers","title":"Types of Load Balancers","text":"<ul> <li>Application LB<ul> <li>Designed to work with specific types of applications or protocols like HTTP or HTTPS</li> <li>Provides flexibility because it can be installed on any standard device</li> <li>Less expensive since no need to purchase or maintain the physical device</li> </ul> </li> <li>Global LB<ul> <li>Distributes traffic depending on the geographic regions</li> <li>Across multiple data centers or geographically distributed servers</li> <li>Considers factors like server proximity, server health</li> </ul> </li> <li>Virtual LB<ul> <li>Implemented as a virtual machine or software instance within a virtualized environment</li> <li>Such as data centers utilizing virtualization technologies like VMware, HyperV</li> </ul> </li> <li>Hardware Load Balancers<ul> <li>Forwards a request to the appropriate server doing bi-directional network address translation (NAT)</li> <li>Can handle a large traffic but are expensive &amp; has limited flexibility</li> <li>Due to this, mostly used as the first entry point for user requests</li> <li>Capable of handling all kinds of HTTP(S), TCP, UDP traffic</li> <li>Preferred by large enterprises for dedicated power</li> </ul> </li> <li>Hardware LB: Layer 4 (Network Layer)<ul> <li>Distributes requests based on the source &amp; destination IP addresses &amp; port numbers</li> <li>Can perform basic NAT to hide server addresses</li> </ul> </li> <li>Hardware LB: Layer 7 (Application Layer)<ul> <li>Distributes requests based on the content of the requests</li> <li>Such as URL, type of HTTP method, HTTP headers, cookies</li> <li>Capable for terminating SSL connections</li> </ul> </li> </ul>"},{"location":"design_concepts/components/load_balancing#load-balancing-algorithms","title":"Load Balancing Algorithms","text":""},{"location":"design_concepts/components/load_balancing#static-load-balancing","title":"Static Load Balancing","text":"<ul> <li>Predetermined assignment of tasks or resources without considering real-time variations</li> <li>Round Robin<ul> <li>Distribute requests equally one by one to servers<ul> <li>Simple to implement, no overhead</li> </ul> </li> <li>Suitable when servers have similar processing capabilities</li> <li>If a server is dead, it will be taken out of rotation</li> <li>It is a static LB, so there is no consideration of server load<ul> <li>A solution can be to periodically query load and adjust traffic</li> </ul> </li> </ul> </li> <li>Weighted Round Robin<ul> <li>Better suited to handle servers with different processing capacities</li> <li>Different servers are assigned weights and traffic is distributed accordingly</li> <li>The weights are based on processing capacity, server health, load, etc.</li> </ul> </li> <li>Source IP hash<ul> <li>Distributes requests based on the hash value of the source IP address</li> <li>Ensures that requests originating from the same IP<ul> <li>Are consistently directed to the same server</li> <li>Beneficial for applications that require maintaining session info or state</li> </ul> </li> <li>May lead to uneven load distribution if certain IPs are more active<ul> <li>Or if the IPs are not distributed evenly</li> </ul> </li> <li>Adding or removing servers may disrupt session persistence</li> </ul> </li> </ul>"},{"location":"design_concepts/components/load_balancing#dynamic-load-balancing","title":"Dynamic Load Balancing","text":"<ul> <li>Real-time decisions based on the changing conditions in the system</li> <li>Least Connection<ul> <li>Directs traffic to the server with the fewest active connections</li> <li>Need additional computation to identify the server with least connections</li> <li>May be costlier than round robin method</li> <li>Ignores capacity, server with fewer connections may still have less capacity</li> </ul> </li> <li>Least Response Time<ul> <li>Directs traffic to the server with the fewest active connectins<ul> <li>And the lowest average response time</li> </ul> </li> <li>Considers historical performance of server response times</li> <li>Adjusts any server changes by continuous monitoring</li> </ul> </li> <li>Least Bandwidth<ul> <li>Selects the server that is serving the least amount of traffic<ul> <li>Measured in Mbps (megabits per second)</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/components/caching","title":"Caching","text":"<ul> <li>Short term memory with fast access that contains most recently accessed items<ul> <li>Used in hardware, OS, web browsers, web applications</li> </ul> </li> <li>Not all information should be stored in cache<ul> <li>Since it's expensive and will increase search time</li> </ul> </li> <li>When an item is queried<ul> <li>Check in the cache, and return the result if present (cache hit)</li> <li>If not present (cache miss), it's fetched from the database and saved in the cache</li> </ul> </li> </ul>"},{"location":"design_concepts/components/caching#application","title":"Application","text":"<ul> <li>OS: kernel extensions, application files</li> <li>Browser: most visited websites</li> <li>Twitter: viral tweets, celebrity profiles</li> <li>Instagram: images keep loading but the text is displayed in slow network</li> </ul>"},{"location":"design_concepts/components/caching#eviction-policy","title":"Eviction policy","text":"<ul> <li>FIFO (First In First Out)</li> <li>LIFO (Last In First Out)</li> <li>LRU (Least Recently Used)</li> <li>MRU (Most Recently Used)</li> <li>LFU (Least Frequently Used)</li> <li>RR (Random Replacement)</li> </ul>"},{"location":"design_concepts/components/caching#cache-invalidation","title":"Cache Invalidation","text":"<ul> <li>If the original data changes, the cached version becomes outdated<ul> <li>Invalidation mechanism ensures that the outdated entries are refreshed or removed</li> <li>Can be done using time-based expiration, event-driven invalidation, etc.</li> </ul> </li> <li>Write-through cache<ul> <li>Data is written into the database and the cache at the same time</li> <li>Ensures data consistency and nothing is lost in case of crash, system disruptions, etc.</li> <li>But increases the latency of write operations</li> </ul> </li> <li>Write-around cache<ul> <li>Data is written directly to permanent storage, bypassing the cache</li> <li>Reduces the flooding of cache with write operation that may not be re-read</li> <li>But a read request for recently written data will have cache miss<ul> <li>Which may increase the latency</li> </ul> </li> </ul> </li> <li>Write-back cache<ul> <li>Data is written to cache alone and completion is immediately confirmed to the client</li> <li>The write to the permanent storage is done<ul> <li>After specified intervals or under certain conditions</li> </ul> </li> <li>This results in low latency and high throughput for write-intensive applications</li> <li>But this speed comes with the risk of data loss in case of a crash</li> </ul> </li> </ul>"},{"location":"design_concepts/components/caching#levels","title":"Levels","text":"<ul> <li>Application Server Cache<ul> <li>User requests are stored in cache and returned when the same request comes again</li> <li>Can be added in in-memory alongside the application server<ul> <li>If the number of results is small</li> </ul> </li> <li>If LB randomly distributes requests<ul> <li>It will go to different servers resulting in cache misses</li> <li>Can be solved using global cache, distributed cache</li> </ul> </li> </ul> </li> <li>Distributed Cache<ul> <li>Each node has a part of the whole cache space</li> <li>Using consistent hashing, each request is routed to where the cached request is found</li> </ul> </li> <li>Global Cache<ul> <li>One single cache space is used by all the nodes</li> <li>If a request is not found in the global cache<ul> <li>Approach 1: The cache finds out the missing piece of data from the database</li> <li>Approach 2: The node directly communicates with the database</li> </ul> </li> <li>One global cache can get overwhelmed by large number of requests</li> </ul> </li> <li>CDN (Content Distribution Network)<ul> <li>Group of servers that are geographically distributed over different locations</li> <li>Used when a large amount of static content is served by the website</li> </ul> </li> </ul>"},{"location":"design_concepts/components/consistent_hashing","title":"Consistent Hashing","text":"<ul> <li>Distributed hashing technique to achieve load balancing<ul> <li>Distributes keys uniformly across a cluster of nodes</li> <li>Minimizes the need of rehashing when nodes are added or removed</li> </ul> </li> <li>Represented by a virtual ring structure<ul> <li>Number of locations in this ring is not fixed</li> <li>Server nodes are hashed at random locations on this ring</li> <li>Request are also hashed on the same ring with the same hash function</li> </ul> </li> </ul>"},{"location":"design_concepts/components/consistent_hashing#requirement","title":"Requirement","text":"<ul> <li>With n servers, using an intuitive hash function like (key % n) has two major drawbacks</li> <li>Not horizontally scalable<ul> <li>When a new host is added, all existing mappings are broken</li> <li>And needs to be updated causing a downtime</li> </ul> </li> <li>Not load balanced<ul> <li>Can cause non-uniformly distributed data</li> <li>Some servers may have heavy load while others may remain idle or almost empty</li> </ul> </li> </ul>"},{"location":"design_concepts/components/consistent_hashing#working","title":"Working","text":"<ul> <li>Deciding a server for a request<ul> <li>Assuming that the clockwise traversal of the ring<ul> <li>Corresponds to the increasing order of location addresses</li> </ul> </li> <li>Each request can be served by the server node<ul> <li>That appears first while traversing clockwise</li> </ul> </li> <li>Only k/n keys need to be reassigned on adding or removing a node<ul> <li>Where k is total number of keys &amp; n is total number of nodes</li> </ul> </li> </ul> </li> <li>To map a key to server<ul> <li>Hash it to single integer</li> <li>Move clockwise on the ring until the first host is encountered<ul> <li>That host contains the key</li> </ul> </li> <li>When a host is added, reassign all the predecessor keys on the ring to the new host</li> <li>When a host is removed, reassign all the predecessor keys on the ring to next host</li> </ul> </li> <li>For load balancing, the real data is essentially randomly distributed<ul> <li>This may make the keys unbalanced due to non-uniform data</li> <li>To handle this issue and avoid non-uniform data from hash function<ul> <li>Virtual replicas are introduced</li> </ul> </li> <li>Instead of mapping each cache to a single point on the ring<ul> <li>Map it to multiple points on the ring, i.e. replicas</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/components/consistent_hashing#advantages","title":"Advantages","text":"<ul> <li>Consistent hashing is a good strategy<ul> <li>For distributed caching system and DHT (Distributed Hash Table)</li> </ul> </li> <li>Distributes data across cluster<ul> <li>That minimizes reorganization when nodes are added or removed</li> </ul> </li> <li>When hash table is resized (new host is added)<ul> <li>Only k/n keys need to be remapped</li> <li>Objects are mapped to same host if possible</li> <li>Takes share from a few host without touching other's shares</li> </ul> </li> <li>When host is removed<ul> <li>Objects on that host are shared by other hosts</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/databases","title":"Databases","text":""},{"location":"design_concepts/data_and_storage/databases#storage-types","title":"Storage Types","text":""},{"location":"design_concepts/data_and_storage/databases#block-storage","title":"Block Storage","text":"<ul> <li>Dividing data into fixed-sized blocks and storing them on block devices like hard drives or SSDs</li> <li>Accessed using low-level block protocols<ul> <li>Typically through Storage Area Networks (SAN) or Direct-Attached Storage (DAS)</li> <li>Block can be read or written directly</li> </ul> </li> <li>Low latency and high performance storage</li> <li>Suitable for databases, VMs, high performance applications</li> <li>Options<ul> <li>Amazon Elastic Block Store (EBS)</li> <li>Azure Virtual Machines (VMs) with managed discs service for block volumes</li> <li>Google Cloud zonal persistent disk</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/databases#object-storage-or-blob-storage","title":"Object Storage (or Blob Storage)","text":"<ul> <li>Files are divided into little parts and dispersed over hardware in a flat structure</li> <li>Typically use a RESTful API for accessing and managing data</li> <li>Highly scalable<ul> <li>Provides built-in redundancy and fault tolerance</li> <li>Can handle large number of requests concurrently</li> </ul> </li> <li>Each object can have an associated metedata for rich data management and search capabilities</li> <li>Suitable for large amounts of unstructured data<ul> <li>Like media files (documents, images, videos, audios), backups, log files</li> </ul> </li> <li>Can be used together with database systems to store and manage different types of data</li> <li>Options<ul> <li>Amazon Simple Storage Service (S3)</li> <li>Azure Blob Storage</li> <li>Google Cloud Storage buckets</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/databases#file-storage","title":"File Storage","text":"<ul> <li>Files stored under a hierarchical directory structure</li> <li>Accessed using file level protocols like Network File System (NFS) or Server Message Block (SMB)</li> <li>Can be implemented using Network Attached Storage (NAS) devices or distributed file systems</li> <li>Supports metadata like permissions, timestamps, file attributes</li> <li>Suitable for file systems and file based applications</li> <li>Options<ul> <li>Amazon Elastic File System (EFS)</li> <li>Azure Files</li> <li>Google Cloud Filestore</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/databases#databases_1","title":"Databases","text":""},{"location":"design_concepts/data_and_storage/databases#rdbms","title":"RDBMS","text":"<ul> <li>Relationship based tabular data model that requires a predefined schema</li> <li>Scalability<ul> <li>Excel in vertical scaling (by increasing horsepower like memory, cpu)</li> <li>Scaling across multiple servers is challenging and time-consuming</li> </ul> </li> <li>Why use SQL<ul> <li>ACID compliance: Reduces anomalies and protects integrity</li> <li>Data is structured &amp; unchanging, can be queried and accessed in a structured way</li> <li>Provides strong relationship between entities</li> </ul> </li> <li>Examples: MySQL, PostgreSQL, Oracle, SQL Server</li> <li>Data Examples: customer records, financial transactions</li> </ul>"},{"location":"design_concepts/data_and_storage/databases#nosql","title":"NoSQL","text":"<ul> <li>Unstructured data model using key-value, document, wide-column, graph</li> <li>Dynamic schema where columns can be added on the fly<ul> <li>Every row doesn't need to have data for each column</li> </ul> </li> <li>Scalability<ul> <li>Excel in horizontal scaling (by adding more servers easily)</li> </ul> </li> <li>Sacrifice ACID compliance for scalability and processing speed</li> <li>Instead of ACID, it complies with BASE<ul> <li>BASE: Basically Available, Soft State, Eventually Consistent</li> <li>Prioritizes availability and performance over strict consistency</li> </ul> </li> <li>Why use NoSQL<ul> <li>Data type is not a bottleneck</li> <li>Storing large volumes of data that have little structure</li> <li>Easily scale across multiple data centres</li> <li>Rapid development, no prep required, data structure can be updated easily</li> </ul> </li> <li>Examples: MongoDB, Cassandra, Redis</li> </ul>"},{"location":"design_concepts/data_and_storage/databases#key-value-stores","title":"Key Value Stores","text":"<ul> <li>Type of NoSQL database where each piece of data is stored under a unique key</li> <li>Used to store data that is accessed frequently</li> <li>Types<ul> <li>In-memory: Stored in memory for fast access</li> <li>Persistent: Stored on disk for durability</li> </ul> </li> <li>Used for caching, session management, real-time analytics, metadata</li> <li>Simpler to use and more scalable than other types of databases</li> <li>Not well suited for complex &amp; structured data that requires advanced querying capabilities</li> </ul>"},{"location":"design_concepts/data_and_storage/databases#indexing","title":"Indexing","text":"<ul> <li>Enhances the speed of data retrieval and finds rows like a library catalog</li> <li>Index columns that have large range of distinct values and are used frequently<ul> <li>Avoid indexing columns that are rarely read and frequently written to</li> </ul> </li> <li>Affects write performance: insert, update, delete<ul> <li>Since an index can become large due to additional keys</li> <li>While adding rows or making updates<ul> <li>We have to write the data as well as update the index</li> </ul> </li> <li>Avoid unnecessary indexes and delete the ones no longer being used</li> </ul> </li> <li>Categories<ul> <li>Single-level: Direct mapping between the index and the actual data</li> <li>Multi-level: Hierarchical layers with better performance, B &amp; B+ trees</li> <li>Clustered<ul> <li>Rows with similar values for the clustering key are stored collectively</li> <li>Aligns information rows based on the order of the clustering key</li> <li>Optimizes retrieval operations, in particular for range queries</li> <li>Insert &amp; update can be slower</li> </ul> </li> <li>Non-clustered<ul> <li>Has a unique value for each record</li> <li>Separate order for the index and the records</li> <li>Fast retrieval, insert &amp; update faster than clustered</li> </ul> </li> </ul> </li> <li>Data structures: B tree, B+ tree, Hashmaps, Bitmap</li> </ul>"},{"location":"design_concepts/data_and_storage/databases#eventual-consistency","title":"Eventual Consistency","text":"<ul> <li>Consistency model where all data replicas are eventually converged to a consistent state</li> <li>It allows the replicas to be inconsistent for a short period<ul> <li>To enable high availability and partition tolerance</li> </ul> </li> <li>Asynchronous updates</li> </ul>"},{"location":"design_concepts/data_and_storage/replication","title":"Replication","text":"<ul> <li>Sharing information to ensure consistency between redundant resources</li> <li>Creating and maintaining duplicate copies of a database on different servers</li> <li>Data remains accessible even if one server fails</li> <li>Improves data reliability<ul> <li>Since there are multiple copies to restore data in case of corruption or loss</li> </ul> </li> <li>Helps distributing the workload among servers improving performance and scalability</li> <li>Can be used to bring data closer to users reducing latency</li> </ul>"},{"location":"design_concepts/data_and_storage/replication#types","title":"Types","text":"<ul> <li>Master-Slave<ul> <li>Copy and synchronize data from a primary database (master)<ul> <li>To one or more secondary databases (slaves)</li> </ul> </li> <li>Master database is responsible for all write operations</li> <li>The changes made to the master database are replicated to the slave databases</li> </ul> </li> <li>Master-Master<ul> <li>There are multiple master databases</li> <li>Changes made to any master database are replicated to all other master databases</li> <li>If conflicting writes occur, conflict resolution mechanisms are needed</li> </ul> </li> <li>Snapshot<ul> <li>Creates a copy of the entire database at a specific point in time</li> <li>And then replicates that snapshot to one or more destination servers</li> <li>Typically done for reporting, backup, or distributed database purposes</li> </ul> </li> <li>Transactional<ul> <li>Keeping multiple copies of a database synchronized in real-time</li> <li>Any changes in one database (publisher) are immediately replicated to other databases (subscribers)</li> <li>Example: Live stock market with constantly changing prices</li> </ul> </li> <li>Merge<ul> <li>Allows both the central server (publisher)<ul> <li>And its connected devices (subscribers) to update data</li> </ul> </li> <li>With multiple parties editing the data, conflicts are bound to occur</li> <li>Pre-defined rules or user interventions are employed to resolve conflicting changes</li> <li>Example: Shared documents</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/replication#strategies","title":"Strategies","text":"<ul> <li>Defines how data is selected, copied and distributed across databases</li> <li>Full Replication<ul> <li>Copying the whole database ensuring that replicas have exact copies</li> <li>Example: Product catalog of e-commerce websites</li> </ul> </li> <li>Partial Replication<ul> <li>Only specific tables, row or columns are replicated</li> <li>Example: Most frequently accessed account information in financial institution</li> </ul> </li> <li>Selective Replication<ul> <li>Replicating data based on predefined criteria allowing granular control</li> <li>Example: Posts liked or shared by a large number of users on a social media platform</li> </ul> </li> <li>Hybrid Replication<ul> <li>Example: Healthcare organization<ul> <li>Full replication for critical patient data</li> <li>Partial replication for less critical data that is accessed occasionally</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/replication#configurations","title":"Configurations","text":"<ul> <li>Synchronous<ul> <li>Changes are replicated in real-time</li> <li>Transaction is not considered committed<ul> <li>Until at least one replica has acknowledged receiving the changes</li> </ul> </li> <li>Example: Banking applications</li> </ul> </li> <li>Asynchronous<ul> <li>Changes are sent to replicas without waiting for acknowledgement</li> <li>Allows fast processing of transactions at the cost of delay in consistency</li> <li>Example: Product inventory data</li> </ul> </li> <li>Semi-synchronous<ul> <li>Data changes are replicated to at least one replica synchronously<ul> <li>Ensuring consistency for critical data</li> </ul> </li> <li>Other replicas are updated asynchonously for better performance</li> <li>Example: Funds transfer</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/sharding","title":"Sharding (or Data Partitioning)","text":"<ul> <li>Splitting a large dataset into smaller chunks (or shards)<ul> <li>And distributing them across multiple machines</li> <li>Required when the database or the table becomes large and affects performance</li> </ul> </li> <li>Technique for horizontal scaling of databases<ul> <li>After a certain point, it is cheaper and more feasible to scale horizontally</li> <li>Rather than growing it vertically by adding powerful servers</li> </ul> </li> <li>Benefits<ul> <li>Manageability</li> <li>Performance: Reduces query times since it has to go through fewer rows</li> <li>Availability: If an outage happens, only the specific shards will be affected</li> <li>Load balancing</li> </ul> </li> <li>Challenges<ul> <li>Complicated task and can result in data loss if not implemented properly</li> <li>Sometimes shards become unbalanced (may be it outgrows or is more frequently accessed)</li> <li>Joining data from multiple shards may be expensive</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/sharding#considerations","title":"Considerations","text":"<ul> <li>How to split the data across shards</li> <li>How to balance the data across shards as the amount of data changes over time</li> <li>How the queries will be directed to the correct shard<ul> <li>Either by using a dedicated routing layer or by including the shard info in the query</li> </ul> </li> <li>How the system will handle the failure of one or more shards<ul> <li>Including recovery and data redistribution</li> </ul> </li> <li>How the sharded database will perform in terms of read &amp; write speed</li> </ul>"},{"location":"design_concepts/data_and_storage/sharding#partitioning-methods","title":"Partitioning Methods","text":"<ul> <li>Horizontal<ul> <li>Row and range based</li> <li>Examples<ul> <li>Sharding based on the first letter of names</li> <li>Sharding based on the created_at timestamp</li> </ul> </li> <li>Choose ranges carefully to avoid unbalanced servers<ul> <li>E.g. names starting with letters in shard A-M may be more than in M-Z</li> </ul> </li> </ul> </li> <li>Vertical<ul> <li>Splitting an entire column(s) from a table<ul> <li>Or hosting similar tables on the same server</li> </ul> </li> <li>Some shards may contain highly accessed tables or columns leading to uneven workload</li> <li>Examples<ul> <li>Keep users and profile on the same server and photos to different servers</li> <li>Split preference based columns from user table to a different table</li> </ul> </li> </ul> </li> <li>Directory Based<ul> <li>Maintains a lookup service<ul> <li>That stores the mapping for each entity to the database servers</li> </ul> </li> <li>Can dynamically scale by adding or removing shards<ul> <li>Without any changes to the application logic</li> </ul> </li> <li>The central directory that stores the mappings can be a single point of failure</li> <li>Introduces an additional layer of overhead for referring the directory</li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/sharding#partitioning-criteria","title":"Partitioning Criteria","text":"<ul> <li>Key or hash based<ul> <li>An attribute or a group of attributes<ul> <li>Are selected to work as the input for hash function</li> </ul> </li> <li>Applying the hash function to this input yields a partition number<ul> <li>The function should always return the same partition for a given input</li> </ul> </li> <li>Adding a new server will require changing hash function and data redistribution<ul> <li>This data redistribution can be minimized using consistent hashing</li> </ul> </li> <li>It can become a bottleneck<ul> <li>If the sharding key is not well-distributed</li> <li>Or if certain keys are accessed more frequently</li> </ul> </li> </ul> </li> <li>List<ul> <li>Each partition is assigned a list of values</li> <li>For example, users from a particular cities will be stored in a particular partition</li> </ul> </li> <li>Round robin<ul> <li>Data is evenly distributed across partitions in a cyclic manner</li> <li>Each partition is assigned the next available data item sequentially</li> <li>May result in uneven data distribution and inefficient data retrieval</li> <li>Does not optimize specific query patterns or access patterns</li> </ul> </li> <li>Composite<ul> <li>Combining any of the above partioning schemes</li> <li>E.g. first apply a list partitioning scheme and then a hash based partioning</li> <li>Consistent hashing could be considered a composite of hash and list partitioning<ul> <li>Where the hash reduces the key space to a size that can be listed</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/sharding#challenges","title":"Challenges","text":"<ul> <li>Joins and denormalization<ul> <li>Joins can be costly after sharding<ul> <li>Since data has to be compiled from multiple servers</li> </ul> </li> <li>Workaround: Denormalize database so that queries can be performed in single table</li> <li>Issue: Perils of denormalization like data inconsistency</li> </ul> </li> <li>Referential integrity<ul> <li>Enforcing data integrity constraints like foreign keys can be difficult</li> <li>Most RDBMS don't support multi-server foreign keys<ul> <li>So they have to be enforced in application code</li> </ul> </li> <li>In such cases applications have to run regular SQL jobs<ul> <li>To clean up the dangling references</li> </ul> </li> </ul> </li> <li>Rebalancing<ul> <li>We may need to change our sharding scheme<ul> <li>Maybe the data distribution is not uniform</li> <li>Or there is a lot of load on a shard</li> </ul> </li> <li>Hence, we may have to create more shards or rebalance existing shards<ul> <li>And move the existing data to new locations</li> </ul> </li> <li>Doing this without downtime is extremely difficult</li> <li>Using a scheme like directory based paritioning can make it easier<ul> <li>But at the cost of increasing the complexity of the system</li> <li>And creating a single point of failure (for lookup service/database)</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/data_processing","title":"Data Processing","text":""},{"location":"design_concepts/data_and_storage/data_processing#etl-pipeline","title":"ETL pipeline","text":"<ul> <li>Extract Transform Load</li> <li>Set of processes which<ul> <li>Extract the data from an input source</li> <li>Transform the data</li> <li>Load the data into an output destination such as datamart, database and data warehouse<ul> <li>For analysis, reporting and data synchronization</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/data_and_storage/data_processing#data-processing-pipeline","title":"Data processing pipeline","text":"<ul> <li>Set of processing elements that moves the data from one system to another<ul> <li>Transforming the data along the way</li> </ul> </li> </ul>"},{"location":"design_concepts/others/devops","title":"Devops","text":""},{"location":"design_concepts/others/devops#devops-life-cycle","title":"Devops Life Cycle","text":"<ul> <li>Documenting and determining project structure and schedule</li> <li>Analyzing project requirements and collecting resources</li> <li>Designing software model with architecture and interface designing</li> <li>Developing an actual product by considering design &amp; requirements</li> <li>Testing the build, fixing errors &amp; bugs, refactoring the code</li> <li>Software deployment and monitoring for further enhancements</li> </ul>"},{"location":"design_concepts/others/devops#cicd","title":"CI/CD","text":"<ul> <li>Practices about how an integrated code on a shared repository is used to release software to production<ul> <li>Deployment can be done multiple times a day with the help of automation</li> <li>Error detection and response are fast due to short cycle and iterations</li> </ul> </li> <li>Continuous Integration (CI)<ul> <li>Regularly build, test, and merge changes to a shared staging repository</li> <li>Using version control to manage conflicts from too many branches at once</li> </ul> </li> <li>Continuous Deployment (CD)<ul> <li>Continuous Delivery<ul> <li>Automatically testing changes for bugs and uploading to staging repository</li> </ul> </li> <li>Automatically releasing changes from staging repository to production</li> <li>Automatically deploying application after testing and build stages</li> </ul> </li> </ul>"},{"location":"design_concepts/others/devops#container-management-platforms","title":"Container Management Platforms","text":"<ul> <li>Manages creation, deployment, scaling, availability, and destruction of software containers</li> <li>Benefits<ul> <li>Solves the problem of moving software from one computing environment or OS to another</li> <li>Serves as a self isolated unit that can run anywhere that supports it, regardless of the host OS</li> <li>Packages application code and associated dependencies and configurations into a virtual container</li> </ul> </li> <li>Containers like Docker encapsulate the mircoservice and its dependencies<ul> <li>Allows to package applications into lightweight &amp; portable containers</li> <li>Encapsulates everything required to run the application: code, runtime, libraries, system tools</li> <li>Ensures consistency across different environments</li> </ul> </li> <li>Orchestration tools like Kubernetes manage the deployment, scaling, operation of containers<ul> <li>Also provides features for container scheduling, service discovery, load balancing, etc.</li> </ul> </li> </ul>"},{"location":"design_concepts/others/testing","title":"Testing","text":""},{"location":"design_concepts/others/testing#quality-assurance","title":"Quality Assurance","text":"<ul> <li>Multi-testing strategy</li> <li>Automation and script based testing</li> <li>Measurement and reporting mechanism</li> <li>Formal techical reviews</li> </ul>"},{"location":"design_concepts/others/testing#unit-testing","title":"Unit Testing","text":"<ul> <li>Focuses on individual units (functions, procedures)<ul> <li>Typically performed or automated by developers</li> </ul> </li> <li>Isolates sections of code and verifies its correctness</li> <li>Helps to fix bugs early in the development cycle and enables to make changes quickly</li> <li>Types<ul> <li>Black Box Testing: Covers iput, user interface &amp; output</li> <li>White Box Testing: Tests the functional behavior and internal design structure</li> <li>Gray Box Testing: Uses partial knowledge of the internal structure</li> </ul> </li> <li>Cannot cover non functional parameters like scalability &amp; performance</li> <li>Not sufficient for testing interactions between units</li> </ul>"},{"location":"design_concepts/others/testing#integration-testing","title":"Integration Testing","text":"<ul> <li>Focuses on interfaces and interactions between two units or modules</li> <li>Types<ul> <li>Big Bang IT<ul> <li>All the modules are put together and tested</li> <li>Useful only for small systems, becomes complex for large systems</li> </ul> </li> <li>Bottom Up IT<ul> <li>Lower level modules are tested with higher level modules until all the modules are tested</li> </ul> </li> <li>Top Down IT<ul> <li>Simulates the behavior of the lower level modules that are not yet integrated</li> <li>Needs many stubs and difficult to observe outputs</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_concepts/others/testing#regression-testing","title":"Regression Testing","text":"<ul> <li>Used to ensure that the new changes do not introduce new bugs or breaks functionality</li> <li>Types<ul> <li>Retesting: Testing entire application or specific functionality affected by the changes</li> <li>Re-execution: Running a previously executed test suite<ul> <li>To ensure that the changes did not break any existing functionality</li> </ul> </li> <li>Comparision: Comparing the current version with a previous version</li> </ul> </li> </ul>"},{"location":"design_concepts/others/testing#security-testing","title":"Security Testing","text":"<ul> <li>To identify and protect against vulnerabilities that can be exploited by attackers</li> <li>Authentication &amp; Authorization: Unauthorized access, weak passwords, data breaches</li> <li>Application: Unpatched software, misconfigured systems, data encryption &amp; leakage</li> <li>Network: Firewalls, routers, denial of service (DoS), man in the middle</li> <li>Database: SQL injection, cross-site scripting</li> </ul>"},{"location":"design_concepts/others/testing#other-types","title":"Other Types","text":"<ul> <li>Smoke Testing<ul> <li>Initial testing to check that the system is ready and stable for further testing</li> <li>Checks that the system did not catch fire and there is no smoke</li> </ul> </li> <li>Alpha Testing: Validation testing within the organization</li> <li>Beta Testing: Testing conducted at customer sites</li> <li>System Testing: Security testing, stress &amp; performance testing, recovery testing</li> </ul>"},{"location":"design_concepts/others/dsa_for_system_design","title":"DSA for System Design","text":"<ul> <li>Consistent Hashing</li> <li>Vector Clocks</li> <li>Paxos Algorithm</li> <li>Map Reduce</li> <li>Distributed Hash Tables (DHT)</li> <li>Gossip Protocol</li> <li>Quorum-based replication</li> </ul>"},{"location":"design_concepts/others/dsa_for_system_design#dsa-examples","title":"DSA Examples","text":"<ul> <li>Hash tables for caching frequently requested web pages</li> <li>Graphs for social networks<ul> <li>Network of friends can be represented as a graph</li> <li>BFS traversal can be used to find connections between users &amp; suggest new connections</li> </ul> </li> <li>Trie for autosuggestions in search engines or messaging apps<ul> <li>Helps to predict and suggest most likely words or phrases as user types</li> </ul> </li> <li>Priority queues for task scheduling</li> <li>Dijkstra's algorithm for routing in GPS navigation systems</li> <li>Binary search for searching records based on a unique identifier</li> <li>Segment tree for range queries<ul> <li>Can be utilized to efficiently calculate total revenue in financial systems</li> </ul> </li> </ul>"},{"location":"design_concepts/others/dsa_for_system_design#maintain-concurrency-and-parallelism","title":"Maintain Concurrency and Parallelism","text":"<ul> <li>Concurrency: Executing multiple task in overlapping time periods</li> <li>Parallelism: Simultaneous execution of multiple tasks<ul> <li>By dividing into subtasks that can be processed concurrently</li> </ul> </li> <li>Concurrency Control: Locks (read-write locks), Mutexes, Semaphores</li> <li>Atomic Operations</li> <li>Task Scheduling</li> <li>Pipeline Processing</li> <li>Parallel Reduction: Aggregate data from multiple sources</li> <li>Fork Join: Divide a task into subtasks, execute concurrently, combine the results</li> </ul>"},{"location":"design_patterns/introduction","title":"Design Patterns","text":"<ul> <li>Blueprint to commonly occuring problems in software design that can be customized</li> <li>The patterns are general concepts that can be applied to solve specific problems</li> <li>The code of the same pattern applied to two different programs may be different</li> </ul>"},{"location":"design_patterns/introduction#types-of-design-patterns","title":"Types of Design Patterns","text":"<ul> <li>Creational: Object creation mechanisms that provide flexibility and reusability</li> <li>Structural: How to assemble objects and classes into larger structures<ul> <li>And keeping these structures flexible and efficient</li> </ul> </li> <li>Behavioral: Concerned with algorithms and assignment of responsibility among objects</li> </ul>"},{"location":"design_patterns/introduction#creational","title":"Creational","text":"<ul> <li>Factory: Create objects of derived classes</li> <li>Abstract Factory: Create objects of families of classes</li> <li>Builder: Complex object construction with different representations</li> <li>Prototype: Clone a fully initialized object</li> <li>Singleton: Class for which only a single object exists</li> </ul>"},{"location":"design_patterns/introduction#structural","title":"Structural","text":"<ul> <li>Adapter: Collaboration of objects with incompatible interfaces</li> <li>Bridge: Split a large class or related classes into two separate independent hierarchies<ul> <li>Abstraction &amp; implementation</li> </ul> </li> <li>Composite: Compose objects into tree structures as if they were individual objects</li> <li>Decorator: Attach new behaviors to objects using wrapper objects</li> <li>Facade: Simplified interface to a library, framework or a set of classes</li> <li>Flyweight: Share common states among objects instead of keeping all the data in each object</li> <li>Proxy: Placeholder for original object<ul> <li>Control access to the original object</li> <li>Perform pre or post request tasks</li> </ul> </li> </ul>"},{"location":"design_patterns/introduction#behavioral","title":"Behavioral","text":"<ul> <li>Chain of Responsibility: Pass requests along a chain of handlers<ul> <li>That decide to process or pass to the next handler</li> </ul> </li> <li>Command: Encapsulate a command request as an object</li> <li>Interpreter: Include language elements in a program</li> <li>Iterator: Traverse elements of a collection without exposing its underlying representation</li> <li>Mediator: Restrict direct communication and reduce chaotic dependencies between the objects<ul> <li>Via a mediator object</li> </ul> </li> <li>Memento: Capture and restore the previous internal state of objects</li> <li>Observer: Subscription to notify objects about events of the object under observation</li> <li>State: Alter an object's behavior when its state changes as if the object changed its class</li> <li>Strategy: Encapsulate a family of algorithms inside classes with interchangeable objects</li> <li>Template: Defer the exact steps of an algorithm to a subclass without changing its structure</li> <li>Visitor: Defines a new operation to a class without change</li> </ul>"},{"location":"design_patterns/solid_principles","title":"SOLID Principles","text":"<ul> <li>The SOLID principle help in reducing tight coupling<ul> <li>Tight coupling means a group of classes are highly dependent on one another</li> </ul> </li> <li>Our code should be loosely coupled<ul> <li>Loosely coupled classes minimize changes in the code</li> <li>Helps in making code more reusable, maintainable, flexible and stable</li> </ul> </li> </ul>"},{"location":"design_patterns/solid_principles#single-responsibility","title":"Single Responsibility","text":"<ul> <li>A class should have only one responsibility<ul> <li>In other words, only one reason to change</li> </ul> </li> <li>Importance<ul> <li>Less coupling and fewer dependencies</li> <li>Well organized classes, easier to understand and modify</li> <li>Better testing due to fewer test cases</li> </ul> </li> <li>Example<ul> <li>Imagine a baker who is responsible for baking bread<ul> <li>Ensuring that the bread is of high quality and properly baked</li> </ul> </li> <li>If the baker is also responsible for other tasks, it will violate single responsibilty<ul> <li>Like managing the inventory, ordering supplies, serving customers, etc.</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/solid_principles#open-for-extension-closed-for-modification","title":"Open for Extension, Closed for Modification","text":"<ul> <li>Software entities (classes, modules, functions, etc.)<ul> <li>Should be open for extension, but closed for modification</li> <li>We should be able to extend a class behavior without modifying it</li> </ul> </li> <li>Importance<ul> <li>New features can be added without modifying existing code</li> <li>Adapts to changing requirements more easily</li> <li>Saves testing the existing implementations and avoids potential bugs</li> </ul> </li> <li>Example<ul> <li>Imagine we have a PaymentProcessor class to processes payments for an online store<ul> <li>Initially, it only supported processing payments using credit cards</li> <li>Now, we want to extend its functionality to also support PayPal</li> </ul> </li> <li>Instead of modifying the PaymentProcessor class to add PayPal support<ul> <li>Create a new class PaypalPaymentProcessor that extends the PaymentProcessor class</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/solid_principles#liskov-substitution","title":"Liskov Substitution","text":"<ul> <li>Functions that use pointers or references to base classes<ul> <li>Must be able to use objects of derived classes without knowing it</li> </ul> </li> <li>If class B is a subtype (or child) of class A<ul> <li>We should be able to replace A with B<ul> <li>Without disrupting the behavior of program</li> </ul> </li> </ul> </li> <li>Importance<ul> <li>Enables the use of polymorphic behavior, making code more flexible and reusable</li> <li>Ensures that subclasses adhere to the contract defined by the superclass</li> <li>Guarantees that replacing a superclass object with a subclass object won't break the program</li> </ul> </li> <li>Example<ul> <li>A rectangle\u2019s can have any height and width</li> <li>A square is a rectangle with equal width and height</li> <li>So the properties of the rectangle class should be extendable into square class</li> </ul> </li> </ul>"},{"location":"design_patterns/solid_principles#interface-segregation","title":"Interface Segregation","text":"<ul> <li>Clients should not be forced to depend upon interfaces that they do not use<ul> <li>Prefer many client interfaces rather than one general interface<ul> <li>Larger interfaces should be split into smaller ones</li> <li>Each interface should have a specific responsibility</li> </ul> </li> <li>The implementing class should only be concerned about the methods that are connected</li> </ul> </li> <li>Importance<ul> <li>Reduces dependencies between classes, making the code more modular and maintainable</li> <li>Allows for more targeted implementations of interfaces</li> <li>Avoids unnecessary dependencies, clients don't have to depend on methods they don't use</li> </ul> </li> <li>Example<ul> <li>Suppose we enter a restaurant and we are pure vegetarian</li> <li>But the menu card includes vegetarian, non-vegetarian, drinks, sweets, etc.</li> <li>The menu should be different for different types of customers</li> </ul> </li> </ul>"},{"location":"design_patterns/solid_principles#dependency-inversion","title":"Dependency Inversion","text":"<ul> <li>Instead of high level modules depending on low level modules<ul> <li>Both should depend on abstractions</li> </ul> </li> <li>Additionally, abstractions should not depend on details, details should depend on abstractions<ul> <li>Classes should rely on abstractions (e.g., interfaces or abstract classes)<ul> <li>Rather than concrete implementations</li> </ul> </li> </ul> </li> <li>Importance<ul> <li>Reduces dependencies between modules, making the code more flexible and easier to test</li> <li>Enables changes to implementations without affecting clients</li> <li>Makes code easier to understand and modify</li> </ul> </li> <li>Example<ul> <li>Developers depend on an abstract version control system like git</li> <li>They don\u2019t depend on specific details of how git works internally</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/factory","title":"Factory","text":"<ul> <li>Also known as Virtual Constructor</li> <li>Provides an interface for creating objects in a superclass<ul> <li>But allows subclasses to alter the type of objects that will be created</li> </ul> </li> <li>Allows creating product objects without specifying their concrete classes<ul> <li>Avoids tight coupling between the creators and the products</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/factory#problem","title":"Problem","text":"<ul> <li>Let's say we're creating a logistics management application<ul> <li>The app handles transportation by trucks</li> <li>So the bulk of the code is in the Truck class</li> </ul> </li> <li>After a while, we receive requests to incorporate sea logistics into the app<ul> <li>Adding a new Ship would require a lot of changes</li> <li>Since the the code is coupled to the Truck class</li> </ul> </li> <li>Adding yet another type of transportation will require making these changes again<ul> <li>This will introduce many conditionals to switch the app's behavior<ul> <li>Depending on the class of the transportation objects</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/creational/factory#solution","title":"Solution","text":"<ul> <li>Replace the direct object construction calls with calls to a special factory method<ul> <li>Objects returned by a factory method are often referred as products</li> </ul> </li> <li>This allows us to override the factory method in a subclass<ul> <li>And change the class of products being created by the method</li> <li>But these products from all the subclasses should have a common base class or interface</li> </ul> </li> <li>The Truck class and the Ship class implement the Transport interface<ul> <li>The Transport interface declares a method called deliver</li> <li>Truck implements it to deliver by land, Ship implements it to deliver by sea</li> </ul> </li> <li>The code that uses the factory method<ul> <li>Doesn\u2019t see a difference between the products returned by various subclasses</li> <li>It knows that all transport objects are supposed to have the deliver method<ul> <li>But exactly how it works isn\u2019t important to it</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/creational/factory#example","title":"Example","text":"Ruby <pre><code># Creator\n# Declares the factory method that returns an object of a product class. The subclasses\n# provide the implementation, but it may provide a default implementation.\n# Its primary responsibility isn't creating products, it usually contains some core\n# business logic that relies on product objects returned by the factory method.\n# Subclasses can indirectly change that business logic by overriding the factory method\n# and returning a different type of product.\nclass Transport\n  def deliver\n    raise('Not Implemented Error')\n  end\nend\n\n# Concrete Creator\n# Implements the creator interface\nclass Truck &lt; Transport\n  def deliver(parcel_details)\n    Box.new(parcel_details)\n  end\nend\n\nclass Ship &lt; Transport\n  def deliver(parcel_details)\n    Container.new(parcel_details)\n  end\nend\n\n# Product\n# Declares the interface common for all objects that are produced by the creators\nclass Parcel\n  def pick\n    raise('Not Implemented Error')\n  end\nend\n\n# Concrete Product\n# Implements the product interface\nclass Box &lt; Parcel\n  def pick\n  end\nend\n\nclass Container &lt; Parcel\n  def pick\n  end\nend\n\n# Client Code\ndef pick_parcel(transporter, parcel_details)\n  parcel = transporter.deliver(parcel_details)\n  parcel.pick\nend\n\nparcel_details = { height: '1m', width: '1m', weight: '10kg' }\ntransporter = Truck.new\npick_parcel(transporter, parcel_details)\n</code></pre>"},{"location":"design_patterns/creational/abstract_factory","title":"Abstract Factory","text":"<ul> <li>Allows to produce families of related objects without specifying their concrete classes</li> <li>Products generated form a factory are compatible to each other</li> <li>Decoupling between concrete products and client code</li> </ul>"},{"location":"design_patterns/creational/abstract_factory#problem","title":"Problem","text":"<ul> <li>Let's say we want to sell furnitures like chair, sofa, etc.<ul> <li>The furnitures are available in two variants: modern, traditional</li> </ul> </li> <li>We need a way to create individual furniture objects<ul> <li>So that they match other objects of the same family</li> <li>If a customer orders modern furniture, all the items like chair &amp; sofa should be modern</li> </ul> </li> <li>Furniture vendors update their catalogs very often<ul> <li>So we wouldn\u2019t want to change the core code when adding new items or variants</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/abstract_factory#solution","title":"Solution","text":"<ul> <li>Explicitly declare interfaces for each distinct product (chair, sofa)<ul> <li>Then make all the variants of the product follow these interfaces</li> <li>So, all the chair variants will implement the Chair interface</li> </ul> </li> <li>After that, declare the abstract factory<ul> <li>An interface with a list of creation methods of all the products</li> <li>For each variant of a product, create a factory class based on the abstract factory</li> </ul> </li> <li>The client code works with both factories and products via their abstract interfaces<ul> <li>This lets us change the type of a factory passed to the client code<ul> <li>And the product variant that the client code receives</li> </ul> </li> <li>The client shouldn\u2019t care about the factory class it works with</li> </ul> </li> <li>If a client wants a factory to produce a chair<ul> <li>It doesn't have to be aware of the factory's class</li> <li>It should treat all the variants of the chair in the same manner</li> <li>It should match the variant of other products like sofa produced by the same factory object</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/abstract_factory#example","title":"Example","text":"<pre><code># Abstract Factory\n# Interface that declares separate methods to return different abstract products.\n# These products are called a family and are related by a high-level theme or concept.\n# Products of one family are usually able to collaborate among themselves.\n# A family of products may have several variants, but the products of one variant are\n# incompatible with products of another.\nclass Furniture\n  def build_chair\n    raise('Not Implemented')\n  end\n\n  def build_sofa\n    raise('Not Implemented')\n  end\nend\n\n# Concrete Factory\n# Produces a family of products that belong to a single variant\nclass ModernFurniture &lt; Furniture\n  def build_chair\n    ModernChair.new\n  end\n\n  def build_sofa\n    ModernSofa.new\n  end\nend\n\nclass TraditionalFurniture &lt; Furniture\n  def build_chair\n    TraditionalChair.new\n  end\n\n  def build_sofa\n    TraditionalSofa.new\n  end\nend\n\n# Abstract Product\nclass Chair\nend\n\n# Concrete Product\nclass ModernChair &lt; Chair\nend\n\nclass TraditionalChair &lt; Chair\nend\n\n# Abstract Product\nclass Sofa\nend\n\nclass ModernSofa &lt; Sofa\nend\n\nclass TraditionalSofa &lt; Sofa\nend\n\n# Client Code\ndef build_furniture(factory)\n  factory.build_chair\n  factory.build_sofa\nend\n\nfactory = ModernFurniture.new\nbuild_furniture(factory)\n</code></pre>"},{"location":"design_patterns/creational/builder","title":"Builder","text":"<ul> <li>Constructs complex objects<ul> <li>That require step by step initialization of many fields and nested objects</li> </ul> </li> <li>Allows to produce different types and representations of an object<ul> <li>Using the same construction code</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/builder#problem","title":"Problem","text":"<ul> <li>Imagine a complex object<ul> <li>That requires step-by-step initialization of many fields and nested objects</li> <li>Such initialization is usually inside a large constructor with lots of parameters</li> <li>It can become too complex by creating subclasses for every possible configuration</li> </ul> </li> <li>Let's say we want to create a House object<ul> <li>To build a simple house, we need to construct walls, floor, door, windows, roof</li> <li>After that we will also need to add plumbling, electrical wiring, etc.</li> <li>The client may also want backyard, heating system, air conditioning</li> </ul> </li> <li>The simplest solution is to extend the base House class and create subclasses<ul> <li>But eventually we'll end up with a lot of subclasses to cover all the combinations</li> <li>Any new parameters like porch style will keep growing this heirarchy</li> </ul> </li> <li>Another solution can be to create a large constructor in the base House class<ul> <li>But in most cases, many of the parameters will be unused</li> <li>Because only a fraction of house will have swimming pools</li> <li>This will make the constructor calls ugly</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/builder#solution","title":"Solution","text":"<ul> <li>Extract the object construction out of its own class<ul> <li>And move it to separate objects called builders</li> </ul> </li> <li>Organize object construction into a set of steps<ul> <li>That can be executed on a builder object</li> </ul> </li> <li>Some steps might require different implementation to build various representations<ul> <li>Walls of a cabin must be built of wood</li> <li>Walls of a castle walls must be built with stone</li> </ul> </li> <li>Create several different builder classes<ul> <li>That implement the same set of building steps but in a different manner</li> <li>Call only those steps that are necessary for a particular configuration</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/builder#director","title":"Director","text":"<ul> <li>We can go further and extract a series of calls to a director class</li> <li>Director class defines the order to execute the building steps<ul> <li>While the builder provides the implementation for those steps</li> </ul> </li> <li>It extracts the details of product construction from the client code<ul> <li>The client only needs to associate a builder with a director</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/builder#example","title":"Example","text":"<pre><code># Builder\n# Declares product construction steps common to all types of builders\nclass HouseBuilder\n  def build_walls\n    raise('Not Implemented')\n  end\n\n  def build_floor\n    raise('Not Implemented')\n  end\nend\n\n# Concrete Builder\n# Provides different implementations for the construction steps\ndef CabinBuilder &lt; HouseBuilder\n  def build_walls\n    cut_wood_logs\n  end\nend\n\ndef CastleBuilder &lt; HouseBuilder\n  def build_walls\n    break_rocks_into_stones\n  end\nend\n\n# Product\n# The resulting objects contructed by different builders\ndef Cabin\nend\n\ndef Castle\nend\n\n# Director\ndef Director\n  attr_reader :builder\n\n  def initialize(builder)\n    @builder = builder\n  end\n\n  def build_room\n    builder.build_walls\n    builder.build_floor\n  end\n\n  def build_swimming_pool\n  end\nend\n\n# Client Code\ndef build\n  builder = CabinBuilder.new\n  director = Director.new(builder)\n  director.build_room\nend\n</code></pre>"},{"location":"design_patterns/creational/prototype","title":"Prototype","text":"<ul> <li>Also known as Clone</li> <li>Helps to copy existing objects without making the code dependent on their classes</li> <li>It's easier to copy complex object rather than constructing them from scratch</li> </ul>"},{"location":"design_patterns/creational/prototype#problem","title":"Problem","text":"<ul> <li>To create an exact copy of an object<ul> <li>We've to create a new object of the same class</li> <li>Then go through all the fields and copy their values to the new object</li> </ul> </li> <li>But not all objects can be copied this way<ul> <li>Some of the fields may be private and not visible from outside of the object</li> <li>Also, it makes the code dependent on the object class<ul> <li>Since it's required to create the new object</li> </ul> </li> <li>Sometimes, we only know the interface that the object follows<ul> <li>But not its concrete class</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/creational/prototype#solution","title":"Solution","text":"<ul> <li>Delegate the cloning process to the actual objects that are being cloned<ul> <li>An object that supports cloning is called a prototype</li> </ul> </li> <li>Declare a common interface for all objects that support cloning<ul> <li>Without coupling the code to the object class</li> <li>Usually contains a single clone method</li> </ul> </li> <li>The clone method creates an object of the class<ul> <li>And carries over all the field values of the old object into the new one</li> <li>Enables to copy private fields because most programming languages<ul> <li>Let objects access private fields of other objects of the same class</li> </ul> </li> </ul> </li> <li>When objects have dozens of fields and hundreds of possible configurations<ul> <li>Cloning them might serve as an alternative to subclassing</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/prototype#example","title":"Example","text":"<pre><code>class Prototype\n  attr_accessor :component, :reference\n\n  def clone\n    @component = deep_copy(@component)\n    @reference = deep_copy(@reference)\n    @reference.prototype = self\n    deep_copy(self)\n  end\n\n  private\n\n    def deep_copy(object)\n      # The marshaling library converts collections of ruby objects into a byte stream\n      # allowing them to be stored outside the currently active script. This data may\n      # subsequently be read and the original objects reconstituted.\n      Marshal.load(Marshal.dump(object))\n    end\nend\n\n# Client Code\ndef clone_component\n  prototype = Prototype.new\n  prototype.component = Component.new\n  prototype.reference = Reference.new\n  prototype.clone\nend\n</code></pre>"},{"location":"design_patterns/creational/singleton","title":"Singleton","text":"<ul> <li>Ensures that a class has only one instance and provides a global access point to this instance</li> <li>Requires special attention in a multi-threaded environment</li> <li>Violates the single responsibility principle</li> </ul>"},{"location":"design_patterns/creational/singleton#problem","title":"Problem","text":"<ul> <li>Need to ensure that a class has a single instance<ul> <li>The most common reason for this is to control access to some shared resource<ul> <li>Like a database or a file</li> </ul> </li> <li>This also prevents the code from being scattered, keeping it within one class</li> </ul> </li> <li>Provide a secure global access point to that instance<ul> <li>We use global variables to store some essential objects</li> <li>They are handy but also unsafe since any code can potentially overwrite them</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/singleton#solution","title":"Solution","text":"<ul> <li>Make the default constructor private to prevent multiple instances</li> <li>Create a static creation method that acts as a constructor<ul> <li>Under the hood, it calls the private constructor</li> <li>Creates an object and saves it in a static field</li> <li>All following calls to this method return the cached object</li> </ul> </li> <li>Just like a global variable, the singleton pattern<ul> <li>Allows accessing the object from anywhere in the program</li> <li>But it also protects that instance from being overwritten by other code</li> </ul> </li> </ul>"},{"location":"design_patterns/creational/singleton#example","title":"Example","text":"<pre><code># Naive Singleton\n# It\u2019s easy to implement a sloppy Singleton, just need to hide the constructor and\n# implement a static creation method. But it behaves incorrectly in a multithreaded\n# environment. Multiple threads can call the creation method simultaneously and get\n# several instances of Singleton class.\nclass Scheduler\n  private_class_method :new\n  @instance = new\n\n  def self.instance\n    @instance\n  end\n\n  def schedule_event\n  end\nend\n\n# Thread-Safe Singleton\n# To fix the multi-thread problem, synchronize threads during the first creation of the\n# Singleton object\nclass Scheduler\n  private_class_method :new\n  @instance_mutex = Mutex.new\n\n  attr_reader :mode\n\n  def initialize(mode)\n    @mode = mode\n  end\n\n  def self.instance(mode)\n    return @instance if @instance\n\n    @instance_mutex.synchronize do\n      @instance ||= new(mode)\n    end\n\n    @instance\n  end\n\n  def schedule_event\n  end\nend\n\n# Client Code\ndef scheduler_mode(mode)\n  scheduler = Scheduler.instance(mode)\n  scheduler.mode\nend\n\ndef process_threads\n  thread1 = Thread.new do\n    scheduler_mode('a') # returns 'a'\n  end\n\n  thread2 = Thread.new do\n    scheduler_mode('b') # returns 'a'\n  end\n\n  # Join the new threads to the main thread\n  # So that the main thread waits for thread1 &amp; thread2 to finish\n  thread1.join\n  thread2.join\nend\n</code></pre>"},{"location":"design_patterns/structural/adapter","title":"Adapter","text":"<ul> <li>Also known as Wrapper</li> <li>Allows objects with incompatible interfaces to collaborate</li> </ul>"},{"location":"design_patterns/structural/adapter#problem","title":"Problem","text":"<ul> <li>Let's say we're creating a stock market monitoring app<ul> <li>It downloads the stock data from multple sources in XML format</li> <li>And displays charts &amp; diagrams for the user</li> </ul> </li> <li>Later, we decide to improve the app by integrating a third party analytics library<ul> <li>But this library only works with data in JSON format</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/adapter#solution","title":"Solution","text":"<ul> <li>Create an adapter that converts the interface of one object so that<ul> <li>Another object can understand it</li> </ul> </li> <li>An adapter wraps one of the objects to hide the complexity of conversion<ul> <li>The wrapped object isn\u2019t even aware of the adapter</li> </ul> </li> <li>Adapters can also help objects with different interfaces collaborate<ul> <li>In addition to converting data into various formats</li> <li>It gets an interface compatible with one of the existing objects</li> <li>Using this interface, the existing object safely calls the adapter\u2019s methods</li> <li>Upon receiving a call, the adapter passes the request to the second object<ul> <li>But in a format and order that the second object expects</li> </ul> </li> </ul> </li> <li>Sometimes it\u2019s even possible to create a two-way adapter<ul> <li>That can convert the calls in both directions</li> </ul> </li> <li>For the stock market app, we can create XML-to-JSON adapters<ul> <li>For every class of the analytics library that the code works with directly</li> <li>The code will communicate with the library only via these adapters</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/adapter#example","title":"Example","text":"<pre><code># Target\nclass Address\n  def state_code\n  end\nend\n\n# Adaptee\n# Needs some adaptation before the client code can use it\nclass Location\n  def state\n  end\nend\n\n# Adapter\n# Makes the adaptee's interface compatible with the target's interface\nclass AddressLocationAdapter &lt; Address\n  def initialize(location)\n    @location = location\n  end\n\n  def state_code\n    state_code_from_state(@location.state)\n  end\nend\n\n# Client Code\ndef state_code(location)\n  address = AddressLocationAdapter.new(location)\n  address.state_code\nend\n\nlocation = Location.new\nstate_code(location)\n</code></pre>"},{"location":"design_patterns/structural/bridge","title":"Bridge","text":"<ul> <li>Splits a large class or a set of closely related classes<ul> <li>Into two seperate hierarchies - abstraction &amp; implementation</li> <li>So that they can be developed independently of each other</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/bridge#problem","title":"Problem","text":"<ul> <li>Let's say we have a geometric Shape class with a pair of subclasses Circle &amp; Square</li> <li>We want to extend this class heirarchy to incorporate colors Red &amp; Blue<ul> <li>So we'll need to create four class combinations<ul> <li>RedCircle, RedSquare, BlueCircle, BlueSquare</li> </ul> </li> </ul> </li> <li>Adding new shapes or new colors will increase the hierarchy exponentially<ul> <li>To add Triangle shape, we would need one subclass for each color</li> <li>Adding a new color would require creating one subclass for each shape</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/bridge#solution","title":"Solution","text":"<ul> <li>This problem occurs because we\u2019re trying to extend the shape classes in<ul> <li>Two independent dimension - by form and by color</li> <li>That\u2019s a very common issue with class inheritance</li> </ul> </li> <li>Switching from inheritance to the object composition can solve this problem<ul> <li>Extract one of the dimensions into a separate class hierarchy</li> <li>So that the original classes will reference an object of the new hierarchy</li> <li>Instead of having all of its state and behaviors within one class</li> </ul> </li> <li>Create a Color class with two subclasses - Red and Blue<ul> <li>The Shape class then gets a reference field pointing to one of the color objects</li> <li>The reference will act as a bridge between the Shape and Color classes</li> <li>Any color related logic can be delegated to the linked color object</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/bridge#abstraction-implementation","title":"Abstraction &amp; Implementation","text":"<ul> <li>This is different from the interfaces or abstract classes of a programming language</li> <li>Abstraction (also called interface) is a high-level control layer for some entity<ul> <li>It isn\u2019t supposed to do any real work on its own</li> <li>It should delegate the work to the implementation layer (also called platform)</li> </ul> </li> <li>For example, consider a web application<ul> <li>It has several different GUIs for different types of users (customers, admins)<ul> <li>And supports several different APIs (to launch the app for different OS)</li> </ul> </li> <li>We can create subclasses for all the combinations of GUI &amp; API<ul> <li>But it will grow the class heirarchy exponentially if new GUIs or APIs are added</li> </ul> </li> <li>Instead, we can divide the classes into two hierarchies<ul> <li>Abstraction: the GUI layer of the app</li> <li>Implementation: the operating system APIs</li> <li>The abstraction object controls the appearance of the app<ul> <li>Delegating the actual work to the linked implementation object</li> <li>This allows to change the GUI classes without touching the API classes</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/structural/bridge#example","title":"Example","text":"<pre><code># Abstraction\nclass Shape\n  attr_reader :color\n\n  def initialize(color)\n    # Reference to the object of the implementation heirarchy\n    # To which all the color related work will be delegated\n    @color = color\n  end\n\n  def draw\n    raise('Not Implemented')\n  end\nend\n\n# Extended Abstraction\nclass Square &lt; Shape\n  def draw\n    pen = color.load_pen\n    pen.draw_line\n  end\nend\n\nclass Circle &lt; Shape\nend\n\n# Implementation\nclass Color\nend\n\n# Concrete Implementation\nclass Red &lt; Color\nend\n\nclass Blue &lt; Color\nend\n\n# Client Code\ndef draw_red_square\n  color = Red.new\n  shape = Square.new(color)\n  shape.draw\nend\n</code></pre>"},{"location":"design_patterns/structural/composite","title":"Composite","text":"<ul> <li>Also known as Object Tree</li> <li>Composes objects into tree structures<ul> <li>And works with these structures as if they were individual objects</li> </ul> </li> <li>Used only when the core model of an app can be represented as a tree</li> </ul>"},{"location":"design_patterns/structural/composite#problem","title":"Problem","text":"<ul> <li>Let's say we want to create an ordering system that uses Products &amp; Boxes<ul> <li>A box can contain several products as well as other smaller boxes</li> <li>The smaller boxes can also hold some products or even smaller boxes and so on</li> </ul> </li> <li>To determine the total price of an order<ul> <li>The direct approach will be to unwrap all the boxes and go over all the products</li> <li>But the nesting level and other nasty details can make it complicated</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/composite#solution","title":"Solution","text":"<ul> <li>Create a common interface that can calculate the total price<ul> <li>For a product, it will simply consider the product's price</li> <li>For a box, it will go over each item and ask its price</li> </ul> </li> <li>The smaller boxes can individually go over their contents and get the price<ul> <li>This way we don't need to worry about how the boxes calculate their price</li> <li>And a box can also add some extra cost like packaging cost</li> </ul> </li> <li>We don\u2019t need to care about the concrete classes of objects<ul> <li>The objects themselves pass the request down the tree</li> <li>If a box is simple or sophisticated or gift wrapped<ul> <li>It can handle these criterias on its own</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/structural/composite#example","title":"Example","text":"<pre><code># Component (Common Interface)\n# Declares common operations for both simple and complex objects of a composition\nclass Component\n  attr_accessor :parent\n\n  def add(component)\n    raise('Not Implemented')\n  end\n\n  def remove(component)\n    raise('Not Implemented')\n  end\n\n  def composite?\n    false\n  end\n\n  def price\n    raise('Not Implemented')\n  end\nend\n\n# Leaf\n# End objects of a composition, can't have any children.\n# Usually, it's the leaf objects that do the actual work, whereas Composite\n# objects only delegate to their sub-components.\nclass Product &lt; Component\nend\n\n# Composite\n# Complex components that may have children\n# Usually delegates the actual work to the children and sum up the results\nclass Box &lt; Component\n  def initialize\n    @children = []\n  end\n\n  def add(component)\n    @children.append(component)\n    component.parent = self\n  end\n\n  def remove(component)\n    @children.remove(component)\n    component.parent = nil\n  end\n\n  def composite?\n    true\n  end\n\n  def price\n  end\nend\n\n# Client Code\ndef get_price\n  product1 = Product.new\n  box1 = Box.new\n  box1.add(product1)\n\n  product2 = Product.new\n  box2 = Box.new\n  box2.add(product2)\n\n  box = Box.new\n  box.add(box1)\n  box.add(box2)\n  box.add(product3)\n  box.price\nend\n</code></pre>"},{"location":"design_patterns/structural/decorator","title":"Decorator","text":"<ul> <li>Also known as Wrapper</li> <li>Attaches new behaviors to objects<ul> <li>By placing them inside special wrapper objects that contain the behaviors</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/decorator#problem","title":"Problem","text":"<ul> <li>Let's say we're working on a notification library<ul> <li>It lets programs notify their users about important events</li> </ul> </li> <li>We create a Notifier class with a few fields, a constructor, and a single send method<ul> <li>The method accepts a message and sends it to a list of emails passed via constructor</li> <li>A third party app can create and configure a notifier object once<ul> <li>And use it to send notifications</li> </ul> </li> </ul> </li> <li>Later, we realize that some users want to receive SMS about critical issues<ul> <li>Other users might want to be notified on facebook or slack</li> </ul> </li> <li>We can extend the Notifier class and create new subclasses<ul> <li>This will require the client to instantiate the required notification class</li> <li>But some users might prefer a combination of these channels</li> <li>This will bloat both the library and the client code</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/decorator#solution","title":"Solution","text":""},{"location":"design_patterns/structural/decorator#inheritance","title":"Inheritance","text":"<ul> <li>Extending a class is the first thing that comes to mind to alter an object's behavior</li> <li>However, inheritance has several caveats<ul> <li>Inheritance is static<ul> <li>The behavior of an existing object cannot be altered at runtime</li> </ul> </li> <li>Subclasses can have just one parent class<ul> <li>Multiple inheritance is not supported in most languages</li> </ul> </li> </ul> </li> <li>One of the ways to overcome these caveats is by using Aggregation or Composition</li> </ul>"},{"location":"design_patterns/structural/decorator#aggregation-or-composition","title":"Aggregation or Composition","text":"<ul> <li>Both of the alternatives work almost the same way<ul> <li>One object has a reference to another and delegates it some work</li> </ul> </li> <li>Inheritance<ul> <li>Object B inherits behavior from object A</li> <li>A itself does the work</li> </ul> </li> <li>Aggregation<ul> <li>Object A contains object B</li> <li>B can live without A</li> </ul> </li> <li>Composition<ul> <li>Object A consists of object B</li> <li>A manages life cycle of B, B can't live without A</li> </ul> </li> <li>With this new approach, the linked helper object can be substituted with another<ul> <li>Changing the behavior of the container at runtime</li> <li>An object can use the behavior of various classes<ul> <li>Having references to multiple objects and delegate them different kinds of work</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/structural/decorator#wrapper","title":"Wrapper","text":"<ul> <li>Wrapper is the alternative name for the Decorator pattern<ul> <li>That clearly expresses the main idea of the pattern</li> <li>A wrapper is an object that can be linked with some target object</li> <li>The wrapper contains the same set of methods as the target<ul> <li>And delegates to it all requests it receives</li> </ul> </li> <li>However, the wrapper may alter the result by doing something<ul> <li>Either before or after it passes the request to the target</li> </ul> </li> </ul> </li> <li>When does a simple wrapper becomes the real decorator<ul> <li>The wrapper implements the same interface as the wrapped object</li> <li>That\u2019s why from the client\u2019s perspective these objects are identical</li> <li>Make the wrapper\u2019s reference field accept any object that follows that interface</li> <li>This will let us cover an object in multiple wrappers<ul> <li>Adding the combined behavior of all the wrappers to it</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/structural/decorator#notifications","title":"Notifications","text":"<ul> <li>Leave the simple email notification behavior inside the base Notifier class<ul> <li>But turn all other notification methods into decorators</li> </ul> </li> <li>The client code would need to wrap a basic notifier object<ul> <li>Into a set of decorators that match the client\u2019s preferences</li> <li>The resulting objects will be structured as a stack</li> </ul> </li> <li>The last decorator in the stack<ul> <li>Would be the object that the client actually works with</li> <li>Since all decorators implement the same interface as the base notifier<ul> <li>The rest of the client code won\u2019t care whether it works with<ul> <li>The pure notifier object or the decorated one</li> </ul> </li> </ul> </li> </ul> </li> <li>The same approach can be applied to other behaviors<ul> <li>Such as formatting messages or composing the recipient list</li> <li>The client can decorate the object with any custom decorators<ul> <li>As long as they follow the same interface as the others</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/structural/decorator#example","title":"Example","text":"<pre><code># Component\nclass Data\n  def write\n    raise('Not Implemented')\n  end\nend\n\n# Concrete Component\nclass FileData &lt; Data\n  def write\n  end\nend\n\n# Decorator\nclass DataDecorator &lt; Data\n  attr_accessor :data\n\n  def initialize(data)\n    @data = data\n  end\n\n  def write\n    data.write\n  end\nend\n\n# Concrete Decorator\nclass EncryptionDecorator &lt; DataDecorator\n  def write\n    encrypt_data\n    data.write\n  end\nend\n\nclass CompressionDecorator &lt; DataDecorator\n  def write\n    compress_data\n    data.write\n  end\nend\n\n# Client Code\n# The client code can support both simple components as well as decorated ones\ndef write_data(data)\n  file_data = FileData.new(data)\n  encyption_data = EncyptionDecorator.new(file_data)\n  compression_data = CompressionDecorator.new(encryption_data)\n  compression_data.write\nend\n</code></pre>"},{"location":"design_patterns/structural/facade","title":"Facade","text":"<ul> <li>Provides a simplified interface to a library, framework, or complex set of classes<ul> <li>That contains a lot of moving parts</li> </ul> </li> <li>Provides limited functionality<ul> <li>Includes only those features that the client cares about</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/facade#problem","title":"Problem","text":"<ul> <li>Let's say our code works with a broad set of objects<ul> <li>That belong to a sophisticated library or framework</li> <li>We would need to initialize all these objects<ul> <li>Keep track of dependencies</li> <li>Execute methods in the correct order</li> </ul> </li> </ul> </li> <li>This can result in tight coupling of<ul> <li>The business logic of the classes and the implementation details of libraries</li> <li>Making it hard to comprehend and maitain the code</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/facade#solution","title":"Solution","text":"<ul> <li>Create a class that provides a simple interface to a complex subsystem<ul> <li>Implements a specifc workflow of a large library</li> <li>Doesn't affect the business logic of the client</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/facade#example","title":"Example","text":"<pre><code># Third Party Library\nmodule MultiMediaLibrary\n  class Video\n    def initialize(file)\n      @video = process(file)\n      @audio = Audio.new(@video)\n    end\n  end\n\n  class Audio\n  end\n\n  class Compressor\n    def compress_video(video)\n    end\n\n    def compress_audio(audio)\n    end\n  end\nend\n\n# Facade\nclass VideoCompressor\n  def compress(file)\n    video = MultiMediaLibrary::Video.new(file)\n    MultiMediaLibrary::Compressor.new.compress_video(video)\n  end\nend\n\n# Client Code\ndef compress_video(file)\n  VideoCompressor.new(file).compress\nend\n</code></pre>"},{"location":"design_patterns/structural/flyweight","title":"Flyweight","text":"<ul> <li>Also known as Cache</li> <li>Fits more objects into the available amount of RAM by sharing common parts of state<ul> <li>Between multiple objects instead of keeping all the data in each object</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/flyweight#problem","title":"Problem","text":"<ul> <li>Let's say we're creating a game where<ul> <li>Players would be moving around a map and shooting each other</li> <li>Vast quantities of bullets, missiles, shrapnel from explosions<ul> <li>Should fly all over the map delivering a thrilling experience</li> </ul> </li> </ul> </li> <li>Each particle such as a bullet or a missile<ul> <li>Is represented by a separate object containing plenty of data</li> <li>At some point, newly created particles may no longer fit into the remaining RAM</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/flyweight#solution","title":"Solution","text":""},{"location":"design_patterns/structural/flyweight#analysis","title":"Analysis","text":"<ul> <li>On inspection of the Particle class, let's say we notice that<ul> <li>The color and sprite fields consume a lot more memory than other fields</li> <li>And these two fields store almost identical data across all particles</li> <li>For example, all bullets have the same color and sprite</li> </ul> </li> <li>Other parts of a particle\u2019s state are unique to each particle<ul> <li>Such as coordinates, movement vector, speed</li> <li>The values of these fields change over time</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/flyweight#intrinsic-and-extrinsic-state","title":"Intrinsic and Extrinsic State","text":"<ul> <li>The constant data of an object is usually called the intrinsic state<ul> <li>It lives within the object</li> <li>Other objects can only read it, not change it</li> </ul> </li> <li>The rest of the object\u2019s state is called the extrinsic state<ul> <li>Which is often altered from the outside by other objects</li> </ul> </li> <li>Stop storing the extrinsic state inside the object<ul> <li>Instead, pass this state to specific methods which rely on it</li> <li>Only the intrinsic state stays within the object that can be reused</li> </ul> </li> <li>As a result, we would need fewer of these objects<ul> <li>Since they only differ in the intrinsic state</li> <li>Which has much fewer variations than the extrinsic</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/flyweight#particle-state","title":"Particle State","text":"<ul> <li>Extract the extrinsic state from the particle class<ul> <li>Only three different objects would suffice to represent all particles<ul> <li>A bullet, a missile, and a piece of shrapnel</li> </ul> </li> <li>An object that only stores the intrinsic state is called a flyweight</li> <li>They should be immutable since they can be used in different contexts</li> </ul> </li> <li>Move the extrinsic state to the container object<ul> <li>In this case, it's the main Game object<ul> <li>That stores all particles in the particles field</li> </ul> </li> </ul> </li> <li>To move it, we need to create several array fields<ul> <li>For storing coordinates, vectors, and speed of each individual particle</li> <li>We need another array for storing references to a specific flyweight<ul> <li>That represents a particle</li> </ul> </li> <li>These arrays must be in sync<ul> <li>So that the particle's data can be accessed using the same index</li> </ul> </li> </ul> </li> <li>A more elegant solution is to create a separate context class<ul> <li>That would store the extrinsic state along with reference to the flyweight object</li> <li>This would require having just a single array in the container class</li> </ul> </li> <li>The most memory-consuming fields have been moved to just a few flyweight objects<ul> <li>A thousand small contextual objects can reuse a single heavy flyweight object<ul> <li>Instead of storing a thousand copies of its data</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/structural/flyweight#example","title":"Example","text":"<pre><code># Flyweight\n# Stores the intrinsic state shared between the individual objects\n# Need only one object per tree type\n# Accepts the extrinsic state via method parameters\nclass TreeType\n  # Instrinsic state\n  def initialize(name:, color:, texture:)\n  end\n\n  # Extrinsic state\n  def draw(canvas, x, y)\n  end\nend\n\n# Flyweight Factory\n# Creates and manages flyweight objects\n# Decides whether to re-use existing flyweight or create a new object\nclass TreeFactory\n  attr_reader :tree_types\n\n  def initialize(tree_type_states)\n    # Flyweights, objects with only instrinsic state\n    @tree_types = {}\n\n    tree_type_states.map |state|\n      name = state_name(state)\n      @tree_types[name] = TreeType.new(state)\n    end\n  end\n\n  def state_name(state)\n    state.sort.join('_')\n  end\n\n  # Returns an existing flyweight with a given state or creates a new one\n  def tree_type(shared_state)\n    name = state_name(shared_state)\n    tree_types[name] ||= TreeType.new(shared_state)\n    tree_types[name]\n  end\nend\n\n# Contextual Object\n# Stores the extrinsic state like coordinates\n# And reference field to flyweight (tree_type)\nclass Tree\n  attr_reader :x, :y, :tree_type\n\n  def initialize(x, y, tree_type)\n    @x = x\n    @y = y\n    @tree_type = tree_type\n  end\n\n  def draw(canvas)\n    tree_type.draw(canvas, x, y, tree_type)\n  end\nend\n\nclass Forest\n  attr_reader :tree_factory, :trees\n\n  def initialize(tree_type_states)\n    @trees = []\n    @tree_factory = TreeFactory.new(tree_type_states)\n  end\n\n  def plant_tree(x, y, tree_state)\n    tree_type = tree_factory.tree_type(tree_state)\n    tree = Tree.new(x, y, tree_type)\n    trees.append(tree)\n  end\n\n  def draw(canvas)\n    trees.each do |tree|\n      tree.draw(canvas)\n    end\n  end\nend\n</code></pre>"},{"location":"design_patterns/structural/proxy","title":"Proxy","text":"<ul> <li>Provides a substitute or placeholder for another object</li> <li>Controls access to the original object<ul> <li>Allowing to perform something before or after the request get through to the original object</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/proxy#problem","title":"Problem","text":"<ul> <li>Let's say we have a massive object that consumes a vast amount of system resources<ul> <li>It is required from time to time but not always</li> </ul> </li> <li>We can implement lazy initialization<ul> <li>Create this object only when it's required</li> <li>All of the object's clients would need to execute some deferred initialization code</li> <li>This would cause a lot of code duplication</li> </ul> </li> <li>We can put this code directly into the object's class but that isn't always possible<ul> <li>For example, the class may be part of a closed third party library</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/proxy#solution","title":"Solution","text":"<ul> <li>Create a proxy class with the same interface as the original service object</li> <li>Update the app such that it passes the proxy object to all the original object's clients</li> <li>On receiving a request from a client<ul> <li>The proxy creates a real service and delegates all the work to it</li> </ul> </li> <li>If anything needs to be executed before or after the primary logic of the class<ul> <li>The proxy can handle it without changing that class</li> <li>Lazy initialization, logging, access control, caching, etc.</li> </ul> </li> <li>Since the proxy implements the same interface<ul> <li>It can be passed to any client that expects a real service object</li> </ul> </li> </ul>"},{"location":"design_patterns/structural/proxy#example","title":"Example","text":"<pre><code># Subject Interface\n# Declares common operations for both Subject and Proxy\nclass Image\n  def download\n    raise('Not Implemented')\n  end\nend\n\n# Subject\nclass RealImage &lt; Image\n  def download\n  end\nend\n\n# Proxy\n# The most common applications are lazy loading, caching, access control, logging, etc.\nclass ProxyImage &lt; Image\n  def initialize(real_image)\n    @real_image = real_image\n  end\n\n  def download\n    return unless access_allowed?\n\n    @real_image.download\n    log_access\n  end\nend\n\n# Client Code\ndef download_image(image)\n  image.download\nend\n\nreal_image = RealImage.new\nproxy = ProxyImage.new(real_image)\ndownload_image(proxy)\n</code></pre>"},{"location":"design_patterns/behavioral/chain_of_responsibility","title":"Chain of Responsibility","text":"<ul> <li>Also known as Chain of Command</li> <li>Passes requests along a chain of handlers</li> <li>Upon receiving a request, each handler decides to either process the request<ul> <li>Or pass it to next handler in the chain</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/chain_of_responsibility#problem","title":"Problem","text":"<ul> <li>Let's say we're working on an online ordering system<ul> <li>We want to restrict access to the system so that only authenticated users can create orders</li> <li>User with administrative permissions must have full access to all orders</li> </ul> </li> <li>After some planning, we realized that these checks must be performed sequentially<ul> <li>Sanitize the data in a request because it's unsafe to pass raw data straight to the ordering system</li> <li>Filter repeated failed request from the same IP to restrict brute force password cracking</li> <li>Speed up the system by returning cached results on repeated requests with the same data</li> </ul> </li> <li>This leads to a complex checking system and changing one check affects the others</li> <li>Also, it can lead to duplicate code if a component require some of these checks but not all</li> </ul>"},{"location":"design_patterns/behavioral/chain_of_responsibility#solution","title":"Solution","text":"<ul> <li>Transform particular behaviors into stand-alone objects called handlers</li> <li>Each check should be extracted to its own class with a single method that performs the check</li> <li>Each linked handler has a field for storing a reference to the next handler in the chain</li> <li>The request travels along the chain until all handlers process it</li> <li>A handler can decide not to pass the request further down and effectively stop further processing</li> </ul>"},{"location":"design_patterns/behavioral/chain_of_responsibility#example","title":"Example","text":"<pre><code># Handler\nclass Policy\n  def next_policy=(policy)\n    raise('Not Implemented')\n  end\n\n  def validate(order)\n    raise('Not Implemented')\n  end\nend\n\n# Abstract Handler\nclass OrderPolicy &lt; Policy\n  def next_policy=(policy)\n    @next_policy = policy\n    policy\n  end\n\n  def validate(order)\n    return true if @next_policy.blank?\n\n    @next_policy.validate(order)\n  end\nend\n\n# Concrete Handler\nclass WeightPolicy &lt; OrderPolicy\n  def validate(order)\n    if order.weight &lt;= order.type.weight_limit\n      # Execute next policy\n      super(order)\n    else\n      false\n    end\n  end\nend\n\nclass PaymentPolicy &lt; OrderPolicy\n  def validate(order)\n    if order.payment_details\n      # Execute next policy\n      super(order)\n    else\n      false\n    end\n  end\nend\n\n# Client Code\ndef validate_orders(policy)\n  weight_policy = WeightPolicy.new\n  payment_policy = PaymentPolicy.new\n  weight_policy.next_policy = payment_policy\n\n  orders.each do |order|\n    weight_policy.validate(order)\n  end\nend\n</code></pre>"},{"location":"design_patterns/behavioral/command","title":"Command","text":"<ul> <li>Also known as Action, Transaction</li> <li>Turns a request into a stand-alone object that contains all the information about the request</li> <li>This lets us<ul> <li>Pass requests as method arguments</li> <li>Delay or queue a request's execution</li> <li>Support undoable operations</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/command#problem","title":"Problem","text":"<ul> <li>Let's say we're working on a new text editor app<ul> <li>We want to create a toolbar with a bunch of buttons for various operations</li> </ul> </li> <li>We create a Button class<ul> <li>But while all the buttons look similar they're supposed to do different things<ul> <li>For example, submit, cancel, apply, save, open, print</li> </ul> </li> <li>Also, where will the click handlers of these buttons be stored?</li> </ul> </li> <li>The simplest solution will be to create subclasses<ul> <li>But that will create a lot of subclasses</li> <li>And there's a risk of breaking the code in these subclasses each time the Button class is modified</li> </ul> </li> <li>Also, some operations like copy &amp; paste can be invoked from multiple places<ul> <li>Like button on toolbar or via context menu or pressing Ctrl + C</li> <li>This will result in duplication of the operation's code to these places<ul> <li>Or make menus dependent on buttons</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/command#solution","title":"Solution","text":"<ul> <li>Principle of separation of concerns: Breaking an app into layers<ul> <li>GUI layer should render buttons &amp; capture inputs</li> <li>When clicked, it should send a request to the business logic layer</li> <li>Business logic layer carries out the operation like copy or paste</li> </ul> </li> <li>The command pattern suggests that GUI objects shouldn't send these requests directly<ul> <li>Extract all the request details (object being called, method name, arguments) into a command class</li> <li>With a single method to trigger this request</li> <li>Command objects serve as links between GUI and business logic objects</li> </ul> </li> <li>The commands should implement the same interface<ul> <li>Usually it has just a single execution method that takes no parameters<ul> <li>How to pass the request details to the receiver?</li> <li>The command should be either pre-configured with this data, or capable of getting it on its own</li> </ul> </li> <li>It lets us use various commands with the same request sender<ul> <li>Without coupling it to concrete classes of commands</li> </ul> </li> <li>As a bonus, we can switch command objects linked to the sender<ul> <li>Effectively changing the sender\u2019s behavior at runtime</li> </ul> </li> </ul> </li> <li>Now we can just put a single field into the base Button class<ul> <li>That stores a reference to a command object and make the button execute that command on a click</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/command#example","title":"Example","text":"<pre><code># Command\nclass Command\n  def initialize(app:, editor:)\n    @app = app\n    @editor = editor\n  end\n\n  def backup\n    @backup = @editor.text\n  end\n\n  def undo\n    @editor.text = @backup\n  end\n\n  def execute\n    raise('Not Implemented')\n  end\nend\n\n# Concrete Command\nclass Copy &lt; Command\n  def execute\n    @app.clipboard = editor.get_selection\n  end\nend\n\nclass Cut &lt; Command\n  def execute\n    backup\n    @app.clipboard = editor.get_selection\n    editor.delete_selection\n  end\nend\n\nclass Paste &lt; Command\n  def execute\n    backup\n    editor.replace_selection(@app.selection)\n  end\nend\n\nclass Undo &lt; Command\n  def execute\n    @app.undo\n  end\nend\n\n# Receiver\nclass Editor\n  def get_selection\n  end\n\n  def delete_selection\n  end\n\n  def replace_selection\n  end\nend\n\n# Invoker\nclass Application\n  def initialize\n    @editor = Editor.new\n  end\n\n  def copy\n    Copy.new(self, @editor).execute\n  end\nend\n</code></pre>"},{"location":"design_patterns/behavioral/interpreter","title":"Interpreter","text":"<ul> <li>Defines a way to interpret and evaluate language grammer or operations</li> </ul>"},{"location":"design_patterns/behavioral/interpreter#example","title":"Example","text":"<pre><code># Abstract Expression\nclass HtmlExpression\n  def initialize(expression)\n    @expression = expression\n  end\n\n  def parse\n    raise_not_implemented_error\n  end\nend\n\n# Non-terminal Expressions\nclass HeaderBeginExpression &lt; HtmlExpression\n  def parse\n    '&lt;head&gt;'\n  end\nend\n\nclass HeaderEndExpression &lt; HtmlExpression\n  def parse\n    '&lt;/head&gt;'\n  end\nend\n\nclass TitleBeginExpression &lt; HtmlExpression\n  def parse\n    '&lt;title&gt;'\n  end\nend\n\nclass TitleEndExpression &lt; HtmlExpression\n  def parse\n    '&lt;/title&gt;'\n  end\nend\n\n# Parser\nclass HtmlParser\n  def initialize(expressions)\n    @expression_list = Array.new\n    expressions.split(\"\\n\").each do |expression|\n      case expression\n      when 'he'\n        @expression_list &lt;&lt; HeaderBeginExpression.new(expression)\n      when '/he'\n        @expression_list &lt;&lt; HeaderEndExpression.new(expression)\n      when 'ti'\n        @expression_list &lt;&lt; TitleBeginExpression.new(expression)\n      when '/ti'\n        @expression_list &lt;&lt; TitleEndExpression.new(expression)\n      else\n        @expression_list &lt;&lt; expression\n      end\n    end\n  end\n\n  def parse\n    html = \"&lt;!DOCTYPE html&gt;\\n\"\n    @expression_list.each do |expression|\n      if expression.class.ancestors.include?(HtmlExpression)\n        html &lt;&lt; expression.parse\n      else\n        html &lt;&lt; expression\n      end\n      html &lt;&lt; \"\\n\"\n    end\n    html &lt;&lt; '&lt;/html&gt;'\n  end\nend\n\n# Context\nclass HtmlContext\n  attr_accessor :expressions\n\n  def initialize(expressions)\n    @expressions = expressions\n  end\n\n  def parse\n    HtmlParser.new(@expressions).parse\n  end\nend\n\n# Client\ntext = &lt;&lt;\"EOS\"\nhe\nti\nMy Title\n/ti\n/he\nEOS\n\ncontext = HtmlContext.new(text)\nhtml = context.parse\nputs html\n#=&gt; &lt;!DOCTYPE html&gt;\n#=&gt; &lt;head&gt;\n#=&gt; &lt;title&gt;\n#=&gt; My Title\n#=&gt; &lt;/title&gt;\n#=&gt; &lt;/head&gt;\n#=&gt; &lt;/html&gt;\n</code></pre>"},{"location":"design_patterns/behavioral/iterator","title":"Iterator","text":"<ul> <li>Traverses elements of a collection without exposing its underlying representation (list, stack, tree, etc.)</li> </ul>"},{"location":"design_patterns/behavioral/iterator#problem","title":"Problem","text":"<ul> <li>How to go sequentially traverse each element of a collection<ul> <li>Without accessing the same elements over and over</li> <li>Especially for complex data structures like trees, graphs</li> </ul> </li> <li>Also we might need different types of traversals<ul> <li>Like DFS, BFS, random access for tree elements</li> <li>Adding more and more traversal algorithms to the collection gradually blurs<ul> <li>Its primary responsibility which is efficient data storage</li> </ul> </li> <li>Some algorithms might be tailored for a specific application<ul> <li>Including them into a generic collection class may not be a good idea</li> </ul> </li> </ul> </li> <li>The client code may not care how a collection stores elements<ul> <li>But since collections provide different ways of accessing their elements</li> <li>We have no option other than to couple the code to the specific collection</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/iterator#solution","title":"Solution","text":"<ul> <li>Extract the traversal behavior of a collection into a separate object called iterator<ul> <li>It implements the traversal algorithm</li> <li>Also encapsulates all of the traversal details such as<ul> <li>The current position, how many elements are left till the end</li> </ul> </li> <li>This allows several iterators to through the same collection at the same time independently</li> </ul> </li> <li>All iterators must implement the same interface<ul> <li>This makes the client code compatible with any collection type or any traversal algorithm</li> <li>If we need a special way to traverse a collection, just create a new iterator class<ul> <li>Without having to change the collection or the client</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/iterator#example","title":"Example","text":"<pre><code># External Iterator\nclass ArrayIterator\n  attr_reader :array\n  attr_accessor :index\n\n  def initialize(array)\n    @array = array\n    @index = 0\n  end\n\n  def has_next?\n    index &lt; array.length\n  end\n\n  def item\n    array[item]\n  end\n\n  def next_item\n    value = item\n    index += 1\n    value\n  end\nend\n\n# Internal Iterator\nclass Account\n  attr_accessor :name, :balance\n\n  def initialize(name, balance)\n    @name = name\n    @balance = balance\n  end\n\n  def &lt;=&gt;(other)\n    balance &lt;=&gt; other.balance\n  end\nend\n\nclass Portfolio\n  include Enumerable\n  attr_accessor :accounts\n\n  def initialize\n    @accounts = []\n  end\n\n  def each(&amp;block)\n    accounts.each(&amp;block)\n  end\n\n  def add_account(account)\n    accounts &lt;&lt; account\n  end\nend\n\n# Client Code\ndef check_portfolio\n  portfolio = Portfolio.new\n  portfolio.add_account(Account.new('Bonds', 200))\n  portfolio.add_account(Account.new('Stocks', 100))\n\n  portfolio.all? { |account| account.balance &gt;= 10 }\n  portfolio.map { |account| account.balance }\nend\n</code></pre>"},{"location":"design_patterns/behavioral/mediator","title":"Mediator","text":""},{"location":"design_patterns/behavioral/mediator#mediator","title":"Mediator","text":"<ul> <li>Also known as Intermediary, Controller</li> <li>Reduces chaotic dependencies between objects</li> <li>Retricts direct communication between objects and force them to collaborate via mediator object</li> </ul>"},{"location":"design_patterns/behavioral/mediator#problem","title":"Problem","text":"<ul> <li>Let's say we have a dialog for creating and editing customer profiles</li> <li>It contains various form controls like textfields, checkboxes, buttons, etc.</li> <li>Some form elements may interact with others. For example:</li> <li>Selecting a checkbox may reveal a hidden textfield to enter more details</li> <li>Clicking save validates the values of the fields</li> <li>If we implement this logic directly in the form elements, it will be hard to reuse them</li> </ul>"},{"location":"design_patterns/behavioral/mediator#solution","title":"Solution","text":"<ul> <li>Cease all direct communication between the components which need to be independent</li> <li>These components must collaborate indirectly by calling a special mediator object<ul> <li>That redirects the calls to appropriate components</li> </ul> </li> <li>The components depend only on a single mediator class instead of being coupled to a dozen colleagues</li> <li>Here, the dialog class itself may act as the mediator</li> <li>The dialog class is already aware of all of its sub-elements</li> <li>So we won\u2019t even need to introduce new dependencies into this class</li> <li>The submit button will no longer have to validate the values of form elements</li> <li>Now its single job is to notify the dialog about the click</li> <li>Upon receiving this notification, the dialog itself performs the validations<ul> <li>Or passes the task to the individual elements</li> </ul> </li> <li>Instead of being tied to the form elements, the button is only dependent on the dialog class</li> <li>We can go further and make the dependency even looser</li> <li>By extracting the common interface for all types of dialogs</li> <li>The interface would declare the notification method which all form elements can use<ul> <li>To notify the dialog about events happening to those elements</li> </ul> </li> <li>Thus, the submit button would be able to work with any dialog that implements that interface</li> </ul>"},{"location":"design_patterns/behavioral/mediator#example","title":"Example","text":"<pre><code># Mediator\n# Interface to declare a method used by components to notify the mediator\n# about various events. The Mediator may react to these events and pass\n# the execution to other components.\nclass Mediator\n  def notify(from, to, msg)\n    raise('Not Implemented')\n  end\nend\n\n# Concrete Mediator\nclass Intercom\n  def initialize\n    @colleagues = Hash.new\n  end\n\n  def add_colleague(colleague)\n    @colleagues[colleague.name] = colleague\n    colleage.set_mediator(self)\n  end\n\n  def notify(from, to, msg)\n    colleague = @colleagues[to]\n    colleague.read_msg(from, msg)\n  end\nend\n\n# Components\n# Various classes that contain some business logic. Has a reference to a mediator,\n# but isn't aware of the actual class of the mediator so that it can be used with\n# different mediators.\nclass Colleague\n  attr_accessor :name\n\n  def initialize(name)\n    @name = name\n  end\n\n  def set_mediator(mediator)\n    @mediator = mediator\n  end\n\n  def send_msg(to, msg)\n    @mediator.notify(self, to, msg)\n  end\n\n  def read_msg(from, msg)\n    puts \"From #{from}: #{msg}\"\n  end\nend\n\n# Client Code\ndef communicate\n  maverick = Colleague.new('Maverick')\n  goose = Colleague.new('Goose')\n\n  intercom = Intercom.new\n  intercom.add_colleague(goose)\n  intercom.add_colleague(maverick)\n\n  maverick.send_msg('Goose', 'Top Gun')\nend\n</code></pre>"},{"location":"design_patterns/behavioral/memento","title":"Memento","text":"<ul> <li>Also known as Snapshot</li> <li>Saves and restores the previous state of an object without revealing the details of its implementation</li> </ul>"},{"location":"design_patterns/behavioral/memento#problem","title":"Problem","text":"<ul> <li>Let's say we're creating a text editor app which can also format text &amp; insert images</li> <li>We also decide to let users undo any operations carried out on text<ul> <li>The direct approach will be to record the state of all objects<ul> <li>And save it in some storage before performing any operation</li> </ul> </li> <li>When a user decides to revert an action, the app fetches the lastest snapshot from the history<ul> <li>And uses it to restore the state of all objects</li> </ul> </li> </ul> </li> <li>Problems with these state snapshots<ul> <li>Will require going over all the fields and copying their values<ul> <li>But most objects won't let others access their data, hiding in private fields</li> </ul> </li> <li>Also, changing the editor class will require changing<ul> <li>The classes responsible for copying the state of its objects</li> </ul> </li> <li>We will require a container class to will store all these states<ul> <li>It may require giving public access to the editor attributes</li> <li>To allow other objects to write and read data to and from a snapshot</li> <li>Which may expose all this state information</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/memento#solution","title":"Solution","text":"<ul> <li>These problems are caused by broken encapsulation<ul> <li>Some objects try to do more than they are supposed to</li> <li>To collect the data required to perform some action<ul> <li>They invade the private space of other objects</li> <li>Instead of letting these objects perform the actual action</li> </ul> </li> </ul> </li> <li>The Memento pattern delegates creating the state snapshots to the actual owner of that state<ul> <li>Instead of other objects trying to copy the editor\u2019s state from the outside</li> <li>The editor class itself can make the snapshot</li> </ul> </li> <li>Store the copy of the object\u2019s state in a special object called memento<ul> <li>The contents of the memento aren\u2019t accessible to any other object except the one that produced it</li> <li>Other objects must communicate with mementos using a limited interface<ul> <li>Which may allow fetching the snapshot\u2019s metadata, but not the state</li> </ul> </li> <li>Such a restrictive policy lets us store mementos inside other objects, usually called caretakers</li> </ul> </li> <li>For the editor, we can create a separate history class to act as the caretaker<ul> <li>A stack of mementos stored inside the caretaker will grow<ul> <li>Each time the editor is about to execute an operation</li> </ul> </li> <li>We can even render this stack within the app\u2019s UI displaying the history</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/memento#example","title":"Example","text":"<pre><code># Originator\n# Produces snapshots of its own state\n# As well as restores its state from snapshots when required\nclass Originator\n  attr_reader :state\n  private :state\n\n  def save\n    Memento.new(@state)\n  end\n\n  def restore(memento)\n    @state = memento.state\n  end\nend\n\n# Memento\n# Acts as a snapshot of the originator's state\nclass Memento\n  attr_reader :state\n  private :state\n\n  def initialize(state)\n    @state = state\n  end\nend\n\n# Caretaker\n# Keeps track of the originator's history by storing a stack of mementos\nclass Caretaker\n  attr_reader :mementos\n  private :mementos\n\n  def initialize(originator)\n    @mementos = []\n    @originator = originator\n  end\n\n  def backup\n    @mementos &lt;&lt; @originator.save\n  end\n\n  def undo\n    return if @memontos.blank?\n\n    memento = @mementos.pop\n    @originator.restore(memento)\n  end\n\n  def history\n    mementos.each { |memento| puts memento.display }\n  end\nend\n\n# Client Code\ndef perform_task_and_undo\n  caretaker = Caretaker.new\n  originator = Originator.new\n  caretaker.backup(originator)\n  perform_task\n  caretaker.undo\nend\n</code></pre>"},{"location":"design_patterns/behavioral/observer","title":"Observer","text":"<ul> <li>Also known as Event-Subscriber, Listener</li> <li>Defines a subscription mechanism to notify multiple objects<ul> <li>About any events that happen to the object they're observing</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/observer#problem","title":"Problem","text":"<ul> <li>Let's say we have two types of objects, Customer and Store</li> <li>If a customer is interested in a particular brand or product (say a new model of iphone)<ul> <li>And the customer wants to know whenever it becomes available in a store</li> </ul> </li> <li>If the customer visits the store every day, most of the trips will be pointless</li> <li>If the store sends emails to all customers about all the new products, it might be considered spam</li> </ul>"},{"location":"design_patterns/behavioral/observer#solution","title":"Solution","text":"<ul> <li>Add a subscription mechanism to the publisher class<ul> <li>Individual objects can subscribe to or unsubscribe from<ul> <li>A stream of events coming from that publisher</li> </ul> </li> <li>Whenever an event happens, the publisher goes over its subscribers<ul> <li>And calls the specific notification method on their objects</li> </ul> </li> </ul> </li> <li>There can be a lot of subscriber classes interested in tracking events<ul> <li>So the publisher shouldn't be coupled with them</li> <li>We might not even know about some of them beforehand<ul> <li>So all subscribers should implement the same interface</li> <li>The publisher should communicate with them only via that interface</li> </ul> </li> </ul> </li> <li>This interface should declare the notification method<ul> <li>Along with a set of paramters that the publisher can use to pass some contextual data</li> </ul> </li> <li>If we've several different types of publishers<ul> <li>And want to make subscribers compatible with all of them</li> <li>We can make all publishers follow the same interface as well</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/observer#example","title":"Example","text":"<pre><code># Publisher\nmodule Observers\n  def subscribe(observer)\n    @observers &lt;&lt; observer\n  end\n\n  def unsubscribe(observer)\n    @observers.delete(observer)\n  end\n\n  def publish\n    @observers.each do |observer|\n      observer.process_event(self)\n    end\n  end\nend\n\n# Subscriber\nclass Employee\n  include Observers\n\n  attr_reader :name, :title, :salary\n\n  def initialize(name, title, salary)\n    @name = name\n    @title = title\n    @salary = salary\n    @observers = []\n  end\n\n  def salary=(new_salary)\n    @salary = new_salary\n    publish\n  end\nend\n\nclass Payroll\n  def process_event(data)\n  end\nend\n\n# Client Code\ndef subscribe_to_employee_events\n  employee = Employee.new('Name', 'Title', 100000)\n\n  payroll = Payroll.new\n  employee.subscribe(payroll)\n\n  tax_tracker = TaxTracker.new\n  employee.subscribe(tax_tracker)\n\n  employee.salary = 500000\nend\n</code></pre>"},{"location":"design_patterns/behavioral/state","title":"State","text":"<ul> <li>Lets an object alter its behavior when its internal state changes</li> <li>It appears as if the object changed its class</li> <li>Closely related to the concept of a finite state machine</li> </ul>"},{"location":"design_patterns/behavioral/state#problem","title":"Problem","text":"<ul> <li>The main idea is that a program can be in a finite number of states at any given time<ul> <li>Within any unique state, the program behaves differently</li> <li>The program can be switched from one state to another instantaneously</li> <li>Program may or may not switch to other states depending on the current state</li> <li>These switching rules are called transitions, which are finite &amp; predetermined</li> </ul> </li> <li>Applying this approach to objects, let's say we have a Document class<ul> <li>A document can be in three states: draft, moderation, published</li> <li>Draft to moderation: Can be moved only by the author</li> <li>Moderation to draft: Moved if review fails</li> <li>Moderation to published: If review passes, only by an administrator user</li> </ul> </li> <li>State machines usually have a lot of conditional statements<ul> <li>That select the appropriate behavior depending on the current state of the object</li> <li>As we add more states and state dependent behaviors to the Document class<ul> <li>They will keep increasing</li> <li>Changing any transition logic may require changing conditionals in every method</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/state#solution","title":"Solution","text":"<ul> <li>Create new classes for all possible states of an object<ul> <li>And extract all state specific behaviors into them</li> </ul> </li> <li>The original object (called context) stores a reference to the current state object<ul> <li>And delegates all state related work to that object</li> </ul> </li> <li>To allow smooth transition of the context to another state<ul> <li>All state classes should follow the same interface</li> </ul> </li> <li>This may look similar to the Strategy pattern<ul> <li>But in this state pattern, states are aware of each other and initiate transitions</li> <li>While in strategy pattern, strategies almost never know about each other</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/state#example","title":"Example","text":"<pre><code># Context\n# Maintains a reference to an instance of the current state subclass\n# And delegates to it all state specific work\nclass TrafficLight\n  attr_accessor :state\n  private :state\n\n  def initialize(state)\n    transition_to(state)\n  end\n\n  def transition_to(state)\n    @state = state\n    @state.context = self\n  end\n\n  def show_signal\n    @state.show_signal\n  end\nend\n\n# State\n# Provides a back reference to the context object\n# Which can be used to transition the context to another state\nclass State\n  attr_accessor :context\n\n  def show_signal\n    raise('Not Implemented')\n  end\nend\n\n# Concrete States\nclass Red &lt; State\n  def show_signal\n    puts \"The traffic light is Red\"\n  end\n\n  def next_signal\n    yellow_signal = Yellow.new\n    context.transition_to(yellow_signal)\n  end\nend\n\nclass Yellow &lt; State\n  def show_signal\n    puts \"The traffic light is Yellow\"\n  end\n\n  def next_signal\n    green_signal = Green.new\n    context.transition_to(green_signal)\n  end\nend\n\nclass Green &lt; State\n  def show_signal\n    puts \"The traffic light is Green\"\n  end\n\n  def next_signal\n    red_signal = Red.new\n    context.transition_to(red_signal)\n  end\nend\n\n# Client Code\ndef transition_red_signal\n  red_signal = Red.new\n  traffic_light = TrafficLight.new(red_signal)\n  traffic_light.show_signal\n  traffic_light.next_signal\nend\n</code></pre>"},{"location":"design_patterns/behavioral/strategy","title":"Strategy","text":"<ul> <li>Defines a family of algorithms<ul> <li>Puts each of them into a separate class</li> <li>Makes their objects interchangeable</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/strategy#problem","title":"Problem","text":"<ul> <li>Let's say we are creating a navigation app for casual travelers<ul> <li>The app is centered around a beautiful map to help users quickly orient themselves in any city</li> </ul> </li> <li>It also has automatic route planning<ul> <li>A user can enter an address and see the fastest route to the destination on the map</li> <li>In the first version, we could only build the driving routes over roads</li> <li>Later, we also added walking routes and public transport routes</li> </ul> </li> <li>We also have plans to add routes for cyclists<ul> <li>And routes through all of a city's tourist attractions</li> </ul> </li> <li>But there are many technical difficulties<ul> <li>Each time a new routing algorithm is added, the main class of the navigator doubles in size</li> <li>Any change to one of the algorithm affects the whole class<ul> <li>Even a simple bug fix or a slight adjustment to street score</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/strategy#solution","title":"Solution","text":"<ul> <li>Take a class that does something specific in a lot of different ways<ul> <li>And extract all of these algorithms into separate classes called strategies</li> <li>The original class (called context) must store a reference to one of the strategies</li> <li>The context should delegate the work to the linked strategy</li> </ul> </li> <li>The context isn't responsible for selecting an appropriate algorithm<ul> <li>The client passes the desired strategy to the context</li> <li>The context works with all the strategies through a generic interface</li> <li>The interface exposes only a single method to trigger the algorithm<ul> <li>Encapsulated within the selected strategy</li> </ul> </li> </ul> </li> <li>This makes the context and the strategies independent of each other<ul> <li>Can add new algorithms or modify existing ones without changing the context or other strategies</li> </ul> </li> <li>In the navigation app, each routing algorithm can be extracted to its own class<ul> <li>Which will return a collection of the route\u2019s checkpoints</li> <li>The main navigator can switch the active routing strategy</li> </ul> </li> <li>This may look similar to the State pattern<ul> <li>But in this state pattern, states are aware of each other and initiate transitions</li> <li>While in strategy pattern, strategies almost never know about each other</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/strategy#example","title":"Example","text":"<pre><code># Context\n# Maintains a reference to the currently choosen strategy\nclass PaymentSystem\n  attr_accessor :payment_mode\n\n  def initialize(amount, payment_mode)\n    @amount = amount\n    @payment_mode = payment_mode\n  end\n\n  def pay\n    @payment_mode.pay(@amount)\n  end\nend\n\n# Strategy\nclass PaymentMode\n  def pay(amount)\n    raise('Not Implemented')\n  end\nend\n\n# Concrete Strategy\nclass Cash &lt; Payment\n  def pay(amount)\n    puts \"You paid $#{amount} by cash\"\n  end\nend\n\nclass CreditCard &lt; Payment\n  def pay(amount)\n    puts \"You paid $#{amount} by credit card\"\n  end\nend\n\n# Client Code\ndef pay(mode)\n  cash = Cash.new\n  credit_card = CreditCard.new\n\n  payment_system = PaymentSystem.new(100, credit_card)\n  payment_system.pay\n  return if payment_system.paid?\n\n  # Let's say the above pay failed, client changes the mode to cach\n  payment_system.payment_mode = cash\n  payment_system.pay\nend\n</code></pre>"},{"location":"design_patterns/behavioral/template","title":"Template","text":"<ul> <li>Defines the skeleton of an algorithm in the superclass<ul> <li>But lets subclassees override specific steps of the algorithm without changing its structure</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/template#problem","title":"Problem","text":"<ul> <li>Let's say we're creating a data mining application<ul> <li>That analyzes corporate documents</li> <li>Users feed documents in various formats (pdf, doc, csv)</li> <li>It tries to extract meaningful data from these documents in a uniform format</li> </ul> </li> <li>The first version could only work with doc files<ul> <li>Then we added support for csv, tand then pdf</li> <li>We observe that the these classes for the formats have a lot of similar code<ul> <li>The code for data processing and analysis is almost identical</li> </ul> </li> </ul> </li> <li>Also, these different classes introduced conditionals in the client code<ul> <li>If there was a common interface, this could have been eliminated</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/template#solution","title":"Solution","text":"<ul> <li>Break down the algorithms into a series of steps<ul> <li>Turn these steps into methods</li> <li>Put a series of calls to these methods inside a single template method</li> <li>The steps may either be abstract or haveg some default implementation</li> </ul> </li> <li>The subclasses will implement the abstract steps<ul> <li>And override some of the optional ones if required</li> </ul> </li> <li>We can also provide hooks as optional steps<ul> <li>Hooks are placed before or after the crucial steps of an algorithm</li> <li>This provides subclasses with additional extenstion points for an algorithm</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/template#example","title":"Example","text":"<pre><code># Abstract Class\n# Defines a template method composed of calls to abstract primitive operations\nclass Sandwich\n  def initialize(bread, condiment)\n    @bread = bread\n    @condiment = condiment\n  end\n\n  def make\n    arrange_bread\n    spread_condiment\n\n    before_vegetable_hook\n\n    put_vegetable\n    put_filling\n\n    after_filling_hook\n  end\n\n  # Optional operation which can be overrided by subclasses\n  def arrange_bread\n    puts \"Arranged #{@bread}\"\n  end\n\n  def spread_condiment\n    puts \"Spreaded #{@condiment}\"\n  end\n\n  # Required Operation\n  def put_vegetable\n    raise('Not Implemented')\n  end\n\n  def put_filling\n    put_base_filling\n  end\n\n  # Hooks that subclasses may override but they are not mandatory\n  # Hooks have the default implementation as empty\n  def before_vegetable_hook\n  end\n\n  def after_filling_hook\n  end\nend\n\n# Concrete Classes\nclass CheeseSandwich &lt; Sandwich\n  def initialize(bread, condiment)\n    super\n  end\n\n  def before_vegetable_hook\n    put_cheese\n  end\n\n  def put_vegetable\n    put_base_vegetables\n  end\nend\n\nclass VegSandwich &lt; Sandwich\n  def initialize(bread, condiment, vegetables)\n    super(bread, condiment)\n    @vegetables = vegetables\n  end\n\n  def put_vegetable\n    put_base_vegetables\n    put_input_vegetables\n  end\nend\n\n# Client\ndef make_sandwiches\n  cheese_sandwich = CheeseSandwich.new('Multigrain Bread', 'Ketchup')\n  cheese_sandwich.make\n\n  veg_sandwich = VegSandwich.new('Oregano Bread', 'Butter', ['Lettuce'])\n  veg_sandwich.make\nend\n</code></pre>"},{"location":"design_patterns/behavioral/visitor","title":"Visitor","text":"<ul> <li>Separates algorithms from the objects on which they operate</li> </ul>"},{"location":"design_patterns/behavioral/visitor#problem","title":"Problem","text":"<ul> <li>Let's say we are developing an app which works with geographic information<ul> <li>Structured as one colossal graph</li> <li>Each node of the graph represents a complex entity (like city)<ul> <li>Or a granular entity (like industry, sightseeing area)</li> </ul> </li> <li>The nodes are connected with others if there's a road between them</li> <li>Each node type is represented by its own class and each specific node is an object</li> </ul> </li> <li>Let's say we want to allow exporting the graph into XML format<ul> <li>The basic approach is to add an export method to each node class</li> <li>And leverage recursion to go over each node</li> </ul> </li> <li>But it might not make sense to have the XML export code within the node classes<ul> <li>Because the primary job of these classes is to work with geodata</li> <li>Also, we don't want to risk breaking the core functionality<ul> <li>Because of potential bugs in the export code</li> </ul> </li> <li>In future, we may also get a request to export in different formats<ul> <li>Or some other related features</li> </ul> </li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/visitor#solution","title":"Solution","text":"<ul> <li>Place the new behavior into a separate class called visitor<ul> <li>Instead of intergrating it into existing classes</li> </ul> </li> <li>But the actual implementation may differ across various node classes<ul> <li>City node &amp; industry node might export their data differently</li> <li>Maybe, the visitor class can take arguments<ul> <li>And define methods to handle different requirements</li> </ul> </li> </ul> </li> <li>But how exactly would we call these methods<ul> <li>Especially when dealing with the whole graph</li> <li>These methods have different signatures, so we can\u2019t use polymorphism</li> <li>To pick a proper visitor method that\u2019s able to process a given object<ul> <li>We\u2019d need to check its class</li> </ul> </li> </ul> </li> <li>Use double dispatch method<ul> <li>Instead of letting the client select a proper version of the method to call</li> <li>Delegate this choice to the objects we're passing to the visitor<ul> <li>Since the objects know their own class, they can pick a proper method</li> </ul> </li> <li>This adds some changes in the node classes but they are trivial</li> </ul> </li> </ul>"},{"location":"design_patterns/behavioral/visitor#example","title":"Example","text":"<pre><code># Acceptors\nclass Motherboard\n  attr_accessor :brand, :model\n\n  def initialize(brand, model)\n    @brand = brand\n    @model = model\n  end\n\n  def accept(visitor)\n    visitor.visit(self)\n  end\nend\n\nclass Cpu\n  attr_accessor :brand, :model\n\n  def initialize(brand, model)\n    @brand = brand\n    @model = model\n  end\n\n  def accept(visitor)\n    visitor.visit(self)\n  end\nend\n\nclass Computer\n  def initialize(motherboard, cpu, dram, hdd)\n    @parts = [motherboard, cpu]\n  end\n\n  def accept(visitor)\n    visitor.visit_parts(@parts)\n  end\nend\n\n# Visitors\nclass ComputerBrand\n  def visit(acceptor)\n    puts \"Brand: #{acceptor.brand}\"\n  end\n\n  def visit_parts(parts)\n    parts.each do |part|\n      part.accept(visitor)\n    end\n  end\nend\n\nclass ComputerModel\n  def visit(acceptor)\n    puts \"Model: #{acceptor.model}\"\n  end\n\n  def visit_parts(parts)\n    parts.each do |part|\n      part.accept(visitor)\n    end\n  end\nend\n\n# Client\ndef computer_brand_and_model\n  motherboard = Motherboard.new(\"ASUS\", \"X99-DELUXE\")\n  cpu = Cpu.new(\"Intel\", \"Core i7-5960X\")\n  computer = Computer.new(motherboard, cpu)\n\n  computer.accept(ComputerBrand.new)\n  # Brand: ASUS\n  # Brand: Intel\n\n  computer.accept(ComputerModel.new)\n  # Model: X99-DELUXE\n  # Model: Core i7-5960X\nend\n</code></pre>"},{"location":"website_designs/introduction","title":"Introduction","text":""},{"location":"website_designs/introduction#quick-tips","title":"Quick Tips","text":"<ul> <li>Explain 80% of the time and ask interviewer 20% of the time<ul> <li>Engage in constructive dialogue, solicit feedback, incorporate suggestions</li> </ul> </li> <li>Clarify requirements to ensure clear understanding of the problem statement<ul> <li>This avoids making assumptions and aligns your design with their expectation</li> </ul> </li> <li>Be prepared for 'why' questions on using specific technologies<ul> <li>Interviewer may ask for more details and justifications</li> <li>Discuss trade-offs and alternatives</li> </ul> </li> <li>Don't go into details prematurely since there is strict timeframe<ul> <li>Wait for interviewer's feedback or response on what needs to be discussed</li> </ul> </li> <li>Requirements may change during the interview to test your flexibility<ul> <li>Don't try to fit the requirements somehow in a set architecture</li> </ul> </li> <li>Be honest and don't try to fake<ul> <li>Find common solutions and show the willingness to learn</li> </ul> </li> </ul>"},{"location":"website_designs/introduction#gather-requirements","title":"Gather Requirements","text":"<ul> <li>System design interviews are by nature vague or abstract<ul> <li>System design is open-ended and there is limited time in the interview</li> <li>Ask questions about the exact scope of the problem and parts to focus on</li> <li>Clarify functional requirements and ambiguities early in the interview</li> </ul> </li> <li>Functional Requirements<ul> <li>Basic functionalities that the end user specifically demands</li> <li>Need to be necessarily incorporated into the system as part of the contract</li> </ul> </li> <li>Non-functional Requirements<ul> <li>Quality constraints that the system must satisfy</li> <li>Priority or extent of these factors varies from one project to another</li> <li>Factors: portability, maintainability, reliability, scalability, security</li> <li>Examples: minimum latency of a request, system should be highly available</li> </ul> </li> <li>Extended Requirements<ul> <li>Nice to have requirements that might be out of scope of the system</li> </ul> </li> </ul>"},{"location":"website_designs/introduction#estimation-constraints","title":"Estimation &amp; Constraints","text":"<ul> <li>Clarify system's constraints and identify use cases</li> <li>Gather information about the problem at hand and design a solution</li> <li>Estimate the scale of the system we need to design</li> <li>Define what APIs are expected from the system</li> <li>Questions<ul> <li>What is the scale that the system need to handle?</li> <li>What is the read &amp; write ratio of the system?</li> <li>How many users? How many of them are daily active? Dozens or millions?</li> <li>How many requests per second?</li> <li>How much storage is required?</li> </ul> </li> </ul>"},{"location":"website_designs/introduction#abstract-design","title":"Abstract Design","text":"<ul> <li>Outline all important components with constraints and use cases<ul> <li>Sketch main components and connections between them</li> <li>First breadth then depth</li> </ul> </li> <li>Data Model Design<ul> <li>Define database schema in early stages of interview to understand the data flow</li> <li>Define all entities and relationships between them</li> </ul> </li> <li>API Design<ul> <li>Design APIs to define the expectations from the system</li> <li>Just a simple interface defining API requirements<ul> <li>Like parameters, functions, classes, types, entities, etc.</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/introduction#high-level-design","title":"High Level Design","text":"<ul> <li>Roadmap for creating complex software systems<ul> <li>Outlines overall structure, components and interactions within the system</li> <li>Represents flows and relationships between modules</li> <li>Selection of components, platforms, and different tools</li> </ul> </li> <li>Draft the first design of our system<ul> <li>Design monolithic or microservice architecture?</li> <li>What type of database to use?</li> <li>Identify system components required to solve our problems<ul> <li>Like load balancer, API gateway</li> </ul> </li> </ul> </li> <li>Capacity Estimation<ul> <li>Predicting the resources required to meet the expected workload<ul> <li>Processing power, memory, bandwidth</li> </ul> </li> <li>How the system can handle the current and the future demands efficiently</li> </ul> </li> </ul>"},{"location":"website_designs/introduction#low-level-design","title":"Low Level Design","text":"<ul> <li>Describe components and interactions from the high level design in more detail<ul> <li>Addressing specific algorithms, data structures, interfaces</li> <li>Present different approaches, advantages, disadvantages</li> </ul> </li> <li>How the components and entities are structured<ul> <li>How should we partition our data?</li> <li>Load distribution</li> <li>Should we use cache?</li> <li>How to handle sudden spike in traffic</li> <li>Object oriented principles, design patterns, solid principles</li> </ul> </li> <li>Discuss with the interviewer which component may need further improvements<ul> <li>Demonstrate your experience in the area of your expertise</li> <li>Explain your design decisions and back them up with examples</li> </ul> </li> </ul>"},{"location":"website_designs/introduction#bottlenecks","title":"Bottlenecks","text":"<ul> <li>Identify bottlenecks and discuss approaches to mitigate them<ul> <li>Do we have enough database replicas?</li> <li>Is there any single point of failure?</li> <li>Is database sharding required?</li> <li>How can we make the system more robust?</li> <li>How to improve the availability of our cache?</li> <li>Handling requests and load balancing</li> <li>Is storage of huge data required?<ul> <li>Will database distribution on multiple machines solve it?</li> <li>Downsides of db distribution: Is db slow? Does it need some in-memory caching?</li> </ul> </li> <li>Each solution is a trade-off</li> </ul> </li> <li>Scaling<ul> <li>Knowing scalability principles and problems it solves</li> <li>Discussing pros, cons and alternatives</li> <li>Abstraction of layers to isolate each component</li> </ul> </li> <li>Read engineering blog of the company to get a sense of their technology stack<ul> <li>And which problems are important to them</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp","title":"Yelp","text":"<ul> <li>Platforms: yelp, proximity server</li> <li>Users can search for nearby places or events<ul> <li>Like restaurants, theaters, shopping malls, etc.</li> <li>They can also view or add reviews for them</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Users can add, update or delete places</li> <li>Given the user's location, find all the nearby places within a given radius</li> <li>Users can add review about a place with text, pictures, rating</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Real time search experience with minimum latency</li> <li>Should support a heavy search load<ul> <li>There will be a lot of search requests compared to adding a place</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#estimation","title":"Estimation","text":"<ul> <li>Places: 500 M</li> <li>Queries per second (QPS): 100K</li> <li>Assume a growth of 20% in the number of places and QPS each year</li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#database","title":"Database","text":"<ul> <li>Schema<ul> <li>Place (id, name, description, category, latitude, longitude, rating)</li> <li>Review (id, place_id, rating, text)</li> <li>Media (id, path, type, review_id)</li> </ul> </li> <li>Length of place id<ul> <li>4 bytes number can uniquely identify 500 M places</li> <li>But keeping future growth in mind, we can go with 8 bytes for place id</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#api","title":"API","text":"<ul> <li>search<ul> <li>api_key, search_terms, category, user_location, radius</li> <li>max_results_to_return, page_token, sort</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#component-design","title":"Component Design","text":"<ul> <li>We need to store and index each dataset (places, reviews, media)</li> <li>While searching for the nearby places, users expect to see the results in real-time<ul> <li>For users to query this massive database, the indexing should be read efficient</li> </ul> </li> <li>Given that the location of a place doesn't change that often<ul> <li>We don't need to worry about frequent updates of the data</li> </ul> </li> <li>To build a service where objects do change their location frequently<ul> <li>Like people, taxis, etc.</li> <li>The design will be very different</li> </ul> </li> <li>Let's see the different ways to store this data and find the best suited method</li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#sql-solution","title":"SQL Solution","text":"<ul> <li>Store all the data in a database like MySQL<ul> <li>To perform a fast search, add index on the latitude and the longitude fields</li> </ul> </li> <li>To query nearby places for a given location L(X, Y) within a radius R<ul> <li>We need to find the distance of each place from L</li> </ul> </li> <li>Efficiency<ul> <li>We have estimated 500 M places which is a huge list<ul> <li>Maybe we can narrow it down by storing the country of places</li> </ul> </li> <li>We have two separate indexes, each can return huge list of places<ul> <li>Performing intersection on the two lists won't be efficient</li> </ul> </li> <li>This will have high latency, we need to narrow down the list of places</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#grids","title":"Grids","text":"<ul> <li>Divide the world map into smaller grids to group places into smaller sets<ul> <li>Each grid will store all the places residing within the range of latitude &amp; longitude</li> </ul> </li> <li>We can uniquely identify each grid by assigning a grid id<ul> <li>And index it for faster search</li> <li>We can keep the index in memory as hash<ul> <li>With key as the grid id and value as the list of the places</li> </ul> </li> </ul> </li> <li>Given a location L(X,Y) and a radius R<ul> <li>Find all the neighboring grids</li> <li>And query the places within these grids to get nearby places</li> </ul> </li> <li>Grid Size<ul> <li>Can be equal to the distance that we want to query</li> <li>It will limit the search to the grid with the location L and the eight neighboring grids</li> <li>Since the grids are statically defined (from the fixed grid size)<ul> <li>Finding the grid number of any location would be easy</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#storage","title":"Storage","text":"<ul> <li>Storage is required for indexing grid</li> <li>Number of grids: 20 M<ul> <li>Total area of earth is 200 M square miles</li> <li>Let's assume the search radius is 10 miles</li> </ul> </li> <li>Grid Id: 4 bytes</li> <li>Place Id: 8 bytes</li> <li>Index size: 4 bytes/grid_id _ 20 M grids + 8 bytes/place_id _ 500 M places = 4 GB</li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#issues","title":"Issues","text":"<ul> <li>Places are not uniformly distributed among grids</li> <li>So it can still run slow for grids with large density of places</li> <li>It can be solved by dynamically adjusting the grid size<ul> <li>By breaking dense grids into smaller ones</li> <li>But how to map grids to locations and how to find neighboring grids?</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#dynamic-grids","title":"Dynamic Grids","text":"<ul> <li>Let's assume we don't want to have more than 500 places in a grid for faster search</li> <li>When a grid reaches this limit<ul> <li>Break it down into 4 equal grids and distribute places among them</li> </ul> </li> <li>This means that thickly populated areas like downtown San Francisco<ul> <li>Will have a lot of small grids</li> </ul> </li> <li>And sparsely populated areas like Pacific Ocean will have large grids<ul> <li>With places only around the coastal lines</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#data-structure","title":"Data Structure","text":"<ul> <li>Quad tree (tree with 4 child nodes) can be used for this<ul> <li>Each node will represent a grid and contain information about all its places</li> <li>If a node reaches the limit, we will break it down to create four child nodes</li> <li>So the leaf nodes will keep a list of places with them</li> </ul> </li> <li>Building a quad tree<ul> <li>Start with the root node that will represent the whole world in one grid</li> <li>Break it down into 4 nodes and distribute places among them</li> <li>Repeat this process until no nodes have more than 500 places</li> </ul> </li> <li>Find the grid for a given location<ul> <li>Start at the root and search downwards</li> <li>Check if the current node has child nodes<ul> <li>If it has, move to the child node which contains the given location</li> <li>Else, it is the required node</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#finding-neighboring-grids","title":"Finding Neighboring Grids","text":"<ul> <li>Only leaf nodes will contain a list of places<ul> <li>We can connect all the leaf nodes through a doubly linked list</li> <li>This way we can iterate forward &amp; backward among the neighboring leaf nodes</li> <li>Till we have enough places or it reaches the maximum radius</li> </ul> </li> <li>Another approach is to keep pointer to parent node in each node<ul> <li>This way we can reach the sibling nodes through the parent node</li> <li>We can keep expanding our search for neighboring grids<ul> <li>By going up through the parent pointers</li> <li>Till enough places are found or it reaches the maximum radius</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#storage-for-quad-tree","title":"Storage for Quad Tree","text":"<ul> <li>To store places: 24 bytes * 500 M = 12 GB<ul> <li>For each place, if we can cache place id and latitude &amp; longitude</li> <li>8 bytes for each field</li> </ul> </li> <li>Total Grids: 500 M places / (500 places/grid) = 1 M<ul> <li>Each grid can have maximum of 500 places</li> </ul> </li> <li>Hence, there will be 1 M leaf nodes holding 12 GB data of places</li> <li>Internal nodes: (1 M _ 1/3 nodes) _ (4 pointers * 8 bytes) = 10 MB<ul> <li>Quad tree with 1 M leaf nodes will approximately have 1/3rd internal nodes</li> <li>Each internal node will have 4 pointers for its childrend</li> <li>Assume that each pointer will take 8 bytes</li> </ul> </li> <li>Total storage: 12 GB + 10 MB = 12.01 GB<ul> <li>This can easily fit into a modern day server</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#inserting-new-place","title":"Inserting New Place","text":"<ul> <li>When a new place is added by a user<ul> <li>It needs to be inserted in the database as well as the quad tree</li> </ul> </li> <li>If the tree resides on one server, it is easy to add a new place<ul> <li>But if it is distributed among multiple servers</li> <li>First find the server and the grid of the new place and add it there</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#ranking-service","title":"Ranking Service","text":"<ul> <li>We may also want to rank the search results not just by proximity<ul> <li>But also by popularity or relevance</li> </ul> </li> <li>We can keep track of the overall popularity of each place<ul> <li>For example, by using the average rating given by users</li> <li>We can store this in the database as well as in the quad tree</li> </ul> </li> <li>We can query top 100 places from each partition of quad tree<ul> <li>And then aggregate these results to get the final top 100 places</li> </ul> </li> <li>But we didn't build our system to update data of places frequently<ul> <li>Searching a place in the quad tree and updating its popularity<ul> <li>Will take a lot of resources and affect search requests &amp; system throughput</li> </ul> </li> <li>Popularity of a place is not expected to be reflected within a few hours<ul> <li>We can decide to update it once or twice a day</li> <li>When the load on the system is minimum</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#scalability","title":"Scalability","text":""},{"location":"website_designs/ecommerce_and_services/yelp#sharding","title":"Sharding","text":""},{"location":"website_designs/ecommerce_and_services/yelp#based-on-region","title":"Based on Region","text":"<ul> <li>Divide the places into region (like zip codes)<ul> <li>Such that all the places for that region will be stored on a fixed node</li> </ul> </li> <li>Issues<ul> <li>If a region becomes hot, there will be a high load on that server</li> <li>Over time, some regions can end up storing a lot of places than others<ul> <li>This can cause non-uniform distribution of places</li> </ul> </li> <li>To recover from these situations<ul> <li>We will have to re-partition the data or use consistent hashing</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#based-on-place-id","title":"Based on Place Id","text":"<ul> <li>While building the quad tree, we will iterate through all the places<ul> <li>And calculate the hash of each place id</li> </ul> </li> <li>To find the nearby places, we will have to query all the servers<ul> <li>And then aggregate the results from all the servers</li> </ul> </li> <li>Will we have different quad tree structure on different partitions<ul> <li>This can happen<ul> <li>Since it is not guaranteed that we will have equal number of places<ul> <li>In a given grid on all partitions</li> </ul> </li> <li>However, we do make sure that<ul> <li>All servers have approximately an equal number of places</li> </ul> </li> </ul> </li> <li>It will not cause any issue though<ul> <li>As we will be searching all the neighboring grids</li> <li>Within the given radius on all partitions</li> </ul> </li> </ul> </li> <li>Hence, this is good paritioning scheme for our use case</li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#replication-and-fault-tolerance","title":"Replication and Fault Tolerance","text":"<ul> <li>To distribute read traffic, we can have replicas of each quad tree server</li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#master-slave-replication","title":"Master-slave replication","text":"<ul> <li>Replicas will only serve the read traffic</li> <li>All write traffic will first go to the master and then applied to replicas</li> <li>Replicas might not have some recently inserted places<ul> <li>There can be a delay of a few milliseconds which is acceptable</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#primary-and-secondary-servers","title":"Primary and secondary servers","text":"<ul> <li>If a quad tree server dies, we can have a secondary replica to failover</li> <li>If both primary and secondary servers die at the same time<ul> <li>We will have to allocate a new server and rebuild the same quadtree</li> </ul> </li> <li>The brute force approach is to iterate through the whole database<ul> <li>And filter place ids using the hash function</li> <li>To figure all the places that should be stored on this partition server</li> <li>This will be inefficient &amp; slow</li> <li>We won't be able to server traffic during this<ul> <li>And miss showing some places that should have been included</li> </ul> </li> </ul> </li> <li>Another approach is to build a reverse index and rebuild the server from that<ul> <li>That will map all the places to their quad tree server</li> <li>A separate quad tree index server will hold this information</li> <li>This can be store in a hash map<ul> <li>Key will be the quad tree server id</li> <li>Value will be the hash set of all its places (will store place id, lat, long)</li> </ul> </li> <li>Hash set will enable us to add or remove places from the index quickly</li> <li>We can also have its replica for fault tolerance</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#caching","title":"Caching","text":"<ul> <li>To deal with popular places, we can introduce a cache in front of the database</li> <li>We can use memcache with LRU</li> <li>We can adjust the number of servers based on the usage pattern of users</li> </ul>"},{"location":"website_designs/ecommerce_and_services/yelp#load-balancing","title":"Load Balancing","text":"<ul> <li>Keep at two places<ul> <li>Between user and application server</li> <li>Between application server and backend server</li> </ul> </li> <li>Modified Round Robin can be used which will also take server load into consideration</li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber","title":"Uber","text":"<ul> <li>Platforms: uber, ola, lyft</li> <li>Ride sharing service that connects passengers and drivers</li> <li>Passengers will see all the nearby available drivers</li> <li>Drivers need to regulary notify the service<ul> <li>About their availability to pick passengers and their current location</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Customers can request a ride</li> <li>Nearby drivers are notified about this request<ul> <li>Which they can accept or decline</li> </ul> </li> <li>Once a driver accepts<ul> <li>They can constantly see each other's current location</li> <li>And contact each other until the trip finishes</li> </ul> </li> <li>Upon reaching the destination, driver will mark the ride complete<ul> <li>And becomes available for the next ride</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#estimation","title":"Estimation","text":"<ul> <li>Number of Users: 300 M</li> <li>Number of Drivers: 1 M</li> <li>Traffic<ul> <li>Daily Active Users: 1 M</li> <li>Daily Active Drivers: 500 K</li> <li>Daily Rides: 1 M</li> <li>Assume that all active drivers notify their current location every 3 secs</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#component-design","title":"Component Design","text":""},{"location":"website_designs/ecommerce_and_services/uber#dynamic-grid","title":"Dynamic Grid","text":"<ul> <li>We will take the solution discussed in yelp<ul> <li>And modify it to make it work for the use cases for uber</li> <li>The quad tree is efficient to find the nearby drivers<ul> <li>But not built to support frequent updates</li> </ul> </li> </ul> </li> <li>All active drivers report their locations every 3 seconds<ul> <li>We need to be update the data structure to reflect that</li> <li>Updating the quadtree will take a lot of time and resources</li> <li>To update a driver to its new location<ul> <li>We must find the right grid based on the driver's previous location</li> <li>If the new position does not belong to the current grid<ul> <li>Remove the driver from current grid and insert in the correct grid</li> </ul> </li> <li>After this, if the new grid reaches the maximum limit of drivers, repartition it</li> </ul> </li> </ul> </li> <li>We need to quickly propagate the current location of all the nearby drivers<ul> <li>To all the active customers in that area</li> <li>When ride is in progress<ul> <li>System needs to notify about the current location of the car</li> <li>To both the driver and the customer</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#updating-driver-location","title":"Updating Driver location","text":"<ul> <li>Do we need to modify the quad tree every time a driver reports their location<ul> <li>If we don't update, it will have some old data<ul> <li>And won't reflect the current location of drivers correctly</li> </ul> </li> <li>Our purpose is find the nearby drivers efficiently</li> </ul> </li> <li>What if we keep the latest position reported by all the drivers in a hash table<ul> <li>And update the quadtree a little less frequently (say 15 seconds)</li> <li>We need to store the driver id, their present and old location</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#storage","title":"Storage","text":"<ul> <li>Driver Location: { driver_id: { old_lat, old_long, new_lat, new_long } }</li> <li>One driver entry will require 35 bytes<ul> <li>The id will take 3 bytes (enough for 1 M drivers)</li> <li>And other fields will take 8 bytes</li> </ul> </li> <li>The request for update location will require 19 bytes<ul> <li>Since it won't include old lat &amp; long</li> </ul> </li> <li>Storage: 1 M * 35 bytes = 35 MB</li> <li>Bandwidth: 1 M * 19 bytes = 19 MB/3s (location is update every 3 second)</li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#distribution","title":"Distribution","text":"<ul> <li>Do we need to distribute this hash onto multiple servers<ul> <li>All this information can fit on one server</li> <li>And our memory and bandwidth requirements don't require this</li> </ul> </li> <li>But for scalability, performance and fault tolerance<ul> <li>We can distribute the driver location hash table on multiple servers</li> <li>We can distribute based on driver id to make it completely random</li> </ul> </li> <li>These servers will also broadcast the location updates to active customers</li> <li>And notify the respective quad tree server to refresh the driver location every 15s</li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#broadcasting-driver-location","title":"Broadcasting Driver Location","text":"<ul> <li>How can we efficiently broadcast the driver's location to customers</li> <li>We can have a push model<ul> <li>Where the server will push the positions to all the relevant users</li> </ul> </li> <li>We can have a dedicated notification service<ul> <li>That can broadcast the current location of drivers to all the active customers</li> <li>This can be based on pub/sub model</li> </ul> </li> <li>Pub/sub model<ul> <li>When a customer opens the app<ul> <li>The client will query the server to find nearby drivers</li> </ul> </li> <li>On server side<ul> <li>The customer will be subscribed to the updates of nearby drivers</li> <li>And then return the list of these drivers to the customer</li> </ul> </li> <li>It can maintain a list of customers (subscribers)<ul> <li>And broadcast the current location of the driver</li> <li>Whenever we have an update in the driver location hash table</li> </ul> </li> </ul> </li> <li>Notify respective quad tree server to refresh driver's location every 15 seconds</li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#storage_1","title":"Storage","text":"<ul> <li>We need to store the driver id (3 bytes) and customer ids (8 bytes)</li> <li>Assume that five customers subscribe to one driver</li> <li>Storage requied: 500 K _ 3 + 500 K _ 5 * 8 = 21 MB<ul> <li>500 K daily active drivers * 3 bytes for driver id</li> <li>500 K daiy active drivers _ 5 subscriptions/driver _ 8 bytes for customer id</li> </ul> </li> <li>Bandwidth<ul> <li>For active drivers: 5 subs/driver * 500 K = 2.5 M</li> <li>For active customers: 500 K _ 5 subs _ 19 bytes = 47.5 MB/s<ul> <li>The location request takes 19 bytes (driver id (3), lat (8), long (8))</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#new-driver-info","title":"New Driver Info","text":"<ul> <li>What will happen when a new driver enters the area the customer is looking at</li> <li>To add a new customer/driver subsription dynamically<ul> <li>We need to keep track of the area the customer is watching</li> <li>But this will make our solution complicated</li> </ul> </li> <li>What if the client pulls this information from the server instead<ul> <li>Clients can send their current location</li> <li>And the server will find all the nearby drivers from the quad tree</li> <li>Upon getting the driver list, the client can update their current location</li> <li>Clients can query every five seconds to limit the number of round trips to server</li> <li>This is simpler than the push model</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#repartioning-grid","title":"Repartioning Grid","text":"<ul> <li>Do we need to repartition the grid as soon as it reaches the maximum limit</li> <li>We can have a buffer to let each grid grow a little bigger beyond the limit<ul> <li>Before we decide to partition it</li> </ul> </li> <li>Let's say the grids can grow/shrink an extra 10% before partitioning/merging<ul> <li>This will reduce the load for a grid partition/merge on high traffic grids</li> <li>And will account for dynamic movement</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#requesting-ride","title":"Requesting Ride","text":"<ul> <li>The client will put a request for a ride</li> <li>One of the aggregator servers will take the request<ul> <li>And ask the quad tree servers to return nearby drivers</li> <li>It will collect the results and sort them by ratings</li> <li>It will send a notification to the top (say 3) drivers simultaneously<ul> <li>The one to accept the request first will be assigned the ride</li> <li>Other drivers will receive a cancellation request</li> <li>If none of them accepts, it will send a notification to next three drivers</li> </ul> </li> </ul> </li> <li>Once a driver accepts the request, the client will be notified</li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#ranking-service","title":"Ranking Service","text":"<ul> <li>Refer yelp</li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#advanced-issues","title":"Advanced Issues","text":"<ul> <li>How to handle clients on slow and disconnecting networks</li> <li>What if a client gets disconnected when they are a part of a ride?<ul> <li>How will we handle billing in such a scenario?</li> </ul> </li> <li>How about if clients pull all the information instead of servers always pushing it?</li> </ul>"},{"location":"website_designs/ecommerce_and_services/uber#scalability","title":"Scalability","text":"<ul> <li>Refer yelp</li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master","title":"Ticketmaster","text":"<ul> <li>Platforms: ticketmaster, bookmyshow</li> <li>Online ticketing system to sell movie tickets</li> <li>Customers can browse through movies currently being played<ul> <li>And book seats anywhere anytime</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>List cities where the affiliated cinemas are located<ul> <li>Once the user selects the city, display the movies that are available there</li> </ul> </li> <li>After selecting the movie<ul> <li>Display the cinemas running that movie and the available show times</li> </ul> </li> <li>Show the seating arrangement and the available seats<ul> <li>User can select multiple seats based on their preference and book them</li> </ul> </li> <li>After selecting the seats<ul> <li>Put a hold on the seats for 5 mins before finalizing and making payment</li> </ul> </li> <li>User should be able to wait if there is a chance that the seats might become available<ul> <li>For example, when the holds by other users expire</li> <li>Waiting customers should be served in a fair, first come, first serve manner</li> </ul> </li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Highly concurrent<ul> <li>There will be multiple booking requests for the same seat at the same time</li> <li>The service should handle this gracefully and fairly</li> </ul> </li> <li>There will be financial transactions<ul> <li>So system should be secure &amp; database should be ACID compliant</li> </ul> </li> <li>Highly scalable and highly available<ul> <li>Since the traffic would spike on popular and much-awaited movie releases</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#considerations","title":"Considerations","text":"<ul> <li>There should not be any partial order, either book all the tickets or nothing</li> <li>Fairness is mandatory for the system</li> <li>To stop system abuse, restrict users from booking more than 10 seats at a time</li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#estimation","title":"Estimation","text":"<ul> <li>Traffic<ul> <li>Page views: 3 B/month</li> <li>Tickets sold: 10 M/month</li> </ul> </li> <li>Business Size<ul> <li>Number of cities: 500</li> <li>Cinemas per city: 10</li> <li>Seats in each cinema: 2000</li> <li>Shows per day: 2</li> </ul> </li> <li>Storage<ul> <li>Database fields<ul> <li>For each booking: 50 bytes</li> <li>For each movie: 25 bytes</li> <li>For each cinema: 25 bytes</li> </ul> </li> <li>Daily Storage: 500 cities _ 10 cinemas _ 2000 seats _ 2 shows _ 100 bytes = 2 GB/day</li> <li>Storage for 5 years: 3.6 TB</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#api","title":"API","text":"<ul> <li>api_key can be used for throttling</li> <li>search_movies<ul> <li>api_key, keyword, city, zipcode, lat, long, radius, start_time, end_time</li> <li>include_spell_check, results_per_page, sorting_order</li> </ul> </li> <li>reserve_seats (returns whether reservation succeeded or failed)<ul> <li>api_key, session_id, movie_id, show_id, seat_numbers</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#database","title":"Database","text":"<ul> <li>Relations<ul> <li>Each city can have multiple cinemas</li> <li>Each cinema will have multiple halls</li> <li>Each movie will have many shows</li> <li>Each show will have multiple bookings</li> <li>Each user can have multiple bookings</li> </ul> </li> <li>Schema<ul> <li>User (id, name, email, phone)</li> <li>Cinema (id, address_id, name, total_cinema_halls)</li> <li>Address (id, city, state, zipcode)</li> <li>Hall (id, cinema_id, regular_seat_capacity, premium_seat_capacity)</li> <li>Seat (id, hall_id, seat_number, seat_type)</li> <li>Show (id, hall_id, movie_id, start_time, end_time)</li> <li>ShowSeat (id, seat_id, show_id, booking_id, status)</li> <li>Movie (id, title, description, duration, genre, language, release_date, country)</li> <li>Pricing (id, show_id, regular_price, premium_price)</li> <li>Booking (id, user_id, show_id, number_of_seats, payment_id, status)</li> <li>Payment (id, booking_id, transaction_id, coupon_id, payment_method, status, amount)</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#component-design","title":"Component Design","text":""},{"location":"website_designs/ecommerce_and_services/ticket_master#workflow","title":"Workflow","text":"<ul> <li>User searches for a movie and selects a movie</li> <li>User is shown the available shows and selects a show</li> <li>User specifies the number of seats to be reserved</li> <li>If the required number of seats are available, user is shown the seat map<ul> <li>User selects the seats and the system tries to reserve them<ul> <li>If seats are reserved successfully<ul> <li>User gets five minutes to pay</li> <li>After payment, booking is marked complete</li> <li>If payment is not successful, seats become available for other users</li> </ul> </li> <li>Else if the specified seats are no longer available<ul> <li>Redirect to the seat map to choose different seats</li> </ul> </li> <li>Else if the show is full<ul> <li>If there are hold orders present<ul> <li>Redirect to the waiting page</li> <li>If the seats become available, redirect to the seat map</li> <li>If the seats get booked or there are fewer seats available/hold<ul> <li>Show the error message and redirect to the seat map</li> </ul> </li> <li>If the user cancels or the request timeouts<ul> <li>Show the error message and redirect to the movie search page</li> </ul> </li> </ul> </li> <li>Else show the error message and redirect to the movie search page</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#reservation-tracker","title":"Reservation Tracker","text":"<ul> <li>We need to track<ul> <li>All the active reservations that haven\u2019t been booked yet<ul> <li>And remove any expired reservations from the system</li> </ul> </li> <li>All the waiting customers<ul> <li>If the seats become available, notify the longest waiting user to choose seats</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#active-reservation-service","title":"Active Reservation Service","text":"<ul> <li>Each active reservation will have a expiry time<ul> <li>Show a timer to user for a better user experience</li> <li>The timer can be a little out of sync, so add a buffer of 5s</li> </ul> </li> <li>We can keep active reservations of a show in memory in addition to the database<ul> <li>We can use data structures like Linked Hash Map or Tree Map<ul> <li>That allows to jump to any reservation to remove it when the booking is complete</li> <li>{ show_id: linked_hash_map { booking_id, created_at } }</li> </ul> </li> <li>Head of the map will always point to the oldest reservation<ul> <li>So that it can be expired after the timeout (FCFS)</li> </ul> </li> </ul> </li> <li>Database<ul> <li>Store the reservation in the Booking table</li> <li>And keep track of the expiry in a timestamp column</li> <li>Status can have these values: reserved, booked, expired</li> </ul> </li> <li>After the seats have been booked or the request expires<ul> <li>Update the database and remove the reservation from the map</li> <li>And send a signal to the Waiting User Service</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#waiting-user-service","title":"Waiting User Service","text":"<ul> <li>Just like Active Reservation Service<ul> <li>We can keep all the waiting users of a show in memory in a Linked Hash Map<ul> <li>{ show_id: linked_hash_map { user_id, wait_start_time } }</li> </ul> </li> <li>And the head of the map will point to the longest waiting user (FCFS)</li> <li>When a user cancels request, we can jump and remove it from the map</li> </ul> </li> <li>Clients can use long polling for keeping themselves updated about the reservation status<ul> <li>When the seats become available, the server can use this request to notify the user</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#concurrency","title":"Concurrency","text":"<ul> <li>No two users should be able to book the same seat</li> <li>We can use transactions in the SQL database to avoid any clashes</li> <li>E.g. Use Transaction Isolation Levels to lock the rows before updating     <pre><code>SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN TRANSACTION;\nselect * from show_seats\nwhere show_id = 99 and show_seat_id in (54, 55, 56) and status = 'available';\n-- If the number of rows returned above is 3, i.e. all the 3 seats are reserved\n-- return success else failure\nupdate show_seats ...;\nupdate bookings ...;\nCOMMIT TRANSACTION;\n</code></pre></li> <li>Reading rows within a transaction gets a write lock on them<ul> <li>And can\u2019t be updated by anyone else</li> </ul> </li> <li>Serializable is the highest isolation level<ul> <li>And guarantees safety from dirty, non-repeatable and phantom reads</li> </ul> </li> <li>Once the transaction is successful<ul> <li>We can start tracking the reservation in Active Reservation Service</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#reservation-overview","title":"Reservation Overview","text":""},{"location":"website_designs/ecommerce_and_services/ticket_master#on-expiration","title":"On Expiration","text":"<ul> <li>Whenever a reservation is expired, the server holding it will carry these actions</li> <li>Update database to mark the booking expired (or remove it)</li> <li>Update the status of seats in show_seats table</li> <li>Remove the reservation from the Linked Hash Map</li> <li>Notify the user that their reservation has expired</li> <li>Figure out the Waiting User Service servers holding the waiting users<ul> <li>By using the consistent hashing scheme</li> </ul> </li> <li>Broadcast a message to all Waiting User Service servers<ul> <li>To figure out the longest waiting user</li> </ul> </li> <li>If the seats become available, request these server to process the user request</li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#on-success","title":"On Success","text":"<ul> <li>Whenever a reservation is successful, following things will happen</li> <li>The Waiting User Service holding the reservation<ul> <li>Sends a message to all the servers holding waiting users of that show</li> <li>To expire all waiting users who need more seats than the available seats</li> </ul> </li> <li>Upon receiving the above message, all the servers holding the waiting users<ul> <li>Will query the database to find how many free seats are available now<ul> <li>Using a database cache would help to run this query only once</li> </ul> </li> <li>Expire all waiting users wanting more than the available seats<ul> <li>The server will iterate through the Linked HashMap of all the waiting users</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#scalability","title":"Scalability","text":""},{"location":"website_designs/ecommerce_and_services/ticket_master#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>What happens when ActiveReservationsService or WaitingUsersService crashes<ul> <li>If Active Reservation Service crashes<ul> <li>We can read all the active reservations from Booking table</li> <li>Or we can use a master-slave configuration</li> </ul> </li> <li>If Waiting User Service crashes<ul> <li>We are not storing the waiting users in the database</li> <li>So the only option is have a master-slave configuration</li> </ul> </li> </ul> </li> <li>Similarly, we can have a master-slave setup for the databases</li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#database-partitioning","title":"Database Partitioning","text":"<ul> <li>Based on Movie Id<ul> <li>All the shows of a movie will be on a single server</li> <li>For a very popular movie, this could cause an overload on that server</li> </ul> </li> <li>Based on Show Id<ul> <li>The load gets distributed among different servers</li> </ul> </li> </ul>"},{"location":"website_designs/ecommerce_and_services/ticket_master#load-balancing","title":"Load Balancing","text":"<ul> <li>The web servers will manage all the active user sessions<ul> <li>And the communication with the users</li> </ul> </li> <li>We can use Consistent Hashing based on the show id<ul> <li>To allocate servers for Active Reservation Service and Waiting User Service</li> <li>This way all reservations and waiting users of a particular show<ul> <li>Will be handled by a certain set of servers</li> </ul> </li> </ul> </li> <li>Let\u2019s assume for load balancing<ul> <li>The consistent hashing allocates three servers for any show</li> <li>So when the reservation expires<ul> <li>Only the server holding that reservation will carry out the required actions</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram","title":"Instagram","text":"<ul> <li>Platforms: instagram, flickr, picasa</li> <li>Social networking service to share photos &amp; videos<ul> <li>They can be shared publicly or privately</li> <li>Users can like these photos</li> <li>Users can search photos &amp; videos based on captions &amp; location</li> </ul> </li> <li>People can follow other and others can follow them<ul> <li>The account can also be kept public or private</li> <li>In private accounts, only the followers can view the feed</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Users can upload, download, and view photos</li> <li>Users can perform searches based on photo tiles</li> <li>Users can follow each other</li> <li>System generates and shows the feed to the user</li> <li>The feed should consist of top photos from all the people the user follows</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>System should be highly available</li> <li>Acceptable latency for the feed generation is 200 ms</li> <li>Consistency over Availability: If a user doesn't see a photo for a while, it's fine</li> <li>Highly Reliable: Any uploaded photo should never be lost</li> </ul> </li> <li>Extended Requirements<ul> <li>Adding tags to photos</li> <li>Searching photos on tags</li> <li>Commenting on photos</li> <li>Tagging users to photos</li> <li>Show follow suggestions</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#considerations","title":"Considerations","text":"<ul> <li>The system would be read heavy<ul> <li>Users will view feed more often than they will upload photos</li> <li>So focus on building a system that retrieves photos quickly</li> </ul> </li> <li>Users can upload as many photos as they like<ul> <li>Storage should be managed efficiently to retrieve these photos</li> <li>Data should be reliable and should never be lost</li> </ul> </li> <li>Should there be a limit on the photo size?<ul> <li>We can skip it for now?</li> <li>For videos, it may be required, maybe 100 MB?</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#estimation","title":"Estimation","text":"<ul> <li>Traffic<ul> <li>Total Users: 500 M</li> <li>Daily Active Users: 1 M</li> </ul> </li> <li>System would be read heavy, let's assume read-write ratio of 100 : 1<ul> <li>Photos Upload: 2 M/day = 23/s</li> <li>Photos Read: 200 M/day = 2300/s</li> </ul> </li> <li>Storage<ul> <li>Average Photo Size: 200 KB</li> <li>Daily Storage: 2 M photos/day * 200 KB = 400 GB</li> <li>Storage required for 10 years: 400 GB _ 365 days _ 10 years = 1425 TB</li> </ul> </li> <li>Bandwidth<ul> <li>Write: 400 GB/day = 400 GB/86400 s = 4.62 MB/s</li> <li>Read: 200 M requests/day * 200 KB = 40000 G requests/86400 s = 462 MB/s</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#database","title":"Database","text":"<ul> <li>Schema<ul> <li>User (id, name, email, dob, created_at, last_login_at)</li> <li>UserFollow (user_id, follower_id)</li> <li>Photo<ul> <li>id, description, user_id, photo_path, created_at</li> <li>photo_latitude, photo_longitude, user_latitude, user_longitude</li> </ul> </li> </ul> </li> <li>Type<ul> <li>Use SQL since relationships and joins are required</li> <li>Use S3 for storing photos</li> </ul> </li> <li>Data Estimation<ul> <li>User<ul> <li>id (4) + name (20) + email (32) + dob (4) + created_at (4) + last_login_at (4)</li> <li>68 bytes * 500 M users = 32 GB</li> </ul> </li> <li>Photo<ul> <li>id (4) + description (256) + user_id (4) + photo_path (256) + created_at (4)</li> <li>photo_latitude (4) + photo_longitude (4) + user_latitude (4) + user_longitude (4)</li> <li>540 bytes * 2 M photos/day = 1 GB/day = 3.65 TB for 10 years</li> </ul> </li> <li>UserFollow<ul> <li>user_id (4) + follower_id (4)</li> <li>8 bytes _ 500 M users _ 500 followers/user = 1.82 TB</li> </ul> </li> <li>Total space required for 10 years = 5.5 TB</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#application-servers","title":"Application Servers","text":"<ul> <li>Uploading photos (writes) will be slow compared to reads<ul> <li>System can get busy with uploading photos since it's a slow process</li> <li>Which can lead to read requests not being served</li> <li>This is because web servers have a limit for concurrent connections (say 500)</li> </ul> </li> <li>To handle this bottleneck<ul> <li>We can split reads and writes into separate services</li> <li>We will have dedicated servers for reads and different servers for write</li> <li>It will also allow us to scale and optimize these operations separately</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#component-design","title":"Component Design","text":""},{"location":"website_designs/social_media/instagram#feed-generation","title":"Feed Generation","text":"<ul> <li>For a given user, we need to fetch the latest, most popular and relevant photos<ul> <li>From the people that the user follows</li> <li>Let's assume we need to fetch top 100 photos for the feed</li> </ul> </li> <li>First get a list of people the user follows<ul> <li>Then fetch metadata of the latest 100 photos from each user</li> <li>Finally submit all these photos to the ranking algorithm</li> <li>The ranking algorithm will determine the top 100 photos based on a decided criteria</li> </ul> </li> <li>There can be high latency to fetch all this data from multiple data<ul> <li>And perform sorting, merging, ranking to get the final result</li> <li>To improve efficiency, we can pre-generate the feed and store it in a table</li> </ul> </li> <li>We can have dedicated servers that continuously generate user feeds and store them<ul> <li>After this feed has been shown to the user</li> <li>New feed can be generated again<ul> <li>If there are new posts since then, then they should be preferred</li> <li>Else old posts which didn't make to the last feed can be considered</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#showing-feed","title":"Showing Feed","text":"<ul> <li>What are the different approaches to send the feed contents to the users</li> <li>Clients can pull the feed from the server<ul> <li>On a regular basis manually whenever they need it</li> <li>Though new data might not be shown until clients issue a pull request</li> <li>If there is no new data, the pull requests might result in an empty response</li> </ul> </li> <li>Servers can push the feed to the users<ul> <li>As soon as it is available</li> <li>Users will have to maintain a long poll request with the server</li> <li>If a user follows a lot of people or a celebrity with millions of followers<ul> <li>The server will have to push updates quite frequently</li> </ul> </li> </ul> </li> <li>Hybrid approach<ul> <li>Move all the celebrity users with large number of follows to a pull based model<ul> <li>Push data to only those users who have comparatively less number of follows</li> </ul> </li> <li>Another approach can be to push updates at a certain frequency<ul> <li>Letting users with a lot of follows/updates to regularly pull data</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#scalability","title":"Scalability","text":""},{"location":"website_designs/social_media/instagram#reliability-and-redundancy","title":"Reliability and Redundancy","text":"<ul> <li>Losing files is not an option for our service<ul> <li>So store multiple copies of each file</li> </ul> </li> <li>High availability is required<ul> <li>So have multiple replicas of services running in the system</li> <li>This will also remove single point of failures</li> </ul> </li> <li>If only one instance of a service is required at any point<ul> <li>We can run a redundant secondary copy that is not serving any traffic</li> <li>It can take control if the primary server fails</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#data-sharding","title":"Data Sharding","text":""},{"location":"website_designs/social_media/instagram#based-on-user-id","title":"Based on User Id","text":"<ul> <li>Will keep all photos of a user on the same shard<ul> <li>If one db shard is 1 TB, we will need 6 shards to store 5.5 TB as estimated before</li> <li>Let's assume for better performance and scalability, we keep 10 shards</li> <li>So we can find the shard number by user_id % 10</li> </ul> </li> <li>To generate photo ids<ul> <li>Each db shard can have its own auto-increment sequence for photo ids</li> <li>We can append shard number with each photo id</li> <li>This will make it unique throughout the system</li> </ul> </li> <li>Issues<ul> <li>How would we handle hot users who are followed by a lot of people</li> <li>Some users will have a lot of photos than others<ul> <li>Which can lead to non-uniform distribution of storage</li> </ul> </li> <li>What if we cannot store all pictures of a user on one shard<ul> <li>If we distribute photos of a user onto multiple shards, will it cause higher latency?</li> </ul> </li> <li>If a shard is down or experiencing a high load<ul> <li>It can cause unavailability of all of the user's data</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#based-on-photo-id","title":"Based on Photo Id","text":"<ul> <li>Generate a unique photo id and then find a shard number like photo_id % 10<ul> <li>This won't require appending shard number to the photo id</li> </ul> </li> <li>To generate photo ids<ul> <li>Cannot have auto-incrementing sequence in each shard</li> <li>We can have a dedicated database instance to generate and store incrementing ids</li> </ul> </li> <li>This key generating db can be a single point of failure<ul> <li>A workaround could be defining two such databases<ul> <li>One generating even numbers and the other generating odd numbers</li> </ul> </li> <li>We can put a load balancer in front of these databases with round robin<ul> <li>These servers can be out of sync with one generating more keys than the other</li> <li>But this won't cause any issues in the system</li> </ul> </li> <li>Alternatively, we can implement a key generation scheme like in 'URL shortening'</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#future-growth","title":"Future Growth","text":"<ul> <li>We can have a large number of logical partitions to accomodate future data growth</li> <li>In the beginning, multiple logical partitions can reside on a single physical db server<ul> <li>One database server can have multiple instances</li> <li>So we can have separate databases for each logical partition on any server</li> </ul> </li> <li>Whenever a particular database server has accumulated a lot of data<ul> <li>We can migrate some logical partitions from it to another server</li> <li>Maintain a config file (or separate database) to map logical partitions to db servers</li> <li>This will enable us to move partitions easily</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#feed-generation-with-sharded-data","title":"Feed Generation with Sharded Data","text":"<ul> <li>To fetch latest photos, we need to sort them by their time of creation<ul> <li>To efficiently do this, we can make creation time part of the photo id</li> <li>As we have primary index on photo id, it will be quick to find the latest photo ids</li> </ul> </li> <li>Photo id can have two parts<ul> <li>First part can have epoch time</li> <li>And second part will be an auto-incrementing sequence from the key generating db</li> <li>Then we can figure out the shard number from photo_id % 10</li> </ul> </li> <li>Size of photo id<ul> <li>Let's say our epoch time starts today<ul> <li>And we store the number of seconds for next 50 years</li> <li>86400 s/day _ 365 days/year _ 50 years = 1.6 billion seconds</li> <li>We would need 31 bits to store this number</li> </ul> </li> <li>We are expecting 23 new photos per second<ul> <li>We can allocate 9 bits for auto-incrementing sequence</li> <li>So every second we can store: 2^9 = 512 new photos</li> <li>We can reset auto incrementing sequence every second</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/instagram#cache","title":"Cache","text":"<ul> <li>Our service would need a massive scale photo delivery system<ul> <li>To serve globally distributed users</li> <li>So the content should be pushed closer to the user</li> <li>This can be done using<ul> <li>Large number of geographically distributed photo cache server</li> <li>And using CDNs</li> </ul> </li> </ul> </li> <li>We can have a cache for metadata servers for hot database rows<ul> <li>LRU can be a reasonable cache eviction policy</li> </ul> </li> <li>We can cache frequently accessed photos and celebrity photos</li> </ul>"},{"location":"website_designs/social_media/twitter","title":"Twitter","text":"<ul> <li>Social networking service where users post and read tweets</li> <li>Tweets are short 140 character messages</li> <li>Only the registered users can post tweets</li> </ul>"},{"location":"website_designs/social_media/twitter#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>User can post new tweets (with a limit of 140 chars)</li> <li>User can follow other users</li> <li>User can mark a tweet as favorite</li> <li>Timeline should be displayed to the user<ul> <li>Consisting of top tweets from the people that the user follows</li> </ul> </li> <li>Tweets can contain photos and videos</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Service should be highly available</li> <li>Acceptable latency for timeline generation: 200 ms</li> <li>Consistency over Availability<ul> <li>It's fine if a user can't see a tweet for a while but it should be consistent</li> </ul> </li> </ul> </li> <li>Extended Requirements<ul> <li>Searching for tweets</li> <li>Replying to tweets</li> <li>Retweet</li> <li>Trending topics &amp; searches</li> <li>Tagging other users</li> <li>Push notifications</li> <li>Show follow suggestions</li> <li>Moments (Snapshot of top news, tweets &amp; topic)</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#estimation","title":"Estimation","text":"<ul> <li>Total users: 1 B<ul> <li>Each user follows: 200 people</li> </ul> </li> <li>Traffic<ul> <li>Daily active users: 200 M</li> <li>Tweets: 100 M/day = 1150/s</li> <li>Daily favorites: 200 M users * 5 favorites/user = 1 B</li> <li>Daily timeline visits: 7<ul> <li>Assume a user visits their timeline 2 times a day on average</li> <li>And visits pages of 5 other users</li> </ul> </li> <li>Average number of tweets per timeline: 20</li> <li>Tweet views: 28 B/day = 325 K/s<ul> <li>200 M users/day _ 7 timelines/day _ 20 tweets/timeline</li> </ul> </li> </ul> </li> <li>Storage<ul> <li>Tweet size: 280 bytes + 30 bytes = 310 bytes<ul> <li>Content size per tweet: 140 chars * 2 bytes = 280 bytes</li> <li>Metadata size per tweet (id, timestamp, user_id, etc.): 30 bytes</li> </ul> </li> <li>Tweet storage per day: 100 M tweets * 310 bytes/tweet = 30 GB</li> <li>Media storage per day: 100 M tweets _ (20% photos _ 200 KB + 10% videos * 2 MB) = 24 TB<ul> <li>Assume that every fifth tweet has a photo on average</li> <li>And every tenth tweet has a video</li> <li>Assume that a photo is 200 KB and a video is 2 MB on average</li> </ul> </li> </ul> </li> <li>Bandwidth<ul> <li>Incoming: 24 TB / 86400 sec = 290 MB/s</li> <li>Outgoing: 35 GB/s<ul> <li>Text: (28 B _ 100%) _ 280 bytes / 86400 sec = 93 MB/s</li> <li>Photos: (28 B _ 20%) _ 200 KB / 86400 sec = 13 GB/s</li> <li>Videos: (28 B _ 10%) _ 2 MB * 30% users watch video / 86400 sec = 22 GB/s</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#database","title":"Database","text":"<ul> <li>We need a system that can efficiently<ul> <li>Store 1150 tweets per second</li> <li>Read 325 K tweets per second</li> <li>Read heavy system</li> </ul> </li> <li>This traffic won't be distributed evenly throught the day<ul> <li>At peak time, we should expect a few thousand write requests</li> <li>And 1 M read requests per second</li> </ul> </li> <li>Schema<ul> <li>User (id, name, email, dob, created_at, last_login_at)</li> <li>Tweet<ul> <li>id, user_id, content, created_at, favorites_count</li> <li>tweet_latitude, tweet_longitude, user_latitude, user_longitude</li> </ul> </li> <li>UserFollow (id, user_id, follower_id)</li> <li>Favorite (id, tweet_id, user_id)</li> </ul> </li> <li>Type<ul> <li>Relations are required, so SQL can be used</li> <li>Media storage: S3</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#component-design","title":"Component Design","text":"<ul> <li>Refer design for 'facebook'</li> </ul>"},{"location":"website_designs/social_media/twitter#scalability","title":"Scalability","text":""},{"location":"website_designs/social_media/twitter#data-sharding","title":"Data Sharding","text":""},{"location":"website_designs/social_media/twitter#based-on-user-id","title":"Based on User Id","text":"<ul> <li>This will store all the data (tweets, favorites, follows) of a user on the same shard</li> <li>We can get shard id using hash function like user_id % total_shards</li> <li>Issue 1: What if a user becomes popular<ul> <li>This will increase the load on that server increasing latency</li> <li>If the shard goes down, the data would be unavailable</li> </ul> </li> <li>Issue 2: Avid Users who end up having a lot of data<ul> <li>All of their data might not be accomodated on the same shard</li> <li>Multiple shards will lead to non-uniform distribution and higher latency</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#based-on-tweet-id","title":"Based on Tweet Id","text":"<ul> <li>Map each tweet id to a random server by using hash function like tweet_id % total_shards<ul> <li>Solves the problem of popular users but increases latency</li> <li>To search for tweets, we will have to query all the servers</li> <li>A centralized server will aggregate these results</li> </ul> </li> <li>Timeline generation<ul> <li>Find all the people the user follows</li> <li>Query all db servers to find tweets of these users</li> <li>Each db server will find the tweets, sort them by recency and return top tweets</li> <li>Merge all results and sort them again to get the top results</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#based-on-tweet-creation-time","title":"Based on Tweet Creation Time","text":"<ul> <li>We will have to query only a small set of servers to fetch the top tweets</li> <li>Issue: Traffic load won't be distributed<ul> <li>All the new tweets will be written to and read from one server</li> <li>The recent server will have very high load while other servers will be sitting idle</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#based-on-tweet-id-and-creation-time","title":"Based on Tweet Id and Creation Time","text":"<ul> <li>We can get benefits of both the approaches<ul> <li>If we don't store tweet creation time separately and use tweet id to reflect that</li> <li>It will be quick to find the latest tweets</li> <li>For this, tweet id should be universally unique in the system</li> </ul> </li> <li>Benefits of both recency and popular users<ul> <li>Decreases latency of only tweet id</li> <li>Decreases load on single server of only creation time</li> </ul> </li> <li>We still have to query all the servers for timeline generation<ul> <li>But the reads &amp; writes will be substantially quicker</li> <li>Since we don't have any secondary index (on creation time)<ul> <li>This will reduce the write latency</li> </ul> </li> <li>While reading, we don't need to filter on creation time<ul> <li>As the primary key has epoch time included in it</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#generating-tweet-id","title":"Generating Tweet Id","text":"<ul> <li>To generate the tweet id<ul> <li>We can take the current epoch time and append an auto-incrementing number to it</li> <li>Tweet Id = Creation Epoch Seconds + Sequence Number</li> <li>We can reset the auto-incrementing sequence every second</li> <li>We can figure out the shard number from this tweet id</li> </ul> </li> <li>Size of tweet id: 48 (31 for epoch sec + 17 for seq number)<ul> <li>Let's say our epoch time starts today</li> <li>Seconds in 50 years: 50 years _ 365 days _ 86400 secnds = 1.6 B</li> <li>Bits required for epoch seconds: 1.6 B &lt; 2^31 ~= 31</li> <li>Bits required for sequence: 17 (can store 2^17 = 130 K new tweets)<ul> <li>On average, we are expecting 1150 new tweets/s</li> <li>But we can get much higher number at peak times</li> </ul> </li> </ul> </li> <li>For fault tolerance and better performance<ul> <li>We can have two database servers to generate auto-incrementing keys</li> <li>One generating even numbered keys and other generating odd numbered keys</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#caching","title":"Caching","text":"<ul> <li>We can introduce cache for database servers to cache popular tweets &amp; users<ul> <li>We can use off-the-shelf solution like memcache that can store the whole tweet objects</li> <li>Based on usage patterns of clients we can determine how many cache servers are required</li> <li>LRU can be reasonable eviction policy</li> <li>We can also use CDN to push content closer to geography</li> </ul> </li> <li>If we go with 80-20 principle, 20% of tweets will generate 80% of the traffic<ul> <li>We can try to cache 20% of daily read volume from each shard</li> </ul> </li> <li>What if we cache the latest data<ul> <li>Let's say 80% of the users view tweets from the past 3 days only</li> <li>Storage required: 30 GB data/day * 3 days = 100 GB</li> <li>This data can easily fit into one server<ul> <li>But we should replicate it to multiple servers</li> <li>To distribute all the read traffic and reduce the load</li> </ul> </li> </ul> </li> <li>While generating a user's timeline<ul> <li>We can ask the cache servers if they have all the recent tweets</li> <li>If we don't have enough tweets in the cache, then only we query backend servers</li> </ul> </li> <li>Cache structure<ul> <li>It can be a hash table with<ul> <li>Key: user_id</li> <li>Value: Doubly linked list of all the tweets from the user in past 3 days</li> </ul> </li> <li>Since we want the most recent data<ul> <li>We can insert new tweets at the head of the linked list</li> <li>We can remove older tweets from tail to make space for newer tweets</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#replication-and-fault-tolerance","title":"Replication and Fault Tolerance","text":"<ul> <li>Since the system would be read heavy<ul> <li>We can have multiple secondary db servers for each DB partition</li> </ul> </li> <li>The secondary servers will be used for read traffic only<ul> <li>All the writes will first go to the primary servers</li> <li>And then will be replicated to secondary servers</li> </ul> </li> <li>Whenever the primary server goes down, we can failover to a secondary server</li> </ul>"},{"location":"website_designs/social_media/twitter#load-balancing","title":"Load Balancing","text":"<ul> <li>We can add load balancing layer at three places in our system<ul> <li>Between clients and application servers</li> <li>Between application servers and database replication servers</li> <li>Between aggregation servers and cache servers</li> </ul> </li> <li>Initially a simple round robin approach can be adopted<ul> <li>But it won't take server load into consideration</li> <li>To handle this, a more intelligent solution can be placed<ul> <li>That periodically queries servers about their load and adjusts traffic</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter#monitoring-metrics","title":"Monitoring Metrics","text":"<ul> <li>Collect data to get an instant insight into how our system is doing<ul> <li>New tweets per day &amp; per second?</li> <li>What is the daily peak time?</li> <li>Timeline delivery stats, how many tweets per day/second it is delivering?</li> <li>Average latency to refresh timeline</li> </ul> </li> <li>These will help us track if we need more replication, load balancing, caching</li> </ul>"},{"location":"website_designs/social_media/twitter_search","title":"Twitter Search","text":"<ul> <li>Stores and allows searching user tweets</li> </ul>"},{"location":"website_designs/social_media/twitter_search#requirements","title":"Requirements","text":"<ul> <li>Search query with multiple words combined with AND, OR</li> </ul>"},{"location":"website_designs/social_media/twitter_search#estimation","title":"Estimation","text":""},{"location":"website_designs/social_media/twitter_search#tweets","title":"Tweets","text":"<ul> <li>Users: 1.5 B</li> <li>Traffic<ul> <li>Daily active users: 800 M</li> <li>Daily tweets: 400 M</li> <li>Daily searches: 500 M</li> </ul> </li> <li>Storage<ul> <li>Tweet size: 300 bytes</li> <li>Daily Storage: 400 M tweets * 300 bytes = 120 GB/day = 1.38 MB/s</li> <li>Storage for 5 years: 120 GB _ 365 days _ 5 years = 200 TB</li> <li>Total storage for 5 years: 500 TB<ul> <li>Assuming that we want to keep 20% buffer: 250 TB</li> <li>We may also want to keep an extra copy of all tweets for fault tolerance: 500 TB</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter_search#index","title":"Index","text":"<ul> <li>Total words: 500 K<ul> <li>Assuming that we have around 300 K english words</li> <li>And around 200K famous nouns like names, cities, etc.</li> </ul> </li> <li>Words index size: 500 K * 5 bytes = 2.5 MB<ul> <li>Assuming average length of a word as 5 characters</li> <li>And that each character requires 1 byte</li> </ul> </li> <li>Words per tweet to index: 15<ul> <li>Assume that each tweet has 40 words on average</li> <li>We won't index prepositions, determinants, and other small words like etc.</li> <li>Let's assume we will have around 15 words that need to be indexed</li> </ul> </li> <li>Storage<ul> <li>Assume that we want to keep the index for the tweets in last 2 years</li> <li>Tweets per year: 400 M tweets/day * 365 days = 146 B</li> <li>Size of each tweet id: 5 bytes</li> <li>Storage for tweet ids: 146 B _ 2 years _ 5 bytes = 1460 GB</li> <li>Total Memory: 1460 GB tweets * 15 words/tweet + 2.5 MB words in english = 21 TB</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter_search#api","title":"API","text":"<ul> <li>search (returns json)<ul> <li>api_key, search_terms, max_results_to_return, sort_criteria, page_token</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter_search#database","title":"Database","text":"<ul> <li>We need to build an index to keep track of which words appear in which tweets<ul> <li>This will help us quickly find tweets that users are trying to search</li> </ul> </li> <li>Since the tweet queries will consist of words<ul> <li>We need to build a index that can tell which word comes in which tweet</li> </ul> </li> <li>The index would be a big distributed hash table<ul> <li>Key will be the word</li> <li>Value will be a list of tweet ids that contain the word</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter_search#component-design","title":"Component Design","text":""},{"location":"website_designs/social_media/twitter_search#ranking-service","title":"Ranking Service","text":"<ul> <li>Rank search results by social graph distance, popularity, relevance, recency</li> <li>We can calculate a popularity number based on likes and comments<ul> <li>And store it with the index</li> </ul> </li> <li>Each partition can sort the results based on this popularity number</li> </ul>"},{"location":"website_designs/social_media/twitter_search#scalability","title":"Scalability","text":""},{"location":"website_designs/social_media/twitter_search#storage","title":"Storage","text":"<ul> <li>Tweets<ul> <li>Since we need 500 TB of storage for 5 years</li> <li>And if we assume a modern server can store up to 4 TB</li> <li>We will need 125 servers to hold the required data</li> </ul> </li> <li>Index<ul> <li>Since we need 21 TB for index</li> <li>And if we assume that the server has 144 GB capacity</li> <li>We will need 152 servers</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter_search#sharding","title":"Sharding","text":""},{"location":"website_designs/social_media/twitter_search#based-on-tweet-id","title":"Based on Tweet Id","text":"<ul> <li>We will have to query all the servers and aggregate the results</li> </ul>"},{"location":"website_designs/social_media/twitter_search#based-on-words","title":"Based on Words","text":"<ul> <li>While building the index, we will iterate through all the words of a tweet<ul> <li>And calculate the hash of each word to find the server</li> <li>All tweets containing a given word can be found on one server</li> </ul> </li> <li>Issues<ul> <li>What if a word becomes popular? There will be high load on that server</li> <li>Over time, some words can end up storing a lot of tweet ids<ul> <li>This can lead to non-uniform distribution</li> </ul> </li> </ul> </li> <li>To solve this, we may have to repartition the data or use consistent hashing</li> </ul>"},{"location":"website_designs/social_media/twitter_search#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>We can have a secondary replica of each index server<ul> <li>If the primary server dies, it can failover to this server</li> </ul> </li> <li>What if both primary &amp; secondary index servers die at the same time?<ul> <li>We have to allocate a new server and rebuild the same index on it</li> <li>But we don't know what words &amp; tweets were kept on this server<ul> <li>The brute force solution would be to iterate through whole database</li> <li>This is inefficient and we won't be able to serve any queries during this</li> </ul> </li> <li>We can build a reverse index to map tweet id to their index server<ul> <li>Index builder server will hold this information<ul> <li>Index server Id : Set of tweet ids</li> </ul> </li> <li>Whenever an index server has to rebuild itself, it can ask the index builder server</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/twitter_search#caching","title":"Caching","text":"<ul> <li>Memcache with LRU can store all the popular tweets in memory</li> </ul>"},{"location":"website_designs/social_media/twitter_search#load-balancing","title":"Load Balancing","text":"<ul> <li>We can add load balancing layer at two places<ul> <li>Between clients and application servers</li> <li>Between application servers and backend servers</li> </ul> </li> <li>We can use weighted round robin to server traffic<ul> <li>And also take the server load into consideration</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed","title":"Facebook Newsfeed","text":"<ul> <li>Allows people to connect with different entities<ul> <li>Can become friends with other people</li> <li>Can join a group</li> <li>Can follow a page</li> </ul> </li> <li>Posts can include text, photos, videos, links<ul> <li>It can also include status updates like milestone, location, app activity</li> <li>Posts are visible depending on the privacy: public, private, group</li> <li>Other people can be tagged (@)</li> <li>Related topics can also be tagged (#)</li> </ul> </li> <li>Newsfeed<ul> <li>Compilation of scroll-able version of life story of friends and followed pages</li> <li>Constantly updating list of stories<ul> <li>Including posts, photos, videos, status updates</li> <li>Activities like liked, commented, followed, shared, etc.</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Generate a user's newsfeed based on the posts from people, pages, groups<ul> <li>It may contain text, images, videos</li> </ul> </li> <li>A user may have many friends and follow a large number of pages/groups</li> <li>Should support appending new posts as they arrive to the newsfeed for all active users</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Should be able to generate any user's newsfeed in real time<ul> <li>With maximum latency of 2s as seen by end user</li> </ul> </li> <li>A post shouldn't take more than 5s to make it to a user's feed<ul> <li>Assuming a new newsfeed request comes in</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#estimation","title":"Estimation","text":"<ul> <li>Assume on average a user has 300 friends and follows 200 pages</li> <li>Traffic<ul> <li>Daily Active Users: 300 M</li> <li>Newsfeed requests: 300 M * 5 = 1.5 B/day = 17.5 K/s<ul> <li>Assume each user fetches their timeline 5 times a day on average</li> </ul> </li> </ul> </li> <li>Storage<ul> <li>Posts in every user's newsfeed: 500 posts<ul> <li>We want to keep them in memory for quick fetch</li> </ul> </li> <li>Average post size: 1 KB</li> <li>Storage per user: 500 posts/user * 1 KB/post = 500 KB</li> <li>Daily storage: 300 M users/day * 500 KB/user = 150 TB</li> <li>Machines Required: 150 TB / 100 GB = 1500<ul> <li>Assuming a server can hold 100 GB</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#database","title":"Database","text":"<ul> <li>Relationships<ul> <li>A user can follow other entities and can become friends with other users</li> <li>Both users &amp; entities can post feed items which can contain text, images or videos</li> <li>Each feed item will have a user id of the author<ul> <li>And optionally an entity id pointing to the page or the group</li> </ul> </li> </ul> </li> <li>Schema<ul> <li>User (id, name, email, phone, dob, created_at, last_login_at)</li> <li>Entity (id, name, type, description, created_at, category, email, phone)</li> <li>UserFollow (user_id, entity_or_friend_id, type)</li> <li>Post<ul> <li>id, user_id, entity_id, contents, created_at, likes_count</li> <li>location_latitude, location_longitude</li> </ul> </li> <li>PostMedia (post_id, media_id)</li> <li>Media (id, type, description, path, location_latitude, location_longitude, created_at)</li> </ul> </li> <li>Type<ul> <li>Relational database (SQL) to store data</li> <li>S3 for media storage</li> <li>Cache to store newsfeed</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#feed-generation","title":"Feed Generation","text":"<ul> <li>To generate the feed for a user<ul> <li>Retrieve list of all users and entites that the user follows</li> <li>Retrieve last, most popular and relevant posts for these entities</li> <li>Rank these posts based on relevance to the user, recency, etc.</li> </ul> </li> <li>Use Pagination<ul> <li>Store this feed in the cache and return top posts (say 20)</li> <li>When the user reaches the end of the current feed, fetch the next 20 posts</li> </ul> </li> <li>We also need to consider the new incoming posts<ul> <li>If the user is online, rank and add incoming posts at regular intervals (say 5 mins)</li> <li>Then notify the user about the newer items in the feed that can be fetched</li> </ul> </li> <li>Pre-generate feed for frequent users</li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#real-time-generation","title":"Real-time Generation","text":"<ul> <li>If the feed is generated in real-time<ul> <li>User will have to wait while it is generated and cause high latency</li> </ul> </li> <li>It will be very slow for users with a lot of friends and entities<ul> <li>It will require sorting, merging, ranking of a huge number of posts</li> </ul> </li> <li>For live updates<ul> <li>Each status update will result in feed updates for all followers<ul> <li>This can result in high backlogs for the newsfeed generation service</li> </ul> </li> <li>The server pushing (or notifying about) new posts could lead to heavy loads<ul> <li>Especially for people or pages with a lot of followers</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#offline-generation","title":"Offline Generation","text":"<ul> <li>Pre-generate the newsfeed and store it in a memory<ul> <li>We can have dedicated servers to continuously generate feeds for users</li> <li>New feed can be generated from the last time the feed was generated</li> </ul> </li> <li>When a user requests for new feed<ul> <li>We can simply serve it from the pre-generated stored location</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#structure","title":"Structure","text":"<ul> <li>This data can be stored in a hash table in memory<ul> <li>Key will be user_id</li> <li>Value will be a struct containing<ul> <li>Linked hash map or tree map of feed items</li> <li>Timestamp of the last generated feed</li> </ul> </li> </ul> </li> <li>We can store 500 feed items per user initially and send 20 items per page<ul> <li>We can adjust this number later based on the usage pattern</li> </ul> </li> <li>When the new feed is generated<ul> <li>Add the new items to the head of the linked hash map</li> <li>And evict the old ones from the tail</li> </ul> </li> <li>When a user wants to fetch more items<ul> <li>The client can send the last feed item id it currently has</li> <li>We can jump to that feed item id in the linked hash map<ul> <li>And return the next batch or page of feed items</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#user-activity","title":"User Activity","text":"<ul> <li>Generate &amp; store newsfeed for all users or just avid and daily users</li> <li>Basic approach: LRU cache<ul> <li>Remove users that haven't logged in or accessed newsfeed for a long time</li> </ul> </li> <li>Smarter approach: Track login patterns of users<ul> <li>At what time of the day a user is active</li> <li>Which days of the week a user accesses the newsfeed</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#feed-publishing","title":"Feed Publishing","text":"<ul> <li>When the user loads the newsfeed<ul> <li>He has to request and pull feed items from the server</li> <li>When current feed ends, he can pull more data from the server</li> <li>The feed capacity can be different based on mobile or desktop</li> </ul> </li> <li>Fanout is the process of pushing a post to all the followers<ul> <li>The push approach is called fanout-on-write</li> <li>The pull approach is called fanout-on-load</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#pull-model","title":"Pull Model","text":"<ul> <li>Keeps all the recent feed data in memory</li> <li>Users can pull the data from the server regularly or when required</li> <li>Issues<ul> <li>New data might not be in feed until the user issues a pull request</li> <li>Hard to find the right pull cadence</li> <li>Most requests will return empty response (causing wastage of resources)</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#push-model","title":"Push Model","text":"<ul> <li>Once a user has published a post, push it to all the followers</li> <li>To efficiently handle it, users can maintain a long poll request with the server</li> <li>Advantages<ul> <li>No need to get friends list and fetch posts for each of them</li> <li>Significantly reduces read operations</li> </ul> </li> <li>Issues<ul> <li>User with millions of followers has to push updates to a lot of users</li> <li>Pushing every post to mobile users can consume a lot of bandwidth for them</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#hybrid-model","title":"Hybrid Model","text":"<ul> <li>Pull from popular users with high number of followers<ul> <li>And Push the updates of other users</li> </ul> </li> <li>Another approach can be to limit the fanout to only the online users</li> <li>To get more benefits<ul> <li>Combination of 'Push to notify' and 'pull for serving' end users is good</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#feed-ranking","title":"Feed Ranking","text":"<ul> <li>Select factors that make a post important<ul> <li>And figure out how to combine them to calculate a final ranking score</li> </ul> </li> <li>For example<ul> <li>Number of likes, comments, shares, recency, time of the update</li> <li>Whether the post has images or videos</li> </ul> </li> <li>This can be further improved by constant evaluation based on metrics like<ul> <li>User stickiness, retention, ads engagement, etc.</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_newsfeed#scalability","title":"Scalability","text":"<ul> <li>Refer to twitter &amp; instagram</li> <li>For feed data, which is being stored in memory, we can parition based on user id<ul> <li>For a given user, we are stroing 500 feed items</li> <li>So it can fit on a single server</li> <li>And we can easily query them on a single server</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger","title":"Facebook Messenger","text":"<ul> <li>Instant message service where users can send text messages to each other</li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Support one-on-one conversations between users</li> <li>Keep track of online/offline status of users</li> <li>Support persistent storage of chat history</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Real-time chat experience with minimum latency</li> <li>System should be highly consistent<ul> <li>Users should see the same chat history on all of their devices</li> </ul> </li> <li>High availability is desired but not over consistency</li> </ul> </li> <li>Extended Requirements<ul> <li>Group chats</li> <li>Push notifications for new messages users got when they were offline</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#estimation","title":"Estimation","text":"<ul> <li>Traffic<ul> <li>Daily Active Users: 500 M</li> <li>Daily Messages: 40/user = 20 B/day</li> </ul> </li> <li>Storage<ul> <li>Average size of a message: 100 bytes</li> <li>Storage required per day: 20 B messages/day * 100 bytes = 2 TB/day</li> <li>Storage required for 5 years: 2 TB/day _ 365 days _ 5 years = 3.6 PB</li> </ul> </li> <li>Bandwidth<ul> <li>Incoming: 2 TB / 86400 sec = 25 MB/s</li> <li>Outgoing: Same as incoming (25 MB/s)<ul> <li>The same incoming message will go out to another user</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#application-server","title":"Application Server","text":"<ul> <li>We will need a chat server that will be the central piece<ul> <li>Orchestrating all communications between users</li> <li>A user will send a message to another user through this chat server</li> <li>The server will store the message to its database and pass it to the recipient</li> </ul> </li> <li>Workflow<ul> <li>User A sends message to User B through chat server</li> <li>Server receives the message and sends an acknowledgement to user A</li> <li>Server stores the message in its database</li> <li>Server sends the message to User B</li> <li>User B receives the message and sends an acknowledgement to the server</li> <li>Server notifies user A that message has been delivered successfully to user B</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#commponent-design","title":"Commponent Design","text":"<ul> <li>Let's try to build a simple solution where everything runs on one server</li> <li>At the high level, our system needs to handle the following use cases<ul> <li>Receive incoming messages and deliver outgoing messages</li> <li>Store and retrieve messages from the database</li> <li>Track which user is online or has gone offline<ul> <li>Notify all the users about changes in this status</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#message-handling","title":"Message Handling","text":"<ul> <li>To send messages to another user<ul> <li>A user needs to connect to the server and post messages for that user</li> </ul> </li> <li>To get messages from the server<ul> <li>The user has two options: pull model &amp; push model</li> </ul> </li> <li>Traffic<ul> <li>Possible concurrent connections: 500 M (daily active users)</li> <li>Assuming a modern server can handle 50K concurrent connections</li> <li>Total servers required: 10 K</li> </ul> </li> <li>How do we know which server holds connection to which user<ul> <li>We can introduce a software load balancer in front of our chat servers</li> <li>That will map user_id to a server_id to redirect a request</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#pull-model","title":"Pull Model","text":"<ul> <li>User can periodically ask server if any new messages<ul> <li>Server needs to keep track of messages that are still waiting to be delivered<ul> <li>Server will return all these pending messages once the user connects to the server</li> </ul> </li> </ul> </li> <li>To minimize latency, the user will have to check the server frequently<ul> <li>And most of the time they will get an empty response</li> <li>This will waste a lot of resources</li> <li>So this does not look like an efficient solution</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#push-model","title":"Push Model","text":"<ul> <li>User can keep a connection open with the server<ul> <li>Server will notify the user whenever there are any new messages</li> <li>Server does not need to keep track of pending messages</li> </ul> </li> <li>It has minimum latency since messages are delivered instantly</li> <li>Open connection can be maintained through long polling or web sockets</li> <li>Long Polling<ul> <li>Client requests info from the server</li> <li>Server holds the request and sends response whenever there is new data<ul> <li>Instead of sending an empty response immediately</li> </ul> </li> <li>After getting response<ul> <li>Client can immediately issue another request for future updates</li> </ul> </li> <li>The request can timeout, in which case the client has to open a new request</li> </ul> </li> <li>Tracking open connections<ul> <li>Maintain a hash table with user_id as key and connection_object as value</li> <li>Whenever server receives a message for a user<ul> <li>It can look up the hash table for the connection_object</li> <li>And send the message on the open request</li> </ul> </li> </ul> </li> <li>If the receiver is offline or if its long poll request timed out<ul> <li>Server can store the message for a while and retry when the receiver reconnects</li> <li>Or, we can ask the sender to retry sending the message</li> <li>If there are any deliver failures, they can be notified to the user</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#sequencing-of-messages","title":"Sequencing of messages","text":"<ul> <li>We can send the timestamp of the message from the client<ul> <li>But what if the time in user devices are not synced (e.g. 5 mins ahead)</li> </ul> </li> <li>We can store the timestamp when the message was received by the server<ul> <li>But it can still have ordering issues<ul> <li>U1 sends M1 to U2 received at T1 by the server</li> <li>U2 sends M2 to U1 received at T2 by the server, such that T2 &gt; T1</li> <li>The server will then send M1 to U2 and M2 to U1</li> <li>U1 will see M1 first and then M2</li> <li>U2 will see M2 first and then M1</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#storing-and-retrieving-messages","title":"Storing and Retrieving Messages","text":"<ul> <li>Whenever the chat server receives a new message, it needs to store it in the database</li> <li>To do so, we have two options<ul> <li>Start a separate thread, which will work with db to store the message</li> <li>Send an async request to db to store the message</li> </ul> </li> <li>Things to keep in mind<ul> <li>How to efficiently work with the database connection pool</li> <li>How to retry failed requests</li> <li>Where to log requests that failed after multiple retries</li> <li>How to retry these logged requests when the issue is resolved</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#storage-system","title":"Storage System","text":"<ul> <li>We will have a large number of messages that need to be inserted into database</li> <li>A user will access messages in sequential order, latest to oldest</li> <li>So we need to have a database that can support<ul> <li>Very high rate of small updates</li> <li>Fetching a range of records quickly</li> </ul> </li> <li>We cannot afford to read/write a row from database<ul> <li>Every time a user receives/sends a message</li> <li>That will have high latency for basic operations</li> <li>And will create a huge load on databases</li> <li>So we cannot use RDBMS like MySQL or NoSQL like MongoDB</li> </ul> </li> <li>These requirements can be met with a wide column database like HBase</li> <li>Clients should paginate while fetching data from the server<ul> <li>Page size could be different for different clients</li> <li>E.g. mobiles have smaller screens, so fewer messages would suffice</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#hbase","title":"HBase","text":"<ul> <li>HBase is a column-oriented key-value NoSQL database</li> <li>Can store multiple values against one key into multiple columns</li> <li>Modeled after google's big table, runs on top of hadoop distributed file system (HDFS)</li> <li>Groups data together to store new data in a memory buffer<ul> <li>Once the bugger is full, it dumps the data to the disk</li> <li>Helps storing a lot of small data quickly</li> <li>Also helps fetching rows by the key or scanning ranges of rows</li> </ul> </li> <li>Efficient in storing variably sized data</li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#user-statuses","title":"User Statuses","text":"<ul> <li>We need to keep track of online/offline status of users<ul> <li>And notify all relevant users whenever these statuses change</li> </ul> </li> <li>Since we are maintaining a connection object on the server for all active users<ul> <li>In a hash table (refer 'push model' under message handling)</li> <li>We can figure a user's current status from this</li> </ul> </li> <li>Broadcasting each user status change to relevant active users<ul> <li>Will consume a lot of resources (500 M active users at any time as estimated before)</li> </ul> </li> <li>When a client starts app, it can pull the current status of all the friends<ul> <li>Whenever the user sends a message to another user that has gone offline<ul> <li>Update the status on the client</li> </ul> </li> <li>Whenever any new friend comes online<ul> <li>The server can broadcast its status with a delay of few seconds</li> <li>Keep this buffer time in case the friend goes offline immediately</li> </ul> </li> <li>For the friends that are being showed on the user's viewport<ul> <li>Client can pull their status at regular non-frequent intervals</li> <li>Stale status is alright for a while, new user will anyway broadcast their status</li> </ul> </li> <li>When the client starts a new chat with another user, pull the status at that time</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#scalability","title":"Scalability","text":""},{"location":"website_designs/social_media/facebook_messenger#data-partitioning","title":"Data Partitioning","text":"<ul> <li>Since we will be storing a lot of data (3.6 PB for 5 years)</li> <li>We need to distribute it onto multiple database servers efficiently</li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#based-on-user-id","title":"Based on User Id","text":"<ul> <li>All messages of a user will be on the same database<ul> <li>This will be quick to fetch chat history of a specific user</li> <li>Required shards (if one shard is 4 TB): 3.6 PB / 4 TB = 900</li> <li>Let's say we keep 1 K shards, so shard number will be hash(user_id) % 1000</li> </ul> </li> <li>In the beginning, we can start with fewer database servers<ul> <li>With multiple shards residing on one physical server</li> <li>Since we can have multiple database instances on a server<ul> <li>We can easily store multiple partitions on a single server</li> </ul> </li> <li>Our hash function needs to understand this logical paritioning scheme<ul> <li>So that it can map multiple logical partitions on one physical server</li> </ul> </li> </ul> </li> <li>Since we will store an unlimited history of messages<ul> <li>We can start with a large number of logical partitions<ul> <li>Which will be mapped to fewer physical servers</li> </ul> </li> <li>As our storage demand increases, we can add more physical servers<ul> <li>To distribute our logical partitions</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#based-on-message-id","title":"Based on Message Id","text":"<ul> <li>Not a good scheme</li> <li>Messages of a user will scattered on separate shards</li> <li>Fetching a range of messages of a chat would be very slow</li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#cache","title":"Cache","text":"<ul> <li>We can cache a few recent conversations (say last 5)<ul> <li>With a few recent messages (say last 15)</li> </ul> </li> <li>Since we decided to store all of the user's messages on one shard<ul> <li>Cache for a user should entirely reside on one machine too</li> </ul> </li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#load-balancing","title":"Load Balancing","text":"<ul> <li>We will need a load balancer in front of the chat servers<ul> <li>That can map user_id to a server that holds the connection for the user</li> </ul> </li> <li>Similarly, we would need a load balancer for the cache servers</li> </ul>"},{"location":"website_designs/social_media/facebook_messenger#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>If a server goes down<ul> <li>Should we have a mechanism to transfer the active connections to some other server?</li> </ul> </li> <li>It's extremely hard to failover TCP connections to other servers</li> <li>An easier approach can be to have clients automatically reconnect</li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox","title":"Dropbox","text":"<ul> <li>Platforms: dropbox, google drive, one drive</li> <li>File hosting service (or cloud file storage)</li> <li>Enables users to store their data on remote servers</li> <li>Simplifies the storage and exchange of digital resources<ul> <li>Users can access data across multiple devices with different platforms &amp; OS</li> <li>Gives portable access from various geographical locations at any time</li> <li>Offers 100% reliability and durability of data by keeping multiple copies of data</li> <li>Unlimited storage space, though after certain limit it needs to be paid for</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Users can upload &amp; download their files from any device</li> <li>Files or folders can be shared with other users</li> <li>Files &amp; folders should be automatically synced among devices</li> <li>Should support storing large files up to a GB</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>ACID properties should be followed for all file operations<ul> <li>Atomicity, Consistency, Isolation, Durability</li> </ul> </li> </ul> </li> <li>Extended Requirements<ul> <li>Support offline editing, the changes should be synced as soon as the device is online</li> <li>Snapshotting data so that user can go back to any version of a file</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#considerations","title":"Considerations","text":"<ul> <li>We should expect huge read &amp; write volumes</li> <li>Read-write ratio is expected to be nearly the same</li> <li>Internally, files can be stored in small parts or chunks (say 4 MB)<ul> <li>If a file upload fails, only the failed chunks will be required to be re-uploaded</li> <li>Will reduce data exchange by transferring only the updated chunks of files while syncing</li> <li>For small changes, clients can intelligently upload diffs instead of the whole chunk</li> </ul> </li> <li>Keeping a local copy of metadata (file name, size, etc) with client<ul> <li>Can save a lot of round trips to the server</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#estimation","title":"Estimation","text":"<ul> <li>Traffic<ul> <li>Total users: 500 M</li> <li>Daily active users: 100 M</li> <li>Active connections: 1 M/min</li> <li>Devices per user: 3</li> </ul> </li> <li>Storage<ul> <li>Average number of files per user: 200</li> <li>Average file size: 100 KB</li> <li>Total files: 500 M users * 200 files/user = 100 B</li> <li>Storage required: 100 B files * 100 KB/file = 10 PB</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#application-servers","title":"Application Servers","text":"<ul> <li>User will specify a folder to be used as workspace on their device<ul> <li>Data placed in this workspace will be synced with the cloud</li> <li>Any modifications &amp; deletions will be reflected to other devices</li> <li>So we need some servers that can help clients to upload &amp; download files to cloud</li> </ul> </li> <li>We need to store files and their metadata (filename, size, directory, etc.)<ul> <li>And who this file is shared with</li> <li>Some servers that can facilitate updating metadata</li> </ul> </li> <li>We also need some mechanismm to notify all clients whenever an update happens<ul> <li>So that they can synchronize their files</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#component-design","title":"Component Design","text":""},{"location":"website_designs/storage_and_streaming/dropbox#client","title":"Client","text":"<ul> <li>A client application will monitor and sync data in the workspace folder with the cloud</li> <li>It will work with storage servers to upload, download &amp; modify files</li> <li>It will interact with synchronization service to handle any metadata updates<ul> <li>Like change in file name, size, modified date, etc.</li> </ul> </li> <li>It will also handle conflicts due to offline or concurrent updates</li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#handling-file-transfer","title":"Handling File Transfer","text":"<ul> <li>We can break each file into smaller chunks (say 4 MB)<ul> <li>So that only the modified chunks need to be transferred and not the whole file</li> <li>Server and clients can calculate a hash (e.g., SHA-256)<ul> <li>If we already have a chunk with a similar hash (even from another user)</li> <li>We don\u2019t need to create another copy, we can use the same chunk</li> </ul> </li> </ul> </li> <li>We can calculate an optimal chunk size based on<ul> <li>Type of storage devices used in the cloud<ul> <li>To optimize space utilization</li> <li>And input/output operations per second (IOPS)</li> </ul> </li> <li>Network bandwidth</li> <li>Average file size in the storage</li> </ul> </li> <li>In the metadata, keep a record of each file &amp; the chunks that constitute it<ul> <li>Keep a copy of metadata with the client</li> <li>It will enable us to do offline updates</li> <li>And save a lot of round trips to update remote metadata</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#syncing-changes","title":"Syncing Changes","text":"<ul> <li>Handling slow servers<ul> <li>Clients should exponentially back off if the server is busy or not responding</li> <li>If the server is too slow, clients should delay their retries<ul> <li>And this delay should increase exponentially</li> </ul> </li> </ul> </li> <li>For web clients, check for file changes regularly</li> <li>For mobile clients, sync on demand to save user's bandwidth and space</li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#listen-changes-on-other-clients","title":"Listen Changes on Other Clients","text":"<ul> <li>Polling<ul> <li>Clients can periodically check with the server for changes</li> <li>But since this is periodic, there can be delay in reflecting changes locally</li> <li>If the client checks frequently, it will waste bandwidth for both client &amp; server<ul> <li>Server will have to keep serving these requests and return empty response</li> </ul> </li> <li>Hence, this is not scalable</li> </ul> </li> <li>Long Polling<ul> <li>Clients can request info from server<ul> <li>With the expectation that server may respond immediately</li> </ul> </li> <li>If the server has no new data for the client<ul> <li>It will hold the request open &amp; wait for new data instead of sending empty response</li> <li>Once the server has some new info, it will send the response to the client</li> </ul> </li> <li>After receiving the response<ul> <li>Client can immediately issue another request for future updates</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#components","title":"Components","text":"<ul> <li>Based on the above considerations, we can divide the client into four parts</li> <li>Interal Metadata Database<ul> <li>Will keep track of all the files, chunks, their versions, their locations</li> </ul> </li> <li>Chunker<ul> <li>Will split the files into chunks</li> <li>Will reconstruct a file from its chunks</li> <li>Will detect the chunks that have been modified and transfer those to cloud</li> </ul> </li> <li>Watcher<ul> <li>Will monitor the local workspace folder</li> <li>Will notify the indexer about any actions performed by users<ul> <li>E.g. creating, deleting, updating files or folders</li> </ul> </li> <li>Will listen to any changes happening on the other clients<ul> <li>That are broadcasted by synchronization service</li> </ul> </li> </ul> </li> <li>Indexer<ul> <li>Will process the events received from the watcher</li> <li>Will update the internal metadata database about chunks of modified files</li> <li>Once the chunks are successfully submitted/downloaded to the cloud<ul> <li>It will communicate with the remote synchronization service</li> <li>To broadcast changes to other clients and update the remote metadata database</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#metadata-database","title":"Metadata Database","text":"<ul> <li>Responsible for maintaining versioning<ul> <li>And metadata info about files, chunks, users, devices, workspaces</li> </ul> </li> <li>Can be SQL or NoSQL<ul> <li>NoSQL data stores don't support ACID in favor of scalability and performance<ul> <li>So ACID properties need to be supported in synchronization service programmatically</li> </ul> </li> <li>Relational database can simplify the implementation of the synchronization service<ul> <li>Since it natively supports ACID properties</li> </ul> </li> </ul> </li> <li>Regardless of the type of database<ul> <li>Synchronization service should be able to provide a consistent view of the files</li> <li>Especially if multiple users are working on the same file simultaneously</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#synchronization-service","title":"Synchronization Service","text":"<ul> <li>Most important part due to its critical role in managing metadata &amp; synchronizing files<ul> <li>It will process and sync file updates made by a client</li> <li>It will sync local databases of clients with the info stored in the metadata db</li> </ul> </li> <li>Desktop clients will communicate with synchronization service to<ul> <li>Obtain files &amp; updates from the cloud storage</li> <li>Send files &amp; updates to the cloud storage, other clients, and other users</li> <li>If a client was offline for a long period<ul> <li>It can poll the system for new updates as soon as they come online</li> </ul> </li> </ul> </li> <li>When the synchronization service receives an update request<ul> <li>It will check with the metadata db for consistency and then proceed with the update</li> <li>A notification will be sent to all subscribed users or devices to report the file update</li> </ul> </li> <li>It should employ file transfer with low response time<ul> <li>To reduce the amount of the data that needs to be synchronized<ul> <li>It can employ a differencing algorithm</li> <li>Which transmits only the difference between two versions of a file</li> <li>This also decreases bandwidth consumption and cloud data storage for the end user</li> </ul> </li> <li>Further details have been discussed in 'handling file transfer' above</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#message-queuing-service","title":"Message Queuing Service","text":"<ul> <li>It should support<ul> <li>Asynchronous message-based communication between clients and synchronization service</li> <li>Asynchronous and loosely coupled message-based communication between distributed components</li> </ul> </li> <li>For an efficient and scalable synchronization protocol<ul> <li>We can use this message queuing service</li> <li>And add a communication/messaging middleware between the clients and the service</li> </ul> </li> <li>Messaging middleware should be able to handle a substantial number of requests<ul> <li>It should be able to efficiently store any number of messages</li> <li>In a highly available, reliable and scalable queue</li> </ul> </li> <li>It will implement two types of queues in our system<ul> <li>Global Request Queue that all the clients will share<ul> <li>Client requests to update the metadata db will be sent to this queue first</li> <li>Synchronization service will take these requests to update the metadata</li> </ul> </li> <li>Response Queues for individual subscribed clients<ul> <li>Responsible for delivering the update messages to each client</li> <li>Required since a message will be deleted from the global queue once received by a client</li> </ul> </li> </ul> </li> <li>Multiple instances of synchronization service can be deployed<ul> <li>Which will receive requests from a global request queue</li> <li>And the middleware will be able to balance their load</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#cloudblock-storage","title":"Cloud/Block Storage","text":"<ul> <li>Stores chunks of files uploaded by the users</li> <li>Clients directly interact with the storage to send and receive objects</li> <li>Separation of the metadata from storage<ul> <li>Enables us to use any storage either in cloud or in-house</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#file-processing-workflow","title":"File Processing Workflow","text":"<ul> <li>Interaction when client A updates a file that is shared with client B and C<ul> <li>Client A uploads chunks to cloud storage</li> <li>Client A updates metadata and commits changes</li> <li>Client A gets confirmation</li> <li>Notifications are sent to Clients B and C about the changes</li> <li>Client B and C receive metadata changes and download updated chunks</li> </ul> </li> <li>If they are not online at the time of the update<ul> <li>Message queuing service keeps the update notifications</li> <li>In their respective queues until they become available later</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#data-deduplication","title":"Data Deduplication","text":"<ul> <li>Technique for eliminating duplicate copies of data to improve storage utilization</li> <li>Can also be applied to network data transfers to reduce the number of bytes that must be sent</li> <li>For each new incoming chunk, calculate a hash<ul> <li>Compare that with all the hashes of the existing chunks</li> <li>To see if we already have the same chunk present in our storage</li> </ul> </li> <li>Can implement deduplication in two ways in our system</li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#post-process-deduplication","title":"Post-process deduplication","text":"<ul> <li>New chunks are first stored on the storage device</li> <li>Later some process analyzes the data looking for duplication</li> <li>Benefits<ul> <li>Clients will not need to wait for the hash calculation<ul> <li>Or lookup to complete before storing the data</li> </ul> </li> <li>Ensuring that there is no degradation in storage performance</li> </ul> </li> <li>Drawbacks<ul> <li>Unnecessarily storing duplicate data, though for a short time</li> <li>Duplicate data will be transferred, consuming bandwidth</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#in-line-deduplication","title":"In-line deduplication","text":"<ul> <li>Deduplication hash calculations can be done in real-time<ul> <li>As the clients are entering data on their device</li> </ul> </li> <li>If our system identifies a chunk which it has already stored<ul> <li>Only a reference to the existing chunk will be added in the metadata</li> <li>This approach will give us optimal network and storage usage</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#permissions","title":"Permissions","text":"<ul> <li>One of the primary concerns users will have is the privacy and security of their data</li> <li>Especially since in our system<ul> <li>Users can share their files with other users or even make them public</li> </ul> </li> <li>To handle this, we will be storing permissions of each file in our metadata DB<ul> <li>To reflect what files are visible or modifiable by any user</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#scalability","title":"Scalability","text":""},{"location":"website_designs/storage_and_streaming/dropbox#metadata-partitioning","title":"Metadata Partitioning","text":"<ul> <li>To scale out metadata db, we need to partition it<ul> <li>So that it can store information about millions of users</li> <li>And billions of files/chunks</li> </ul> </li> <li>We need to come up with a partitioning scheme<ul> <li>That would divide and store our data to different db servers</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#vertical-partitioning","title":"Vertical Partitioning","text":"<ul> <li>Store tables related to one particular feature on one server<ul> <li>Store all the user related tables in one database</li> <li>Store all files/chunks related tables in another database</li> </ul> </li> <li>Although it is straightforward to implement, it has some issues:<ul> <li>Will we still have scale issues?<ul> <li>What if we have trillions of chunks to be stored?</li> <li>And our database cannot support storing such huge number of records?</li> <li>How would we further partition such tables?</li> </ul> </li> <li>Joining two tables in two separate databases can cause performance and consistency issues<ul> <li>How frequently do we have to join user and file tables?</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#range-based-partitioning","title":"Range Based Partitioning","text":"<ul> <li>Store files/chunks in separate partitions based on the first letter of the file path<ul> <li>Save all the files starting with letter \u2018A\u2019 in one partition</li> <li>Those that start with letter \u2018B\u2019 into another partition</li> </ul> </li> <li>We can even combine certain less frequently occurring letters into one database partition</li> <li>We should come up with this partitioning scheme statically<ul> <li>So that we can always store/find a file in a predictable manner</li> </ul> </li> <li>Drawback: Can lead to unbalanced servers<ul> <li>If we decide to put all files starting with letter E into a DB partition</li> <li>And later we realize that we have too many files that start with letter E</li> <li>To such an extent that we cannot fit them into one DB partition</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#hash-based-partitioning","title":"Hash-Based Partitioning","text":"<ul> <li>Take a hash of the object and figure out the DB partition<ul> <li>Can take the hash of the FileID of the File object</li> </ul> </li> <li>Hashing function will randomly distribute objects into different partitions<ul> <li>Example: Map any ID to a number between 1 to 256, and this number would be the partition</li> </ul> </li> <li>Drawbacks<ul> <li>Can still lead to overloaded partitions</li> <li>Can be solved by using Consistent Hashing</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#caching","title":"Caching","text":"<ul> <li>Can have two kinds of caches in our system</li> <li>To deal with hot files/chunks, we can have a cache for Block storage<ul> <li>Can use Memcache that can store whole chunks with their respective IDs/Hashes</li> <li>Before hitting block storage, block servers can quickly check if the cache has desired chunk</li> </ul> </li> <li>Based on clients\u2019 usage pattern we can determine how many cache servers we need<ul> <li>A high-end commercial server can have up to 144GB of memory</li> <li>So one such server can cache 36K chunks</li> </ul> </li> <li>Which cache replacement policy would best fit our needs?<ul> <li>How do we choose to replace a chunk with a newer/hotter chunk?</li> <li>Least Recently Used (LRU) can be a reasonable policy</li> </ul> </li> <li>Similarly, we can have a cache for Metadata DB</li> </ul>"},{"location":"website_designs/storage_and_streaming/dropbox#load-balancer","title":"Load Balancer","text":"<ul> <li>We can add load balancing layer at two places<ul> <li>Between Clients and Block servers</li> <li>Between Clients and Metadata servers</li> </ul> </li> <li>Initially, a simple Round Robin approach can be adopted<ul> <li>Distributes incoming requests equally among backend servers</li> <li>Simple to implement and does not introduce any overhead</li> <li>If a server is dead, LB will take it out of the rotation and stop sending any traffic</li> <li>But it won\u2019t take server load into consideration</li> <li>If a server is overloaded or slow, LB won't stop sending new requests to it</li> <li>To handle this, a more intelligent LB solution can be placed</li> <li>That periodically queries backend server about their load and adjusts traffic</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix","title":"Youtube or Netflix","text":"<ul> <li>Platforms: youtube, netflix, vimeo, dailymotion</li> <li>Users can upload, view, search, share videos</li> <li>Users can also like, dislike, comment, report videos</li> <li>It also shows number of views &amp; comments on a particular video</li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>User can upload, view, search, share videos</li> <li>Record stats of videos like likes, dislikes, total views, etc.</li> <li>User can also add &amp; view comments on a video</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Should be highly reliable, any uploaded video should not be lost</li> <li>Should be highly available</li> <li>Consistency over availability</li> <li>Real-time experience while watching videos and should not feel any lag</li> </ul> </li> <li>Extended Requirements<ul> <li>Video recommendations</li> <li>Trending videos</li> <li>Channels</li> <li>Subscriptions</li> <li>Watch later</li> <li>Favorites</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#estimation","title":"Estimation","text":"<ul> <li>Traffic<ul> <li>Total users: 1.5 B</li> <li>Daily active users: 800 M</li> <li>Views: 800 M users/day * 5 views/user = 4 B/day = 46 K/s</li> <li>Assume that upload-view ratio is 1 : 200</li> <li>Uploads: 46 K / 200 = 230 videos/s</li> </ul> </li> <li>Storage<ul> <li>Assume that every minute, 500 hours of videos are uploaded</li> <li>Assume that 1 min of video requires 50 MB on average<ul> <li>Since videos need to be stored in multiple formats</li> </ul> </li> <li>Storage required for videos: 1.5 TB/min = 25 GB/s<ul> <li>500 hours/min _ 60 mins/hour _ 50 MB/min</li> <li>This does not include video compression &amp; replication</li> </ul> </li> </ul> </li> <li>Bandwidth<ul> <li>Assume that each video upload takes a bandwidth of 10MB/min</li> <li>Incoming: 500 hrs/min _ 60 mins/hr _ 10 MB/min = 300 GB/min = 5 GB/sec</li> <li>Outgoing: 1 TB/sec (1:200 ratio)</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#api","title":"API","text":"<ul> <li>upload_video<ul> <li>api_key, title, description, tags, category_id, language, location, video</li> </ul> </li> <li>search_video<ul> <li>api_key, search_query, location, maximum_videos_to_return, page_token</li> </ul> </li> <li>stream_video<ul> <li>api_key, video_id, offset, codec, resolution</li> <li>Codec &amp; resolution are required to support different devices</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#high-level-design","title":"High Level Design","text":"<ul> <li>Processing Queue<ul> <li>Each uploaded video will be pushed to processing queue</li> <li>To be dequeued later for encoding, thumbnail generation, storage</li> </ul> </li> <li>Encoder: To encode each uploaded video into multiple formats</li> <li>Thumbnail Generator: To generate a few thumbnails for each video</li> <li>Database</li> <li>Video &amp; thumbnail storage</li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#database","title":"Database","text":"<ul> <li>Schema<ul> <li>Video (id, title, description, size, thumbnail, uploader, likes, dislikes, views)</li> <li>Comment (id, video_id, user_id, comment, created_at)</li> <li>User (id, name, email, age, address, registration details)</li> </ul> </li> <li>Storage<ul> <li>Regular Database: SQL</li> <li>Video Storage: Distributed file storage like HDFS, GlusterFS</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#thumbnails","title":"Thumbnails","text":"<ul> <li>Thumbnails are small files, say a maximum of 5 KB each<ul> <li>Assuming every video will have five thumbnails</li> <li>We would need a efficient storage system that can serve a huge read traffic</li> </ul> </li> <li>Read traffic for thumbnails will be huge compared to videos<ul> <li>Users will watch one video at a time</li> <li>But they might look at a page that has 20 thumbnails of other videos</li> </ul> </li> <li>If we store them on a disk storage<ul> <li>Given that we have a huge number of files</li> <li>We will have to perform a lot of seeks to different locations on the disk</li> <li>This is inefficient and will result in high latencies</li> </ul> </li> <li>Bigtable can be a reasonable choice<ul> <li>It combines multiple files into a block to store on the disk</li> <li>Efficient in reading a small amount of data</li> </ul> </li> <li>Cache popular thumbnails, it's easy to cache them since they are small in size</li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#component-design","title":"Component Design","text":"<ul> <li>System would be read heavy</li> <li>So we will focus on building a system that can retrieve videos quickly</li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#manage-read-traffic","title":"Manage Read Traffic","text":"<ul> <li>We should segregate the read traffic from write traffic</li> <li>Since we will have multiple copies of each video<ul> <li>We can distribute the read traffic on different servers</li> </ul> </li> <li>For metadata, we can have master-slave configuration<ul> <li>Writes will go to the master first and then gets applied to the slaves</li> <li>But slaves may return the stale results for some time<ul> <li>This might be acceptable in our system as it would be for short while</li> <li>And user can see the new videos after a few milliseconds</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#uploading-encoding","title":"Uploading &amp; Encoding","text":"<ul> <li>Videos could be huge<ul> <li>If the connection drops while uploading</li> <li>We should suppport resuming from the same point</li> </ul> </li> <li>Encoding<ul> <li>Store newly uploaded videos on the server</li> <li>Add a new task to processing queue to encode the video into multiple formats</li> <li>Once the encoding is completed, notify the uploader<ul> <li>And make the video available for viewing &amp; sharing</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#video-deduplication","title":"Video Deduplication","text":"<ul> <li>With a huge number of users uploading a massive amount of video data<ul> <li>Our service may have to deal with widespread video duplication</li> </ul> </li> <li>Duplicate videos often differ in aspect ratios or encodings<ul> <li>Can contain overlays or additional borders</li> <li>Or can be excerpts from a longer original video</li> </ul> </li> <li>Proliferation of duplicate videos can have an impact on many levels<ul> <li>Wastage of storage space</li> <li>Degraded cache efficiency by taking up space that could be used for unique content</li> <li>Network usage<ul> <li>Will increase the amount of data that must be sent over the network</li> <li>To in-network caching systems</li> </ul> </li> <li>Energy wastage due to higher storage, inefficient cache and network usage</li> </ul> </li> <li>User experience will also be impacted<ul> <li>Duplicate search results</li> <li>Longer video startup times</li> <li>Interrupted streaming</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#inline-deduplication","title":"Inline Deduplication","text":"<ul> <li>De-duplication makes most sense early when a user is uploading a video<ul> <li>As compared to post-processing it to find that the video is duplicate</li> </ul> </li> <li>Will save resources used to encode, transfer and store the duplicate video</li> <li>When user starts uploading, run video matching algorithm to find duplicates<ul> <li>Like block matching, phase correlation</li> </ul> </li> <li>If a duplicate is found<ul> <li>Stop the upload and use the existing copy</li> <li>Or continue the upload and use the new video if it is of higher quality</li> </ul> </li> <li>If the uploaded video is a subpart of an existing video or vice versa<ul> <li>We can intelligently divide the video into smaller chunks</li> <li>So that we only upload the parts that are missing</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#scalability","title":"Scalability","text":""},{"location":"website_designs/storage_and_streaming/youtube_netflix#sharding","title":"Sharding","text":"<ul> <li>We will have a huge number of new videos every day<ul> <li>And our read load is extremely high</li> <li>Hence, we need to distribute the data on multiple machines</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#based-on-user-id","title":"Based on User Id","text":"<ul> <li>All user data will stored on the same server</li> <li>To search videos by title, we will have to query all servers<ul> <li>And each server will return a set of videos</li> <li>A centralized server will then aggregate and rank these results</li> </ul> </li> <li>Issues<ul> <li>If a user becomes popular<ul> <li>It can increase the load on that specific server</li> <li>And create a performance bottleneck</li> </ul> </li> <li>Over time, some users can end up storing a lot of videos<ul> <li>This can lead to non-uniform distribution of data</li> </ul> </li> </ul> </li> <li>To recover from these issues<ul> <li>We will have to repartition/redistribute the data</li> <li>Or use consistent hashing to balance the load between the servers</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#based-on-video-id","title":"Based on Video Id","text":"<ul> <li>The hash function will map each video id to a randonm server</li> <li>To find videos of a user, we will have to query all servers and aggregate them</li> <li>This solves the problem of popular users but shifts it to popular videos</li> <li>We can introduce cache in front of db servers to store popular videos</li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#replication","title":"Replication","text":"<ul> <li>Read traffic: Secondary servers</li> <li>Meta-data: Master-Slave, write -&gt; master, read -&gt; slave</li> <li>Multiple copies of each video</li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#caching","title":"Caching","text":"<ul> <li>To serve globally distributed user, we need a massive-scale video delivery system</li> <li>The service should push its content closer to the user<ul> <li>Using a large number of geographically distributed video cache servers</li> </ul> </li> <li>We need a strategy to maximize user performance<ul> <li>And evenly distribute the load on cache servers</li> </ul> </li> <li>We can go with the 80-20 principle<ul> <li>Cache 20% of daily read volume that will generate by 80% traffic</li> <li>We can change this strategy after getting more data</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#content-delivery-network-cdn","title":"Content Delivery Network (CDN)","text":"<ul> <li>CDN is a system of distributed servers that deliver web content to a user<ul> <li>Based on the geographic location of user</li> <li>The origin of the web page and a content delivery server</li> </ul> </li> <li>CDNs replicate content in multiple places<ul> <li>There's a better chance of videos being closer to the users</li> <li>With fewer hops, videos will stream from a friendlier network</li> </ul> </li> <li>CDN machines make heavy use of caching and can mostly serve out of memory</li> <li>Less popular videos (1-20 views per day) that are not cached by CDNs<ul> <li>Can be served by the servers in various data centers</li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#load-balancing","title":"Load Balancing","text":"<ul> <li>We should use consistent hashing among the cache servers<ul> <li>This will also help in balancing the load among the cache servers</li> </ul> </li> <li>We can use a static hash-based scheme to map videos to hostnames<ul> <li>But it can lead to uneven load on logical replicas<ul> <li>Due to different popularity of each video</li> </ul> </li> <li>These uneven loads for logical replicas<ul> <li>Can then translate into uneven load distribution on corresponding physical servers</li> </ul> </li> </ul> </li> <li>To resolve this issue<ul> <li>Any busy server in one location<ul> <li>Can redirect to a less busy server in the same cache location</li> </ul> </li> <li>We can use dynamic HTTP redirections for this scenario</li> </ul> </li> <li>Drawbacks of redirections<ul> <li>Since the service tries to load balance locally<ul> <li>It leads to multiple redirections if the receiver host can't serve the video</li> </ul> </li> <li>Each redirection requires a client to make an additional HTTP request<ul> <li>It leads to higher delays before the video starts playing back</li> </ul> </li> <li>Inter-tier (or cross-data-center) redirections<ul> <li>Lead a client to distant cache location</li> <li>Because the higher tier caches are only present at small number of locations</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/storage_and_streaming/youtube_netflix#fault-tolerance","title":"Fault tolerance","text":"<ul> <li>Use consistent hashing for distributing load among database servers</li> <li>It will also help in replacing a dead server</li> </ul>"},{"location":"website_designs/tools/url_shortening","title":"URL Shortening Service","text":"<ul> <li>Platforms: tinyurl.com, bit.ly, goo.gl, qlink.me</li> <li>Provides short aliases that redirects to the original URL</li> <li>Short links save a lot of space when displayed, printed, messaged, tweeted</li> <li>Used for tracking and analytics<ul> <li>Audience, campaign performance, redirection frequency</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Generate a short &amp; unique alias for a given URL</li> <li>When users access a short link, redirect them to the original link</li> <li>Links will expire after a standard default timespan</li> <li>Users can optionally specify custom short link and custom expiration time</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Should be higly available (or else URL redirections will fail)</li> <li>URL redirection should be real-time and with minimal latency</li> <li>Shortened links should not be predictable</li> </ul> </li> <li>Extended Requirements<ul> <li>Tracking &amp; analytics</li> <li>Should be accessible through REST API</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#estimation","title":"Estimation","text":"<ul> <li>System would be read-heavy<ul> <li>There will be lots of redirection requests than new URL shortenings</li> <li>Assume the read-write ratio to be 100 : 1</li> </ul> </li> <li>Traffic<ul> <li>Write (New short links): 500 M/month = 200/s</li> <li>Read (Redirections): 500 M * 100 = 50 B/month = 20 K/s</li> </ul> </li> <li>Storage<ul> <li>Storage time for each short link: 5 years</li> <li>Total short links: (500 M/month _ 12) _ 5 years = 30 B</li> <li>Storage size for each short link: 500 bytes</li> <li>Total storage size: 30 B * 500 bytes = 15 TB</li> </ul> </li> <li>Bandwidth<ul> <li>Write: 200/s * 500 bytes = 100 KB/s</li> <li>Read: 20 K/s * 500 bytes = 10 MB/s</li> </ul> </li> <li>Memory<ul> <li>Cache frequently accessed links: 20%<ul> <li>Assuming 20% URLs generate 80% traffic (80-20 Principle)</li> </ul> </li> <li>Read requests: 20 K/s * 86400 s/day = 1.7 B/day</li> <li>Memory/day: 1.7 B/day * 500 bytes = 170 GB</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#api","title":"API","text":"<ul> <li>createShortLink (returns the short link)<ul> <li>Required: api_key, original_url</li> <li>Optional: user_id, custom_alias, expiration_date</li> </ul> </li> <li>getShortLinkInfo<ul> <li>Required: api_key, short_link</li> </ul> </li> <li>editShortLink<ul> <li>Required: api_key, short_link</li> <li>Payload: original_url, user_id, custom_alias, expiration_date</li> </ul> </li> <li>deleteShortLink<ul> <li>Required: api_key, short_link</li> </ul> </li> <li>Throttling<ul> <li>A malicious user can put us out of the business by consuming all URL keys</li> <li>Limit number of creations and redirections via api_key</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#database","title":"Database","text":"<ul> <li>Constraints<ul> <li>We need to store billions of records (15 TB)</li> <li>Each object is small (500 bytes)</li> <li>Read-heavy (20K requests/s)</li> <li>No relationship between records</li> </ul> </li> <li>Schema<ul> <li>ShortLink (id, link, original_url, created_at, expiration_at)</li> <li>User (id, email, name, api_key, created_at, last_login_at)</li> </ul> </li> <li>Type<ul> <li>We anticipate storing billions of rows and don't need relationships</li> <li>NoSQL key-value store (like Dynamo or Cassandra) is a good choice</li> <li>It would be easier to scale as well</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#component-design","title":"Component Design","text":""},{"location":"website_designs/tools/url_shortening#generating-key-for-url","title":"Generating Key for URL","text":"<ul> <li>We need to generate a short and unique for a given URL</li> <li>We can explore two solutions here<ul> <li>Encoding actual URL</li> <li>Generating keys offline</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#encoding-url","title":"Encoding URL","text":"<ul> <li>Compute unique hash using MD5 or SHA256 for the given URL<ul> <li>Let's use MD5 (produces 128-bit hash value)</li> </ul> </li> <li>Encode the hash for displaying<ul> <li>Could be base36 ([a-z][0-9]), base62 ([A-Z][a-z][0-9]), or base64 ([A-Z][a-z][0-9][-.])</li> <li>Let's use base64</li> <li>Encoding 128-bit hash value will give a string with 21 characters<ul> <li>Each base64 char encodes 6 bits of hash value</li> </ul> </li> </ul> </li> <li>What should be the length of the key?<ul> <li>Using 6 letters with base64: 64^6 = 69 B keys</li> <li>Using 8 letters with base64: 64^8 = 281 T keys</li> <li>6 letters should suffice (Since we estimated 30 B links for 5 years)</li> <li>We can take first 6 letters from the encoded string with 21 chars<ul> <li>This can result in duplication since we're taking partial encoded string</li> </ul> </li> </ul> </li> <li>Issue 1: If multiple users enter the same URL, they will get the same key<ul> <li>Append an increasing sequence number to each URL to make it unique<ul> <li>And then generate the hash</li> <li>Will the sequence number overflow since it will keep increasing?</li> <li>Will appending the number impact the performance?</li> </ul> </li> <li>Append user_id to the URL<ul> <li>If the user is not logged in, we would need to ask the user for a unique key</li> <li>If we have a conflict after this, we have to keep generating until a unique key</li> </ul> </li> </ul> </li> <li>Issue 2: What if parts of the URL are URL-encoded<ul> <li><code>http://www.example.com?id=design</code></li> <li><code>http://www.example.com%3Fid%3Ddesign</code></li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#generating-keys-offline","title":"Generating Keys Offline","text":"<ul> <li>We can have a standalone Key Generation Service (KGS)<ul> <li>That generates random 6 letter strings beforehand and stores them in database</li> </ul> </li> <li>While shortening a URL, we can pick a key from this key-db<ul> <li>This will be simple &amp; fast</li> <li>We won't have to encode anything</li> <li>We won't have to worry about duplications or collisions</li> </ul> </li> <li>How to handle concurrency (multiple servers picking keys)<ul> <li>As soon as a key is picked, it should be marked as used</li> <li>KGS can maintain 2 tables: used keys &amp; unused keys</li> <li>As soon as a key is given to a server, it can be moved to the used keys table</li> <li>What if multiple servers pick up the same key?<ul> <li>Get a lock (or synchronize) before removing &amp; passing it to server</li> </ul> </li> </ul> </li> <li>Keep some keys in memory so that they can be quickly provided<ul> <li>Though they should be first moved to used keys before loading in the memory</li> <li>If KGS dies before assigning these loaded keys, they will be wasted<ul> <li>Which is acceptable given the huge number of keys we have</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#common-considerations","title":"Common Considerations","text":"<ul> <li>Storage<ul> <li>With base64 encoding, 69 B unique 6 letter keys can be generated</li> <li>Since one byte is required to store one char, so one key will take 6 bytes</li> <li>Storage: 69 B * 6 bytes = 414 GB</li> </ul> </li> <li>KGS is single point of failure<ul> <li>Can be solved by keeping standby replica of KGS</li> <li>It will take over if the primary server dies</li> </ul> </li> <li>Caching<ul> <li>Each app server can keep some keys from key-db to speed things up</li> <li>If a server dies, we'll lose these cached keys<ul> <li>Which is acceptable given that we have 69 B keys</li> </ul> </li> </ul> </li> <li>Keep size limit on custom aliases<ul> <li>Impose a size limit on a custom alias to ensure we've consistent URL database</li> <li>Let's assume users can specify a max of 16 chars per customer key</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#permissions","title":"Permissions","text":"<ul> <li>Can users create private URLs that allow only specific users to access it?</li> <li>Store permission level (public/private) with each URL</li> <li>Create a table to store user_ids that have permission to view a specific URL</li> <li>Return HTTP 401 if the user is not permitted</li> <li>Schema: Permission (key, [user_ids])</li> </ul>"},{"location":"website_designs/tools/url_shortening#scalability","title":"Scalability","text":""},{"location":"website_designs/tools/url_shortening#data-partitoning","title":"Data Partitoning","text":""},{"location":"website_designs/tools/url_shortening#range-based","title":"Range Based","text":"<ul> <li>Based on first letter of the URL or the hash key</li> <li>We can combine less frequently used consequent letters</li> <li>But it can lead to unbalanced Servers<ul> <li>E.g. if we decide to keep one partition for E</li> <li>And then later realize that we have too many URLs starting with E</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#hash-based","title":"Hash Based","text":"<ul> <li>Based on the hash calculated on the URL or the hash key</li> <li>Hashing function will randomly distribute URLs to different partitions<ul> <li>It will map to a partition number between (1..256)</li> </ul> </li> <li>This can still lead to over overloaded partitions<ul> <li>Which can be solved by using consistent hashing</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#caching","title":"Caching","text":"<ul> <li>We can cache frequently accessed URLs</li> <li>We can start with 20% of daily traffic<ul> <li>And then adjust the number of cache servers based on the user usage pattern</li> <li>This requires 170 GB memory as calculated in the estimation</li> <li>One machine will be sufficient (since a modern server can have 256 GB memory)</li> <li>Alternatively, we can use a couple of small servers</li> </ul> </li> <li>Eviction Policy<ul> <li>LRU (Least Recently Used)</li> <li>We can use Linked Hash Map to store URLs, hashes &amp; recent access timestamp</li> </ul> </li> <li>We can replicate our caching servers to distribute load between them<ul> <li>During a cache miss, servers would hit the backend database</li> <li>When this happens, update the cache and pass the new entry to all the cache replicas</li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#load-balancers","title":"Load Balancers","text":"<ul> <li>We can add load balancing layer at three places<ul> <li>Between Client &amp; Application servers</li> <li>Between Application servers &amp; Database servers</li> <li>Between Application servers &amp; Cache servers</li> </ul> </li> <li>Initially we can use a simple Round Robin approach<ul> <li>It will distributes incoming requests equally amond backend servers</li> <li>It is simple and introduces no overhead</li> <li>If server is dead, it will be taken out of rotation</li> </ul> </li> <li>A problem with round robin is that server load is not taken into consideration<ul> <li>If a server is overloaded or slow, LB won't stop sending requests to it</li> <li>Intelligent RR can be used to solve this<ul> <li>It will periodically query servers about its load and adjust traffic</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/url_shortening#purging-db-cleanup","title":"Purging / DB Cleanup","text":"<ul> <li>We should purge entries when they are expired</li> <li>Have a default expiration time for each link (e.g. 2 years)</li> <li>Actively searching for expired links to remove them will put a lot of pressure on db<ul> <li>Instead, we can slowly remove expired links and do a lazy cleanup</li> <li>Whenever a user tries to access expired link, delete the link &amp; show an error to user</li> </ul> </li> <li>Run a cleanup service periodically to remove expired links from storage and cache<ul> <li>It should be lightweight and run only when traffic is expected to be low</li> <li>Should we remove keys not been used for some time, say 6 months?<ul> <li>Storage is getting cheap, we can decide to keep links forever</li> </ul> </li> </ul> </li> <li>After removing an expired link, put the key back to the unused-keys table</li> </ul>"},{"location":"website_designs/tools/url_shortening#telemetry","title":"Telemetry","text":"<ul> <li>Metrics<ul> <li>How many times a short link has been used</li> <li>Location of the users that accessed it</li> <li>Date &amp; time of the access, at what time of day it has highest traffic?</li> <li>From which web page it was referred</li> <li>Browser or platform where it was accessed</li> </ul> </li> <li>How to store this info?<ul> <li>Keep a metadata table with a unique row per short link</li> <li>It will get updated on each view?</li> <li>What happens when a popular url is slammed with a large number of concurrent requests?</li> </ul> </li> </ul>"},{"location":"website_designs/tools/pastebin","title":"Pastebin","text":"<ul> <li>Platforms: pastebin.com, pasted.co, chopapp.com</li> <li>Users can store plain text or images</li> <li>Users get a randomly generated URL to access it</li> <li>Used to share data quickly (source code, configs, logs)</li> </ul>"},{"location":"website_designs/tools/pastebin#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Users can upload or paste their data and get a unique URL to access it</li> <li>Users can only upload text</li> <li>Data and links will expire automatically after a default timespan</li> <li>Users can optionally pick a custom alias and specify custom expiration time</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>System should be highly reliable, any uploaded data shouldn't be lost</li> <li>System should be highly available, users should be able to access their data at any time</li> <li>Users should be able to access their data in real time with minimum latency</li> <li>Paste links shouldn't be predictable</li> </ul> </li> <li>Extended Requirements<ul> <li>Analytics like how many times a paste was accessed</li> <li>Service should be accessible through rest api</li> </ul> </li> </ul>"},{"location":"website_designs/tools/pastebin#considerations","title":"Considerations","text":"<ul> <li>What should be the limit of the text that can be pasted<ul> <li>Since it's not a full fledged storage cloud, it should be small</li> <li>We can keep the limit at 10 MB to stop the abuse of the service</li> </ul> </li> <li>Should we impose size limit on custom URLs<ul> <li>Impose a size limit on a custom alias to ensure we've consistent URL database</li> <li>Let's assume users can specify a max of 16 chars per customer key</li> </ul> </li> </ul>"},{"location":"website_designs/tools/pastebin#estimation","title":"Estimation","text":"<ul> <li>System would be read heavy, assume the read-write ratio to be 5 : 1</li> <li>Traffic<ul> <li>Write (New pastes): 1M/day = 12/s</li> <li>Read (Paste accesses): 5M/day = 58/s</li> </ul> </li> <li>Storage<ul> <li>Average size of each paste: 10KB<ul> <li>Users store text like source code, configs, logs</li> <li>Such texts are not huge</li> </ul> </li> <li>Daily data: 1M paste/day * 10KB/paste = 10GB/day</li> <li>Storage time for each paste: 10 years</li> <li>Storage required: (10GB _ 365 days) _ 10 years = 36TB</li> <li>Total storage with margin of 30%: 36TB * 1.3 = 51.4TB</li> </ul> </li> <li>Storage for Keys<ul> <li>6 letter keys with base64 encoding ([A-Z, a-z, 0-9, ., -])<ul> <li>64^6 ~= 68.7 billion unique strings</li> </ul> </li> <li>Keys required for 10 years: 1M paste/day _ 365 days _ 10 years = 3.6B keys</li> <li>If it takes one byte to store one char: 3.6B * 6 = 22GB</li> <li>22GB is negligible compared to 36TB required for data</li> </ul> </li> <li>Bandwidth<ul> <li>Write: 12 paste/s * 10KB/paste = 120 KB/s</li> <li>Read: 58 paste/s * 10KB/paste = 0.6 MB/s</li> </ul> </li> <li>Memory<ul> <li>We can cache 20% of the pastes that are frequently accessed (80-20 principle)</li> <li>Cache storage: 5M paste/day _ 10KB/paste _ 20% = 10 GB/day</li> </ul> </li> </ul>"},{"location":"website_designs/tools/pastebin#api","title":"API","text":"<ul> <li>addPaste (returns the paste url)<ul> <li>Required: api_key, data</li> <li>Optional: custom_url, user_id, paste_name, expiration_date</li> </ul> </li> <li>getPasteInfo<ul> <li>Required: api_key, paste_url</li> </ul> </li> <li>editPaste<ul> <li>Required: api_key, paste_url</li> <li>Payload: data, custom_url, paste_name, expiration_date</li> </ul> </li> <li>deletePaste<ul> <li>Required: apiKey, paste_url</li> </ul> </li> </ul>"},{"location":"website_designs/tools/pastebin#database","title":"Database","text":"<ul> <li>Constraints<ul> <li>We need to store billions of records</li> <li>Read heavy (58 requests/s)</li> <li>Each metadata object would be small (&lt; 100 bytes)</li> <li>Each paste object can have max size of 10 MB</li> <li>There are no relationship between records</li> </ul> </li> <li>Schema<ul> <li>Paste (id, key, paste_url, content_url, created_at, expiration_at)</li> <li>User (id, email, name, api_key, created_at, last_login_at)</li> </ul> </li> <li>Type<ul> <li>We anticipate storing billions of rows and don't need relationships</li> <li>NoSQL key-value store (like Dynamo or Cassandra) is a good choice</li> <li>Should we store the content in this db itself or in external file storage like S3?</li> <li>It would be easier to scale as well</li> </ul> </li> </ul>"},{"location":"website_designs/tools/pastebin#component-design","title":"Component Design","text":""},{"location":"website_designs/tools/pastebin#generating-key-for-pastes","title":"Generating Key for Pastes","text":"<ul> <li>Refer to URL shortening for detailed description, the main points are listed below</li> <li>Generate a 6 letter random string which would server as the key of the paste<ul> <li>If user has provided a custom key, then consider that</li> <li>If the key is a duplicate, keep generating keys until a unique key is found</li> </ul> </li> <li>Another solution is to run a standalone Key Generation Service (KGS)<ul> <li>It will generate these keys beforehand and store them in a separate database</li> <li>It can have two tables: used keys, unused keys</li> <li>It can keep some keys in memory to provide keys quickly</li> </ul> </li> </ul>"},{"location":"website_designs/tools/pastebin#scalability","title":"Scalability","text":"<ul> <li>Similar to URL shortening</li> </ul>"},{"location":"website_designs/tools/api_rate_limiter","title":"API Rate Limiter","text":"<ul> <li>Throttles users based upon the number of the received requests<ul> <li>To allow only a certain number of requests per second or min<ul> <li>So that the servers are not overloaded</li> </ul> </li> <li>It limits the number of events an entity (user, device, IP, etc.) can perform<ul> <li>In a particular time window</li> </ul> </li> </ul> </li> <li>Examples<ul> <li>A user can send only one message per second</li> <li>A user is allowed only three failed credit card transactions per day</li> <li>A single IP can only create twenty accounts per day</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Limit the number of requests an entity can send to an API within a time window<ul> <li>E.g. 15 requests per second</li> </ul> </li> <li>The APIs are accessible through a cluster<ul> <li>So the rate limit should be considered across different servers</li> </ul> </li> <li>The user should get an error message when the threshold is crossed</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>System should be highly available<ul> <li>It should always work to protect the service from external attacks</li> </ul> </li> <li>Should not introduce substantial latencies affecting the user experience</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#component-design","title":"Component Design","text":""},{"location":"website_designs/tools/api_rate_limiter#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Rate limiting defines the rate and speed at which consumers can access APIs<ul> <li>Throttling controls the usage of the APIs by customers during a given period</li> </ul> </li> <li>Helps to protect services against attacks on application layer<ul> <li>Like denial of service (DOS), brute force password or credit card transactions</li> <li>These attacks are usually a barrage of https requests<ul> <li>Which may look like they are coming from real users</li> <li>But are typically generated by machines or bots</li> </ul> </li> </ul> </li> <li>Also used to prevent revenue loss and to reduce infrastructure costs<ul> <li>To stop spam and to stop online harassment</li> <li>To make sure that the service stays up for everyone</li> <li>To make sure low priority requests don't affect the high priority traffic<ul> <li>Like high volume of requests for analytics data</li> <li>Should not be hamper critical transactions for other users</li> </ul> </li> <li>There could be default limits for the API service<ul> <li>To go beyond that, the user has to buy higher limits</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#types-of-rate-limiting","title":"Types of Rate Limiting","text":"<ul> <li>Hard Throttling: Number of requests cannot exceed the throttle limit</li> <li>Soft Throttling: Number of requests can exceed a certain percentage<ul> <li>E.g. rate-limit of 100 messages/minute with 10% exceed limit</li> <li>Allows up to 110 messages/minute</li> </ul> </li> <li>Elastic or Dynamic Throttling<ul> <li>Number of requests can go beyond the threshold if free resources are available</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#fixed-window-algorithm","title":"Fixed Window Algorithm","text":"<ul> <li>Time window is considered from the start of the time-unit to the end of the time-unit<ul> <li>A period is considered as 0-60 seconds for a minute</li> <li>Irrespective of the time frame at which the request is made</li> </ul> </li> <li>For example, consider the rate-limit of 2 messages/second<ul> <li>There are two messages between 0-1 second and three messages between 1-2 second</li> <li>The messages will be considered individually for each of these seconds</li> <li>Hence, only the last message in 1-2 second will be throttled</li> </ul> </li> <li>Disadvantage: It can potentially allow twice the number of requests<ul> <li>If we send 2 messages at the last of the second</li> <li>We can immediately send 2 messages at the start of the next second</li> <li>Resulting in 4 messages within a span of 1 second</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#atomicity","title":"Atomicity","text":"<ul> <li>In a distributed environment<ul> <li>Read-and-then-write behavior can create a race condition</li> </ul> </li> <li>If the current count is 2 and we issue two more requests<ul> <li>If two separate processes serve these requests</li> <li>And concurrently read the count before either of them update it</li> <li>Then, each process will allow one more request</li> </ul> </li> <li>If we use Redis and Memcached to store our key-value<ul> <li>We can use their locking feature for the duration of the operation</li> <li>But this will also slow down concurrent requests from the same user</li> </ul> </li> <li>If we use simple hash-table, we can custom implement locking feature</li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#memory-estimation","title":"Memory Estimation","text":"<ul> <li>Let's assume the simple solution where we're storing all the data in a hash table<ul> <li>{ user_id: { count: integer, count_started_at: epoch_time } }</li> </ul> </li> <li>Row size: 12 bytes<ul> <li>user_id: 8 bytes</li> <li>count: 2 bytes (can count upto 65k)</li> <li>count_started_at: 2 bytes<ul> <li>Although epoch time will need 4 bytes</li> <li>But we can store only the minute and second parts</li> </ul> </li> </ul> </li> <li>Total row size: 12 + 20 + 4 = 32 bytes<ul> <li>Assume an overhead of 20 bytes for each record</li> <li>Assume that we need a 4 byte number to lock each user's record<ul> <li>To resolve the atomicity problems</li> </ul> </li> </ul> </li> <li>Users: 1 M</li> <li>Total Storage = 36 bytes * 1 M = 36 MB<ul> <li>This can easily fit on a single server</li> <li>But we won't like to route all of the traffic through a single machine</li> <li>Also, if we assume a rate-limit of 10 requests/second for each user<ul> <li>This would translate into 10 M queries/second for the rate limiter</li> <li>This would be too much for a single server</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#rollingsliding-window-algorithm","title":"Rolling/Sliding Window Algorithm","text":"<ul> <li>Time window is considered<ul> <li>From the fraction of the time at which the request is made</li> <li>Plus the time window length</li> </ul> </li> <li>For example, consider the rate-limit of 2 messages/second<ul> <li>There are two messages sent at 300th &amp; 400th millisecond between 0-1 second</li> <li>There are two messages sent at 100th &amp; 200th millisecond between 1-2 second</li> <li>The messages will be counted from the 300th millisecond of the first second<ul> <li>Up to the 300th millisecond of next second</li> </ul> </li> <li>Hence, the last two messages will be throttled</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#memory-estimation_1","title":"Memory Estimation","text":"<ul> <li>Store timestamps and counts in redis<ul> <li>{ user_id: { count: integer, timestamps: sorted set of epoch-times } }</li> </ul> </li> <li>Assume rate limit of 600 requests/hour = 10 requests/min</li> <li>Timestamps: 24 bytes _ 10 requests/min _ 60 mins = 14.4 KB<ul> <li>Each epoch time will require 4 bytes</li> <li>We will need 16 bytes for pointers<ul> <li>In a sorted set, we can assume that we need at least 2 pointers</li> <li>To maintain order among elements</li> <li>One pointer to the previous element and one to the next element</li> <li>On a 64 bit machine, each pointer will cost 8 bytes</li> </ul> </li> <li>We can add an extra word (4 bytes) for storing other overhead</li> </ul> </li> <li>Row size: 14428 bytes = 15 KB<ul> <li>user_id: 8 bytes</li> <li>timestamps: 15 KB</li> <li>Assume an overhead of 20 bytes for each record</li> </ul> </li> <li>Users: 1 M</li> <li>Total Storage: 15 KB * 1 M = 15 GB</li> <li>It takes a lot of memory compared to the fixed window algorithm<ul> <li>This can lead to scalability issue</li> <li>What if we combine the two algorithms to optimize the memory usage?</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#sliding-window-with-counters","title":"Sliding Window with Counters","text":"<ul> <li>Keep track of request counts for each user using multiple fixed time windows<ul> <li>E.g. the 1/60th size of rate-limit's time window</li> <li>So for an hourly rate limit, we can keep count for each minute</li> <li>And calculate the sum of all these minor windows to calculate the throttling limit</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#memory-estimation_2","title":"Memory Estimation","text":"<ul> <li>Store counters in redis which offers efficient storage for fewer than 100 keys<ul> <li>{ user_id: { count: integer, counter_hash: epoch-time with counts } }</li> </ul> </li> <li>Assume rate limit of 600 requests/hour = 10 requests/min<ul> <li>There will be 60 minor windows since this is hourly limit</li> </ul> </li> <li>Counter hash: 26 bytes * 60 minor windows = 1.560 KB<ul> <li>Each epoch time will need 4 bytes</li> <li>Counter would need 2 bytes</li> <li>Assume an overhead of 20 bytes</li> </ul> </li> <li>Row size: 1588 bytes = 1.6 KB<ul> <li>user_id: 8 bytes</li> <li>counter_hash: 1.560 KB</li> <li>Assume an overhead of 20 bytes</li> </ul> </li> <li>Users: 1 M</li> <li>Storage: 1.6 KB * 1 M = 1.6 GB</li> <li>This uses 86% less memory than the simple sliding window algorithm</li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#rate-limit-criterias","title":"Rate Limit Criterias","text":""},{"location":"website_designs/tools/api_rate_limiter#ip","title":"IP","text":"<ul> <li>It's not optimal in differentiating good &amp; bad actors<ul> <li>But still better than not having rate limiting at all</li> </ul> </li> <li>When multiple users share a single public IP<ul> <li>One bad user can cause throttling for all others</li> </ul> </li> <li>There are a huge number of IPv6 addresses available to a hacker from even one computer</li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#user-id","title":"User Id","text":"<ul> <li>On user authentication, the user will be provided with a token<ul> <li>Which the user will have to paass with each request</li> </ul> </li> <li>This will ensure that we rate limit against a particular API<ul> <li>That has a valid authentication token</li> </ul> </li> <li>But this cannot rate limit on the login API itself<ul> <li>A hacker can perform denial of service (DOS) attack by entering wrong credentials</li> <li>After that, the actual user won't be able to login</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#hybrid","title":"Hybrid","text":"<ul> <li>Use both per-IP and per-user rate limiting</li> <li>But this will result in more cache entries with more details per entry<ul> <li>Hence this will require more memory and storage</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#scalability","title":"Scalability","text":""},{"location":"website_designs/tools/api_rate_limiter#data-sharding","title":"Data Sharding","text":"<ul> <li>We can shard based on User Id</li> <li>If we want different limits for different APIs, we can shard per user per API<ul> <li>For example, in url shortener we have create_url &amp; delete_url APIs</li> <li>We can consider separate rate limiter for each API shard as well</li> </ul> </li> </ul>"},{"location":"website_designs/tools/api_rate_limiter#caching","title":"Caching","text":"<ul> <li>We can cache recently active users</li> <li>Write-back cache<ul> <li>Update all counters and timestamps in cache only</li> <li>The write to the permanent storage can be done at fixed intervals</li> <li>This will ensure minimum latency added to the user requests by the rate limiter</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion","title":"Typeahead Suggestion","text":"<ul> <li>Real-time suggestion service that recommends terms to users as they enter text<ul> <li>Suggests known and frequently searched terms</li> </ul> </li> <li>It tries to predict the query based on the characters the user has entered<ul> <li>And gives a list of suggestions to complete the query</li> <li>It helps the user to articulate their search queries better</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#requirements","title":"Requirements","text":"<ul> <li>Functional Requirements<ul> <li>Suggest top 10 terms starting with whatever user has typed</li> </ul> </li> <li>Non-Functional Requirements<ul> <li>Suggestions should appear in real-time within 200ms</li> <li>Efficiently store huge amount of strings that can be searched on any prefix</li> <li>Serve large number of queries</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#estimation","title":"Estimation","text":"<ul> <li>Traffic<ul> <li>If we are building a service that has the same scale as google</li> <li>Searches: 5 B/day = 60 K/s</li> <li>Unique searches: 20%<ul> <li>Since there will be a lot of duplicates in 5 billion queries</li> </ul> </li> <li>Daily unique queries to index: 5 B/day _ 20% unique _ 10% = 100 M<ul> <li>If we only want to index the top 10% of the search terms</li> <li>We can get rid of a lot of less frequently search queries</li> </ul> </li> </ul> </li> <li>Storage<ul> <li>Average query: 15 characters<ul> <li>If on average each query consists of 3 words</li> <li>And there are 5 characters in each word on average</li> </ul> </li> <li>Average query size: 15 chars * 2 bytes = 30 bytes<ul> <li>Assuming we need 2 bytes to store a character</li> </ul> </li> <li>Storage for current day: 100 M indexed queries/day * 30 bytes/query = 3 GB</li> <li>Storage for year long queries: 3 GB/day _ 365 _ 20% = 22 GB<ul> <li>We can expect some growth in this data every day</li> <li>But we should also remove some terms that are not searched anymore</li> <li>Assume we have 2% new queries every day</li> </ul> </li> <li>Year Storage: 3 GB for current day + 22 GB for year long queries = 25 GB</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#database","title":"Database","text":"<ul> <li>Database cannot serve large number of queries<ul> <li>To search on prefix in real-time with minimum latency</li> </ul> </li> <li>Need to store index in memory using highly efficient data structure like trie</li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#component-design","title":"Component Design","text":"<ul> <li>We need to serve a lot of queries with minimum latency<ul> <li>We need to store a lot of strings in such a way that users can search with any prefix</li> <li>We cannot depend on some database for this</li> <li>We need to store our index in memory in a highly efficient data structure</li> <li>One of the most appropriate data structures for this purpose is trie</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#typeahead-trie","title":"Typeahead Trie","text":"<ul> <li>Trie is a tree like data structure used to store words &amp; phrases<ul> <li>Each node stores a character of the phrase in a sequential manner</li> </ul> </li> <li>We can merge nodes that have only one branch to save storage space<ul> <li>E.g. if capital is the only word in the T branch<ul> <li>Then we can merge TAL like C -&gt; A -&gt; P -&gt; I -&gt; TAL</li> </ul> </li> <li>Or if we have only words having AP after C<ul> <li>C -&gt; AP -&gt; I -&gt; TAL</li> <li>C -&gt; AP -&gt; T -&gt; A -&gt; IN</li> </ul> </li> </ul> </li> <li>Should we have case insensitive trie?<ul> <li>Let's assume our data is case insensitive</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#ranking-searches","title":"Ranking Searches","text":"<ul> <li>Store the count of searches that terminated at each node<ul> <li>E.g. if users have searched 'captain' 100 times</li> <li>Store this number with the last character of the phrase</li> </ul> </li> <li>When user types 'cap'<ul> <li>We can get the searches with phrase 'cap' by traversing that sub-tree</li> <li>And also get the count of times they were selected to be searched</li> </ul> </li> <li>Another criterias to consider<ul> <li>Recency, user location, language, demographics, personal history</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#traversing-trie","title":"Traversing Trie","text":"<ul> <li>Given a prefix, it can take a long time to search these results<ul> <li>Given the amount of data we need to index, we should expect a huge tree</li> <li>Even traversing a sub-tree would take a long time depending on its depth</li> <li>Large amount of data to be indexed, huge tree</li> </ul> </li> <li>To solve this, we can store top 10 suggestions with each node<ul> <li>But it will require a lot of extra storage</li> <li>We can optimize the storage<ul> <li>By storing only references of the terminal nodes of these top suggestions</li> <li>Rather than storing the entire phrase</li> </ul> </li> </ul> </li> <li>To find suggested terms<ul> <li>We need to traverse back using the parent reference from the terminal node</li> <li>We will also need to store the frequency with each reference<ul> <li>To keep track of top suggestions</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#building-trie","title":"Building Trie","text":"<ul> <li>We can efficiently build the trie bottom up</li> <li>Each parent node will recursively call all the child nodes<ul> <li>To calculate their top suggestions and their counts</li> </ul> </li> <li>Parent nodes will combine top suggestions from all of their children</li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#updating-trie","title":"Updating Trie","text":"<ul> <li>Assuming five billion searches every day, i.e. 60 K queries per seconds<ul> <li>Updating trie for every query will be extremely resource intensive</li> <li>And this can hamper the read requests</li> <li>One solution for this could be to update the trie offline at regular intervals</li> </ul> </li> <li>As the new queries come, log them and track their frequencies<ul> <li>Either we can log every query or do sampling and log every 1000th query</li> <li>This will only track queries that are searched for more than 1000 times</li> </ul> </li> <li>We can have a Map-Reduce setup<ul> <li>To process the logging data periodically, say every hour</li> <li>These MR jobs will calculate frequencies of all the searched terms in the past hour</li> <li>We can then update the trie with this new data</li> </ul> </li> <li>Before updating, we can take the current snapshot of the trie<ul> <li>We can make a copy of the trie on each server to update it offline<ul> <li>Once done, we can switch to the updated one</li> </ul> </li> <li>Another option is to have a master-slave configuration for each trie server<ul> <li>We can update the slave while the master is serving traffic</li> <li>Once updated, we can make the slave the new master</li> <li>And later update the old master to start serving the traffic too</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#updating-frequencies","title":"Updating Frequencies","text":"<ul> <li>We can update only the differences in frequencies<ul> <li>Rather than recounting all searched items from scratch</li> </ul> </li> <li>We can add &amp; subtract the frequencies<ul> <li>Based on Exponential Moving Average (EMA) of each term</li> <li>It gives more weight to the latest data</li> </ul> </li> <li>After inserting a new term in the trie<ul> <li>We will go to the terminal node of the phrase and increase its frequency</li> </ul> </li> <li>Since we're storing the top 10 queries in each node<ul> <li>It is possible that this particular search term<ul> <li>Jumped into the top 10 queries of a few other nodes</li> </ul> </li> <li>So we need to update top 10 queries for those nodes then<ul> <li>We have to traverse back from the node to all the way up to the root</li> </ul> </li> </ul> </li> <li>For every parent<ul> <li>We check if the current query is part of the top 10<ul> <li>If so we update the correspoonding frequency</li> </ul> </li> <li>If not, we check if the current query's frequency is high enough<ul> <li>To be part of the top 10</li> <li>If so, we insert this new term and remove the term with the lowest frequency</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#removing-term","title":"Removing Term","text":"<ul> <li>We may need to remove a term due to legal issue, hate, piracy, etc.</li> <li>We can completely remove such terms when the regular update happens</li> <li>Meanwhile, we can add a filtering layer on each server<ul> <li>Which will remove such terms before sending them to users</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#permanent-storage-of-trie","title":"Permanent Storage of Trie","text":"<ul> <li>How to store trie in a file so that we can rebuild the trie easily<ul> <li>This will be needed when a machine starts</li> </ul> </li> <li>We can take a snapshot of the trie periodically and store it in a file<ul> <li>This will enable us to rebuild a trie if the server goes down</li> </ul> </li> <li>Start with the root node and save the trie level by level<ul> <li>With each node, store the character it contains and the children count</li> <li>Right after each node, we should put all of its children</li> </ul> </li> <li>Example<ul> <li>C<ul> <li>A<ul> <li>R -&gt; T</li> <li>P</li> </ul> </li> <li>O -&gt; D</li> </ul> </li> <li>Stored as: C2, A2, R1, T, P, O1, D</li> </ul> </li> <li>It is hard to store top suggestions and their count<ul> <li>Because the trie is being stored top down<ul> <li>Child nodes are not created before the parent</li> <li>So there is no way to store their references</li> </ul> </li> <li>So we have to recalculate them while rebuilding the trie<ul> <li>Each node will calculate its top suggestions and pass it to its parent</li> <li>Each parent node will merge results from all of its children</li> </ul> </li> <li>But how will we get this lost data since it's based on user activity?</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#client-side-optimizations","title":"Client Side Optimizations","text":"<ul> <li>Clients can use debouncing<ul> <li>Hit the servers at regular intervals (500ms)</li> <li>Hit the server only if the user has not pressed any key for 50ms</li> <li>If the user is constantly typing, cancel in-progress requests</li> </ul> </li> <li>Clients can pre-fetch some data from the server to save future requests</li> <li>Clients can store the recent history of suggestions locally<ul> <li>We can also take location &amp; language into account</li> <li>We can also store these on the server for each user for better personalization</li> </ul> </li> <li>Establish an early connection with the server<ul> <li>As soon as the user opens the website, the client can open a connection</li> <li>So when a user types the first character<ul> <li>The client doesn't waste time in establishing the connection</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#scalability","title":"Scalability","text":""},{"location":"website_designs/tools/typeahead_suggestion#data-partitioning","title":"Data Partitioning","text":"<ul> <li>Although the index can easily fit on one server<ul> <li>But we can still partition it to meet our requirements</li> <li>For higher efficiency and lower latency</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#based-on-first-letter","title":"Based on First Letter","text":"<ul> <li>Store phrases in separate partitions based on their first letter</li> <li>We can also combine certain less frequently occuring letters</li> <li>But it can lead to unbalanced partitions<ul> <li>If we decide to put all terms starting with letter 'E' in one partition</li> <li>And later we realize that we've too many terms starting with 'E'<ul> <li>That we cannot fit into one partition</li> </ul> </li> <li>This problem will occur with every statically defined scheme<ul> <li>It is not possible to calculate if each partition will fit on one server statically</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#based-on-hash-of-terms","title":"Based on Hash of Terms","text":"<ul> <li>Pass the term to a hash function to generate the server number</li> <li>This will make our term distribution random and minimize hotspots</li> <li>To find a term, we will have to query all the servers and aggregate all the servers</li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#based-on-server-capacity","title":"Based on Server Capacity","text":"<ul> <li>Partition the trie based on maximum memory capacity of the servers</li> <li>Keep storing data on a server as long as it has memory available</li> <li>Whenever a sub-tree cannot fit into a server<ul> <li>Break the parition there to assign that range to this server</li> <li>And move on to the next server to repeat the process</li> </ul> </li> <li>E.g. Server 1: A to AABC, Server 2: AABD to BXA, Server 3: BXB to CDA<ul> <li>If the user has typed 'A' or 'AA', we have to query both server 1 &amp; 2</li> <li>But if the user has typed 'AAA', we need to query only server 1</li> <li>We can have a load balancer in front of the trie server<ul> <li>Which can store this mapping and redirect traffic</li> </ul> </li> </ul> </li> <li>If we're querying from multiple servers<ul> <li>We need to merge the results at the server side to aggregate results<ul> <li>This will require another layer of servers between load balancers &amp; trie servers</li> <li>To aggregate the results and return the top results to the client</li> </ul> </li> <li>Or make the clients do that</li> </ul> </li> <li>If there are a lot of queries for terms starting with 'cap'<ul> <li>The server holding it will have a high load compared to others</li> </ul> </li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#caching","title":"Caching","text":"<ul> <li>Caching top searched terms would be extremely helpful</li> <li>Small percentage of queries will be responsible for the most traffic</li> <li>We can have separate cache servers in front of the trie servers<ul> <li>Holding most frequently searched terms and their typeahead suggestions</li> </ul> </li> <li>We can also build a simple machine learning model<ul> <li>To predict the engagement on each suggestion</li> <li>Based on simple counting, personalization, trending data, etc.</li> </ul> </li> <li>Use CDNs to cache geographically searched terms that are used frequently</li> </ul>"},{"location":"website_designs/tools/typeahead_suggestion#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>We can have a master-slave configuration<ul> <li>If the master dies, the slave can take over after failover</li> <li>Any server that comes back up, can rebuild the trie based on the last snapshot</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler","title":"Web Crawler","text":"<ul> <li>Also known as web spiders, robots, worms, walkers, bots</li> <li>Browses the world wide web (WWW) in a methodical and automated manner<ul> <li>Collects documents by recursively fetching links from a set of starting pages</li> </ul> </li> <li>Many sites, particularly search engines, use web crawling<ul> <li>As a means of providing up-to-date data</li> <li>Search engines download all the pages to create an index on them to perform faster searches</li> </ul> </li> <li>Other uses<ul> <li>Test web pages and links for valid syntax and structure</li> <li>Monitor sites for changes in their structure or contents</li> <li>Maintain mirror sites for popular sites</li> <li>Search for copyright infringements</li> <li>Build a special-purpose index<ul> <li>E.g. for understanding of the content stored in multimedia files on the web</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#requirements","title":"Requirements","text":"<ul> <li>Crawl only HTML over HTTP</li> <li>Scalability<ul> <li>Can crawl the entire web</li> <li>Can fetch hundreds of millions of web documents</li> </ul> </li> <li>Extensibility<ul> <li>Modular design with the expectation that new functionality will be added</li> <li>Newer document types can be added later to be downloaded and processed</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#estimation","title":"Estimation","text":"<ul> <li>Crawling target: 15 B/month = 6200 pages/sec</li> <li>Row Size: 100.5 KB<ul> <li>Page sizes vary a lot, but we will be dealing with html text only</li> <li>Assume page size of 100 KB</li> <li>With each page we're storing 500 bytes of metadata</li> </ul> </li> <li>Storage<ul> <li>Required Storage: 100.5 KB * 15 B = 1.5 Petabytes</li> <li>Total Storage: 1.5/0.7 = 2.14 Petabytes<ul> <li>Assuming 70% Capacity Model</li> <li>We don't want to go above 70% of the total capacity of the storage</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#considerations","title":"Considerations","text":"<ul> <li>Crawling the web is a complex task, and there are many ways to go about it</li> <li>Decide the objective because it can change the design<ul> <li>Is it a crawler for HTML pages only?<ul> <li>It should be extensible</li> <li>It should be easy to add support for new media types later</li> </ul> </li> <li>Is it a general-purpose crawler to download different media types?<ul> <li>We should break down the parsing module into different sets of modules<ul> <li>One for HTML, another for images, and so on for videos &amp; music</li> </ul> </li> <li>Each module extracts what is considered interesting for that media type</li> </ul> </li> </ul> </li> <li>Decide the protocols that the crawler should handle<ul> <li>HTTP, FTP, etc.</li> <li>Should be extensible</li> </ul> </li> <li>Expected number of pages it will crawl<ul> <li>How big will the url database become?</li> <li>Let's assume we need to crawl 1 B websites</li> <li>A website can contain many other urls</li> <li>Let's assume an upper bound 15 B different web pages that will be reached</li> </ul> </li> <li>Robots Exclusion Protocol<ul> <li>Allows webmasters to declare parts of their sites off limits to crawlers</li> <li>Requires a web crawler to fetch robot.txt containing these declarations<ul> <li>From a website before downloading any real content</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#crawling-workflow","title":"Crawling Workflow","text":"<ul> <li>The basic algorithm executed by any web crawler<ul> <li>Is to take a list of seed urls as its input</li> <li>And repeatedly execute the given process</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#process","title":"Process","text":"<ul> <li>Pick a URL from the unvisited URL list</li> <li>Determine the IP Address of its hostname</li> <li>Establish a connection to the host to download the corresponding document</li> <li>Parse the document contents to look for new URLs</li> <li>Add the new URLs to the list of unvisited URLs</li> <li>Process the downloaded document, e.g. store it or index its contents</li> <li>Repeat</li> </ul>"},{"location":"website_designs/tools/web_crawler#breadth-first-or-depth-first","title":"Breadth-first or Depth-first","text":"<ul> <li>BFS is usually used<ul> <li>Will cover more ground though the given set of links</li> <li>DFS may get stuck in nested links</li> </ul> </li> <li>DFS is also used in some situations<ul> <li>If the crawler has already established a connection with the website</li> <li>It might just DFS all the URLs within this website</li> <li>To save some handshaking overhead</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#path-ascending-crawling","title":"Path-ascending crawling","text":"<ul> <li>Helps discover a lot of isolated resources<ul> <li>Or resources for which inbound link won't be found in regular crawling</li> </ul> </li> <li>Crawler would ascend to every path in each URL that it intends to crawl<ul> <li>E.g. given a seed URL of http://foo.com/a/b/page.html</li> <li>It will attempt to crawl /a/b/, /a/, and /</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#challenges","title":"Challenges","text":"<ul> <li>Large volume of web pages<ul> <li>Web crawler can only download a fraction of the web pages at any time</li> <li>Crawler should be intelligent enough to prioritize download</li> </ul> </li> <li>Rate of change on web pages<ul> <li>Web pages on the internet change very frequently</li> <li>By the time the last page is downloaded from a website<ul> <li>The pages may change or a new page may be added</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#crawler-structure","title":"Crawler Structure","text":"<ul> <li>A bare minimum crawler needs at least these components</li> <li>URL Frontier: To store the list of URLs to download and prioritize them</li> <li>HTTP Fetcher: To retrieve a web page from the server</li> <li>Extractor: To extract links from HTML documents</li> <li>Duplicate Eliminator: To make sure the same content is not extracted twice</li> <li>Datastore: To store retrieved pages, URLs, and metadata</li> </ul>"},{"location":"website_designs/tools/web_crawler#working","title":"Working","text":"<ul> <li>Let's assume the crawler is running on one server<ul> <li>And the crawling is done by multiple working threads</li> </ul> </li> <li>Each thread performs all the steps required to download &amp; process documents in loop</li> </ul>"},{"location":"website_designs/tools/web_crawler#working-thread","title":"Working Thread","text":"<ul> <li>Remove an absolute URL from the shared URL frontier for downloading<ul> <li>An absolute URL begins with a scheme like HTTP</li> <li>Which identifies the network protocol that should be used to download it</li> <li>Based on the scheme, the worker calls the appropriate protocol module to download it</li> </ul> </li> <li>After Downloading<ul> <li>Downloaded document is placed into a Document Input Stream (DIS)</li> <li>It enables other modules to re-read the document multiple times</li> </ul> </li> <li>After document has been written to DIS<ul> <li>The worker thread invokes a dedupe test to determine whether this document<ul> <li>Has been seen before associated with a different URL</li> </ul> </li> <li>If so, document is not processed further</li> </ul> </li> <li>Repeat the process</li> </ul>"},{"location":"website_designs/tools/web_crawler#document-processing","title":"Document Processing","text":"<ul> <li>Based on the downloaded document's MIME type (like HTML, image, video, etc.)<ul> <li>Invoke the process method of each processing module associated with that MIME type</li> <li>Implement the protocols and the MIME types in a modular way for extensibility</li> </ul> </li> <li>HTML processing module will extract all links from the page<ul> <li>Each link is converted into an absolute URL</li> <li>The links are filtered using user-supplied URL filter</li> <li>Then URL-seen test is performed to see if it is already in frontier or downloaded</li> <li>If the url is new, it is added to the frontier</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#url-frontier","title":"URL Frontier","text":"<ul> <li>Stores list of URLs to download and prioritizes them</li> <li>We can crawl by performing breadth first traversal of the web<ul> <li>Starting from the pages in the seed set</li> <li>Such traversals are easily implemented using a FIFO queue</li> </ul> </li> <li>The URL list will be huge, so distribute the frontier into multiple servers<ul> <li>Each server can have multiple worker threads performing crawling tasks</li> <li>A hash function will map each URL to a server which will be responsible for crawling it</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#politeness-requirements","title":"Politeness Requirements","text":"<ul> <li>Politeness requirements must be kept in mind while designing a distributed URL frontier<ul> <li>Crawler should not overload a server by downloading a lot of pages from it</li> <li>We should not have multiple machines connecting a web server</li> </ul> </li> <li>To implement this politeness constraint<ul> <li>Crawler can have a collection of distinct FIFO sub-queues on each server<ul> <li>Each worker thread will have its separate sub-queue</li> <li>From which it can remove URLs for crawling</li> <li>This will avoid overloading the web server</li> </ul> </li> <li>When a new URL needs to be added<ul> <li>The sub-queue for it will be determined by the URL\u2019s canonical hostname</li> <li>A hash function will map each hostname to a thread number</li> <li>Hence, at most one worker thread will download documents from a given server</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#storing-urls","title":"Storing URLs","text":"<ul> <li>There will be hundreds of millions of URLs, hence we need to store them on a disk</li> <li>Queues can have separate buffers for enqueuing and dequeuing</li> <li>Enqueue buffer, when filled, will be dumped to the disk</li> <li>Dequeue buffer will keep a cache of URLs to be visited<ul> <li>It can periodically read from disk to fill the buffer</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#fetcher","title":"Fetcher","text":"<ul> <li>Downloads the document corresponding to a given URL<ul> <li>Using the appropriate network protocol like HTTP</li> </ul> </li> <li>Web masters create robot.txt<ul> <li>To make certain parts of their websites off limits for crawler</li> </ul> </li> <li>To avoid downloading robot.txt file on every request<ul> <li>The HTTP protocol module can maintain a fixed-sized cache</li> <li>Mapping hostnames to their robot\u2019s exclusion rules</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#document-input-stream","title":"Document Input Stream","text":"<ul> <li>The crawler's design enables the same document to be processed by multiple processing modules<ul> <li>To avoid downloading a document multiple times</li> <li>We can cache the document locally using an abstraction called Document Input Stream (DIS)</li> </ul> </li> <li>It is an input stream that caches the entire contents of the document<ul> <li>It also provides methods to re-read the document</li> <li>It can cache small documents (&lt; 64 KB) entirely in memory</li> <li>While larger documents can be temporarily written to a backing file</li> </ul> </li> <li>Each worker thread has an associated DIS, which it reuses from document to document<ul> <li>After extracting a URL from the frontier<ul> <li>The worker passes that URL to the relevant protocol module</li> </ul> </li> <li>The protocol module initializes the DIS from a network connection<ul> <li>To contain the document\u2019s contents</li> </ul> </li> <li>The worker then passes the DIS to all relevant processing modules</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#document-dedupe-test","title":"Document Dedupe Test","text":"<ul> <li>Many documents on the web are available<ul> <li>Under multiple different URLs or mirrored on various servers</li> </ul> </li> <li>To avoid downloading duplicate documents, we can perform a dedupe test<ul> <li>Calculate a 64-bit checksum (using MD5 or SHA) of every processed document<ul> <li>And store it in a database</li> </ul> </li> <li>For every new document, we can compare its checksum to previously calculated ones</li> </ul> </li> <li>For 15 billion distinct web pages<ul> <li>We would need: 15 B * 8 bytes = 120 GB</li> <li>This can fit in a server\u2019s memory<ul> <li>But if we don't have enough memory available</li> <li>We can keep smaller LRU cache on each server</li> </ul> </li> <li>Check for the checksum in the cache first and then the persistent storage<ul> <li>If not present, add the new checksum to cache and the back storage</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#url-filters","title":"URL Filters","text":"<ul> <li>Controls the set of URLs that are downloaded</li> <li>Used to blacklist websites for the crawler to ignore them</li> <li>Used to restrict URLs by domain, prefix, or protocol type</li> <li>Before adding each URL to the frontier<ul> <li>The worker thread consults the user-supplied URL filter</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#domain-name-resolution","title":"Domain Name Resolution","text":"<ul> <li>Before contacting a web server<ul> <li>The crawler must use DNS to map the web server's hostname to an IP address</li> </ul> </li> <li>DNS name resolution will be a big bottleneck given the amount of URLs<ul> <li>To avoid repeated requests, cache DNS results by building our local DNS server</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#url-dedupe-test","title":"URL Dedupe Test","text":"<ul> <li>While extracting links, we will encounter multiple links to the same document<ul> <li>To avoid downloading duplicate documents</li> <li>We can perform a URL dedupe test before adding to the frontier</li> </ul> </li> <li>We can store all the URLs seen by the crawler in canonical form in a database<ul> <li>To save space, we can store a fixed-sized checksum rather than the textual form of URL</li> <li>To reduce the number of operations on the database store<ul> <li>We can keep an in-memory cache of popular URLs on each host shared by all threads</li> </ul> </li> </ul> </li> <li>Storage estimation: 15 B * 4 bytes = 60 GB<ul> <li>Considering 15 B distinct URLs and 4 bytes for checksum</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#bloom-filters","title":"Bloom Filters","text":"<ul> <li>Can we use bloom filters for deduping?<ul> <li>Bloom filters are a probabilistic data structure for set membership testing</li> <li>That may yield false positives</li> </ul> </li> <li>A large bit vector represents the set<ul> <li>An element is added to the set by computing \u2018n\u2019 hash functions of the element</li> <li>And setting the corresponding bits</li> </ul> </li> <li>An element is deemed to be in the set<ul> <li>If the bits at all \u2018n\u2019 of the element\u2019s hash locations are set</li> <li>Hence, a document may incorrectly be deemed to be in the set<ul> <li>But false negatives are not possible</li> </ul> </li> </ul> </li> <li>Disadvantage: Each false positive will cause the URL not to be added to the frontier<ul> <li>Hence the document will never be downloaded</li> <li>The chance of a false positive can be reduced by making the bit vector larger</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#checkpointing","title":"Checkpointing","text":"<ul> <li>A crawl of the entire web takes weeks to complete</li> <li>To guard against failures, the crawler can write regular snapshots of its state to disk</li> <li>An interrupted or aborted crawl can easily be restarted from the latest checkpoint</li> </ul>"},{"location":"website_designs/tools/web_crawler#scalability","title":"Scalability","text":""},{"location":"website_designs/tools/web_crawler#fault-tolerance","title":"Fault tolerance","text":"<ul> <li>We should use consistent hashing for distribution among crawling servers<ul> <li>It will not only help in replacing a dead host</li> <li>But also help in distributing load among crawling servers</li> </ul> </li> <li>All the crawling servers will be performing regular checkpointing<ul> <li>And store their FIFO queues to disks</li> </ul> </li> <li>If a server goes down, we can replace it<ul> <li>Meanwhile, consistent hashing will shift the load to other servers</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#data-partitioning","title":"Data Partitioning","text":"<ul> <li>Our crawler will be dealing with three kinds of data<ul> <li>URLs to visit</li> <li>URL checksums for dedupe</li> <li>Document checksums for dedupe</li> </ul> </li> <li>Since we are distributing URLs based on the hostnames<ul> <li>We can store all of this data on the same host</li> <li>Each host will store its set of URLs<ul> <li>That needs to be visited</li> <li>Checksums of previously visited URLs</li> <li>Checksums of all the downloaded documents</li> </ul> </li> </ul> </li> <li>Since we are using consistent hashing, URLs will be redistributed from overloaded hosts</li> <li>Each host will perform checkpointing periodically<ul> <li>And dump a snapshot of all the data onto a remote server</li> <li>This will ensure that if a server dies, another can replace it using the last snapshot</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#crawler-traps","title":"Crawler Traps","text":"<ul> <li>There are many crawler traps, spam sites, and cloaked content</li> <li>A crawler trap is a URL or set of URLs that cause a crawler to crawl indefinitely<ul> <li>Some are unintentional<ul> <li>Like a symbolic link within a file system can create a cycle</li> </ul> </li> <li>Others are intentional<ul> <li>Like written traps that dynamically generate an infinite web of documents</li> </ul> </li> </ul> </li> <li>Solutions<ul> <li>Anti-spam traps are designed to catch crawlers<ul> <li>Used by spammers looking for email addresses</li> </ul> </li> <li>Other sites use traps to catch search engine crawlers<ul> <li>Used to boost their search ratings</li> </ul> </li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#adaptive-online-page-importance-computation","title":"Adaptive Online Page Importance Computation","text":"<ul> <li>AOPIC algorithm can help mitigate common types of bot-traps</li> <li>AOPIC solves this problem by using a credit system<ul> <li>Start with a set of N seed pages</li> <li>Before crawling starts, allocate a fixed X amount of credit to each page</li> </ul> </li> <li>Process the pages<ul> <li>Select a page P with the highest amount of credit<ul> <li>Or select a random page if all pages have the same credit</li> </ul> </li> <li>Crawl page P (assume the credit of 100)<ul> <li>Extract all the links from page P (let's say there are 10 links)</li> <li>Set the credits of P to 0</li> <li>Take a 10% tax and allocate it to a Lambda page</li> </ul> </li> <li>Allocate equal amount of credits to each link found on page P<ul> <li>Distribute credits from P subtracting the tax</li> <li>(100 credits from P - 10% tax) / 10 links = 9 credits per link</li> </ul> </li> <li>Select another page</li> </ul> </li> </ul>"},{"location":"website_designs/tools/web_crawler#aopic-explaination","title":"AOPIC Explaination","text":"<ul> <li>Since the Lambda page continuously collects tax<ul> <li>Eventually it will have the largest amount of credit, and we\u2019ll have to crawl it</li> </ul> </li> <li>By crawling the Lambda page, we just take its credits<ul> <li>And distribute them equally to all the pages in our database</li> </ul> </li> <li>Since bot traps only give internal links credits<ul> <li>And they rarely get credit from the outside</li> <li>They will continually leak credits (from taxation) to the Lambda page</li> </ul> </li> <li>The Lambda page will distribute those credits out to all the pages in the database evenly<ul> <li>Upon each cycle, the bot trap page will lose more and more credits</li> <li>Until it has so little credits that it almost never gets crawled again</li> </ul> </li> <li>This will not happen with good pages<ul> <li>Because they often get credits from backlinks found on other pages</li> </ul> </li> </ul>"}]}